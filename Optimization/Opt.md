# Optimization

## Laguange Dual
- A good summary:
	- https://blog.csdn.net/asd136912/article/details/79149881
	- https://www.cnblogs.com/90zeng/p/Lagrange_duality.html
	- https://www.zhihu.com/question/58584814
	- https://zhuanlan.zhihu.com/p/26514613
- Application:
	- SVM;

## Deep-Learning optimizers
- A good summary:
	- http://ruder.io/optimizing-gradient-descent/
- SGD:
<img src="/Optimization/images/sgd.png" alt="drawing" width="200"/>

- SGD with batch:
<img src="/Optimization/images/sgd-batch.png" alt="drawing" width="200"/>

- Momentum:
<img src="/Optimization/images/momentum.png" alt="drawing" width="200"/>

- Nestorov:
<img src="/Optimization/images/nag.png" alt="drawing" width="200"/>

- Adagrad:
<img src="/Optimization/images/adagrad.png" alt="drawing" width="500"/>

- Rmsprop:
<img src="/Optimization/images/rmsprop.png" alt="drawing" width="400"/>

- ADAM:
<img src="/Optimization/images/adam.png" alt="drawing" width="600"/>

- Adadelta:
<img src="/Optimization/images/adadelta.png" alt="drawing" width="600"/>

## SVGD
- Stein variational gradient descent: A general purpose bayesian inference algorithm, NIPS 2016
	- https://github.com/DartML/Stein-Variational-Gradient-Descent