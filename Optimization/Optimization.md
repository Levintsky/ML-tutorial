# DL, SGD

## Loss Design
- Visualizing the Loss Landscape of Neural Nets. NIPS'18

## Deep-Learning optimizers
- Summaries:
	- http://ruder.io/optimizing-gradient-descent/

## Regularization
- M. Jordan. Stochastic Cubic Regularization for Fast Nonconvex Optimization. NIPS'18

## Noise
- Mirrored Langevin Dynamics. NIPS'18
- The promises and pitfalls of Stochastic Gradient Langevin Dynamics. NIPS'18

## Variance Reduction:
- UCLA. Stochastic Nested Variance Reduced Gradient Descent for Nonconvex Optimization. NIPS'18
- SEGA: Variance Reduction via Gradient Sketching. NIPS'18
- Stochastic Expectation Maximization with Variance Reduction. NIPS'18

## Large-Scale
- R Anil, V Gupta, T Koren, and Y Singer. Memory-efficient adaptive optimization for large-scale learning. 2019
- Frank Wood. Bayesian Distributed Stochastic Gradient Descent. NIPS'18

## Unclassified
- Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. 2018
- Provably Correct Automatic Sub-Differentiation for Qualified Programs. NIPS'18
- L4: Practical loss-based stepsize adaptation for deep learning
- Natasha 2: Faster Non-Convex Optimization Than SGD. NIPS'18
- Learning with SGD and Random Features. NIPS'18