# SGD

## 1st-Order
- SGD:\
	<img src="/Optimization/images/sgd/sgd.png" alt="drawing" width="200"/>
- SGD with batch:\
	<img src="/Optimization/images/sgd/sgd-batch.png" alt="drawing" width="200"/>
- Momentum:\
	<img src="/Optimization/images/sgd/momentum.png" alt="drawing" width="200"/>
- Nestorov:\
	<img src="/Optimization/images/sgd/nag.png" alt="drawing" width="200"/>
- **AdaGrad**: J Duchi, E Hazan, and Y Singer. Adaptive subgradient methods for online learning and stochastic optimization. JMLR'11\
	<img src="/Optimization/images/sgd/adagrad.png" alt="drawing" width="450"/>
- Rmsprop:\
	<img src="/Optimization/images/sgd/rmsprop.png" alt="drawing" width="400"/>
- **ADAM**: D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. 2014
	<img src="/Optimization/images/sgd/adam.png" alt="drawing" width="500"/>
- Adadelta: M Zeiler. ADADELTA: An Adaptive Learning Rate Method. 2012
	<img src="/Optimization/images/sgd/adadelta.png" alt="drawing" width="600"/>
- U Şimşekli，L Sagun, M Gürbüzbalaban. A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks. ICML'19 best paper honorable mention

## Unclassified
- Zeyuan Zhu. The Lingering of Gradients: How to Reuse Gradients Over Time. NIPS'18
- Wotao Yin. On Markov Chain Gradient Descent. NIPS'18