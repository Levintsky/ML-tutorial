# Optimization for Deep Learning

## Unclassified
- Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. 2018
- Provably Correct Automatic Sub-Differentiation for Qualified Programs. NIPS'18
- L4: Practical loss-based stepsize adaptation for deep learning
- Natasha 2: Faster Non-Convex Optimization Than SGD. NIPS'18
- Learning with SGD and Random Features. NIPS'18

## NIPS'19
- Difan Zou, Quanquan Gu. An Improved Analysis of Training Over-parameterized Deep Neural Networks
- Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, Yaron Lipman. Controlling Neural Level Sets
- Shaojie Bai, J. Zico Kolter, Vladlen Koltun. Deep Equilibrium Models
- Junbang Liang, Ming Lin, Vladlen Koltun. Differentiable Cloth Simulation for Inverse Problems
- Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, George Pappas. Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks
- Yuan Cao, Quanquan Gu. Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks
- Lili Su, Pengkun Yang. On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective
- Zhuoning Yuan, Yan Yan, Rong Jin, Tianbao Yang. Stagewise Training Accelerates Convergence of Testing Error Over SGD
- Yuanzhi Li, Colin Wei, Tengyu Ma. Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks
- Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, Bin Dong. You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle
- Florian Scheidegger, Luca Benini, Costas Bekas, A. Cristiano I. Malossi. Constrained deep neural network architecture search for IoT devices accounting for hardware calibration
- Gauthier Gidel, Francis Bach, Simon Lacoste-Julien. Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks
- Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan Lim. In-Place Zero-Space Memory Protection for CNN
- Stanislav Fort, Stanislaw Jastrzebski. Large Scale Structure of Neural Network Loss Landscapes
- Zeyuan Allen-Zhu, Yuanzhi Li, Yingyu Liang. Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers
- Frederik Kunstner, Philipp Hennig, Lukas Balles. Limitations of the empirical Fisher approximation for natural gradient descent
- Michael Arbel, Anna Korba, Adil SALIM, Arthur Gretton. Maximum Mean Discrepancy Gradient Flow
- Lénaïc Chizat, Edouard Oyallon, Francis Bach. On Lazy Training in Differentiable Programming
- Sébastien Arnold, Pierre-Antoine Manzagol, Reza Babanezhad Harikandeh, Ioannis Mitliagkas, Nicolas Le Roux. Reducing the variance in online optimization by transporting past gradients
- Yuan Cao, Quanquan Gu. Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks
- Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, Junyang Lin. Understanding and Improving Layer Normalization
- Janice Lan, Rosanne Liu, Hattie Zhou, Jason Yosinski. LCA: Loss Change Allocation for Neural Network Training
