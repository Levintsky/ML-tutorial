# Basics of Optimization

## Basics
- Textbook:
	- Linear and Nonlinear Programming: David Luenberger, Yinyu Ye;
	- Numerical Optimization: Jorge Norcedal, Stephen Wright.
- Courses:
	- CME307/MS&E311: Optimization
- Software
	- Gurobi
	- Mosek
- Linear programming
	- Simplex (L/NL-P Chap-3; NO Chap-13)
	- Duality and Complementarity (L/NL-P Chap-4)
	- Interior-Point (L/NL-P Chap-5; NO Chap-14)
	- Conic LP (L/NL-P Chap-6)
- Uncontrained
	- Basics: (L/NL-P Chap-7)
		- 0/1/2-order
		- convex/concave
		- speed of convergence;
	- Basic descent:  (L/NL-P Chap-8; NO, Chap-3,4)
		- Line search
		- GD/Newton/Coord-descent;
		- Trust-region;
	- Conjugate directions: (L/NL-P Chap-9; NO Chap-5)
	- Quasi-Newton: (L/NL-P Chap-10; NO Chap-6)
		- BFGS, SR1
	- Large-scale: (NO Chap-7)
- Constrained
	- Constrained: Tangent-subspace (L/NL-P Chap-11)
 	- Primal: (L/NL-P Chap-12)
 	- Penalty and Barrier: (L/NL-P Chap-13)
 	- Duality: Local/global-duality, Lagrange (L/NL-P Chap-14; NO Chap-12)
 	- Primal-dual: (L/NL-P Chap-15)
- Derivative:
	- Calculating Derivatives/Hessian (NO Chap-8)
	- Derivative-Free Optimization (NO Chap-9)
- Least square: (NO Chap-10)
	- Gauss-Newton;
	- Levenberg–Marquardt;
- Nonlinear Equations: (NO Chap-11)
- QuadraticProgramming: (NO Chap-16)
	- KKT;
- Penalty and Augmented Lagrangian Methods: (NO Chap-17)
- Sequential Quadratic Programming: (NO Chap-18)
	- SQP, IQP, EQP;
- Interior-Point Methods for Nonlinear Programming: (NO Chap-19)

## Math Preliminaries
- Affine set:
	- x, y ∈ S and α ∈ R ⇒ αx + (1 − α)y ∈ S.
	- 0 ≤ α ≤ 1: convex;
- Cone: x ∈ C ⇒ αx ∈ C ∀ α>0
	- **Dual**: C∗ :={y: x•y ≥ 0 ∀ x∈C}.
	- Theorem 1 The dual is always a closed convex cone, and the dual of the dual is the closure of convex hall of C.
	- Polyhedral Convex Cones: C = {x: Ax ≤ 0}
- Lipschitz Functions
	- ∥∇f(x) − ∇f(y)∥ ≤ β∥x − y∥
- Inequalities:
	- Cauchy-Schwarz: |x'y| ≤ ∥x∥p ∥y∥q, where 1/p+1/q=1;
	- Triangle: ∥x + y∥p ≤ ∥x∥p + ∥y∥p for p ≥ 1
	- Arithmetic: 1/n ∑xj ≥ (∏xj)^1/n
- Linear equations:
	- Ax = b has a solution iff A'y=0, b'y≠0 has no solution.
		- infeasibility certificate
- Lipchitz condition:
	- https://zhuanlan.zhihu.com/p/27554191

## Mathematical Optimization Models and Applications
- Stanford CME307/MS&E311 Chap-1
	- Primal: min f(x) s.t. x ∈ X
	- X usually:
		- ci(x) = 0;
		- ci(x) ≤ 0;
- Model:
	- Unconstrained, Linear, Nonlinear, Convex;
	- Conc Linear, Mixed-Integer, stochastic;
	- Fixed-Point or Min-Max;
- Structured: Conic Linear Programming;
	- min (c, x)
	- s.t. Ax = b, x ∈ K
	- LP: K nonnegative orthant cone
	- SOSP: K is the second-order cone
	- SDP: K is the semidefinite matrix cone
- Examples:
	- Facility Location: POCP (p-th order CLP)
	- Sparse linear regression: min|x|0 s.t. Ax=b;
		- LASSO: L1; (LP)
	- SVM: find slope x and scalar x0, s.t.
		- (ai, x) + x0 ≥ 1
		- (bj, x) + x0 ≤ -1
		- Introduce β for inseparable and slope regularization:
			- min β + μ|x|^2
			- s.t. (ai, x) + x0 + β ≥ 1
			- s.t. (bj, x) + x0 - β ≤ -1
			- β ≥ 0
			- Constrained QP;
		- Ellipsoidal Separation: SDP;	
	- Logistic: NLL loss;
	- QP: Portfolio Management
		- min x'Vx, s.t. (r,x)≥μ, (e,x)=1, x≥0;
			- Convex QP
		- Robust: QCQP, SOCP;
	- Portfolio Selection Problem: MIP;
	- Optimal Transportation: Wasserstein Barycenter Problem;
		- ∑∑cij xij
			- s.t. ∑j xij = si
			- s.t. ∑j xij = dj
			- xij ≥ 0
	- Graph Realization and Sensor Network Localization
		- SOCP;
	- Stochastic: minx E_Fξ[h(x, ξ)]
	- Learning with noise distortion;
		- Verification: min|x-xˆ|^2
			- s.t. f(x) ∈ ...
	- DRO (Distributionally Robust Optimization and Learning)
		- minx maxD E_Fξ∈D[h(x, ξ)]
	- Reinforcement Learning: Markov Decision/Game Process
		- yi∗ = min{cj + γpj' y∗, ∀j ∈ Ai}, ∀i,
		- πi∗ = argmin{cj + γpj' y∗, ∀j ∈ Ai}, ∀i.

## Conic
- Stanford CME307/MS&E311 Lec-3
	- Caratheodory's theorem
	- Basic and Basic Feasible Solution I
	- Separating and supporting hyperplane theorem
	- Farkas' Lemma: {x: Ax=b, x≥0} feasible iff −A' y ≥ 0 and b' y > 0  infeasible;
	- Alternative systems I: Ax=b, x≥0 | −A'y≥0, b'y=1(>0)
	- Alternative systems II: Ax=0, x≥0, c'x=−1(<0) | c − A' y ≥ 0
	- General: {x: Ax=b, x∈K} | {y: −A'y∈K∗, b'y>0}.
	- General: Ax=0, x∈K, c'x=−1(<0) | c − A'y ∈ K∗
	- Dual of Conic LP:
		- CLP: min c'x s.t. ai'x=bi
		- CLD: max b'y s.t. ∑yiai + s = c
	- The dual of the dual is the primal.
	- Transportation Dual: Economic Interpretation
	- The dual of the Max-Flow problem: Min-Cut
	- LP formulation of RL/MDP
	- LP formulation of Nash Equilibrium
- CME307/MS&E311 Lec-4: Conic Duality Theorems and Applications
	- Primal/Dual of Conic LP:
		- CLP: min c'x s.t. ai'x=bi
		- CLD: max b'y s.t. ∑yiai + s = c
	- Weak duality: c•x−b'y = x•s ≥ 0
	- Strong dualiy: Let x∗ ∈ Fp and (y∗, s∗) ∈ Fd. Then, c • x∗ = bT y∗ implies that x∗ is optimal for (CLP) and (y∗, s∗) is optimal for (CLD).
	- Complementarity Condition:
		- For feasible x and (y,s), xT s = xT (c − AT y) = cT x − bT y is called the **complementarity gap**;
			- x' s = 0: complementary to each other;

## Unconstrained Problems
- NO-Chap-2:
	- Local/global/strict/isolate optimzer
	- Recognizing local minimum:
		- Taylor: 1st, 2nd order;
	- Two strategies:
		- Line Search: 
		- Trust Region;
	- Search directions (for line search):
		- GD;
		- Newton direction:
		- Quasi-Newton: superlinear convergence;
			- Approximate and update Hessian: SR1, BFGS;
		- Nonlinear conjugate gradient:
	- Trust Region:
		- Assume B=0, steepest descent with radius;
		- Scaling;
- L/NL-P Chap-11: Optimality Conditions for Nonlinear Optimization
	- first-order necessary condition (FONC): ∇f(x) = 0
	- second-order necessary condition (SONC): ∇f(x) = 0 and ∇^2f(x) ≽ 0
	- **Lagrange Theorem**: minf(x) s.t. Ax = b, then local minimizer:
		- ∇f(x) = y'A
		- The geometric interpretation: the objective gradient vector is perpendicular to or the objective level set tangents the constraint hyperplanes.
	- **KKT theorem**: minf(x) s.t. Ax ≥ b, local minimizer:
		- ∇f(x) = yA, y ≥ 0 exists for some y;
		- y: **Lagrange or dual multipliers**;
	- **KKT**: if x is a local minimizer, then there exists y s.t.
		- aix (≤, =, ≥) bi; Original Problem Constraints (OPC)
		- ∇f(x) = yA; Lagrangian Multiplier Conditions (LMC)
		- yi (≤, free, ≥) 0; Multiplier Sign Constraints (MSC)
		- yi = 0 if i ∉ A(x); Complementarity Slackness Conditions (CSC)
- L/NL-P Chap-11: GCO
	- Generalized Constrained: GCO
		- min f(x), s.t. h(x)=0; c(x)≥0;
	- Lemma 1: let x be a feasible, if x is a local minimizer of GCO, then no d satisfy:
		- ∇f(x)d < 0;
		- ∇h(x)d = 0;
		- ∇ci(x)d ≥ 0, ∀i ∈ Ax;
	- Theorem (**1st-order KKT** of Optimality Condition) local minimizer:
		- ∇f(x) = y' ∇h(x) + s' ∇c(x)
		- si ci(x) = 0; ∀i (complementarity slackness)
		- Proof: based on alternative system theory, or Farkas Lemma;
	- Lagrange function: L(x,y,s) = f(x) − y' h(x) − s' c(x); y: free; s ≥ 0;
	- Lagrangian Relaxation Problem: (LRP) minx L(x, y, s).
	- **2nd-order necessary** conditions for GCO:
		- Tx := {z: ∇h(x)z=0, ∇ci(x)z=0 ∀i ∈ Ax}.
		- Theorem: GCO, let x,y,s satisfies 1st-order KKT, it is necessary to have:
			- d' ∇^2x L(x,y,s) d ≥ 0 ∀ d ∈ Tx.
	- **2nd-order sufficient** conditions for GCO:
		- d' ∇^2x L(x, y) d > 0 ∀0 ≠ d ∈ Tx;
	- d in the null space of A iff
		- d = (I −A(AA')^−1A)u = PA u form some u;
		- PA: projection matrix of A;
	- Test with constraint Ad=0 becomes: u' PA Q PA u ≥ 0, ∀ u ∈ Rn

## Search Directions
- NO-Chap-3: Line Search
	- 3.1 Step length:
		- Armijo condition (sufficient decrease);
			- f(xk + αpk) ≤ f(xk) + c1 α ∇fk pk
		- Curvature condition;
			- ∇f(xk + αkpk)' pk ≥ c2 ∇fk' pk, for c2 ∈ (c1, 1);
				- Typically c2=0.9 (quasi)-Newton, 0.1 CG; 
		- Wolfe condition: Armijo + Curvature;
		- Goldstein condition:
			- f(xk) + (1−c)αk ∇fk' pk ≤ f(xk +αkpk) ≤ f(xk) + c αk ∇fk pk; for 0 < c < 1/2.
			- to make sufficient decrease not too short;
		- Backtracking:
			- repeat α ← ρα with ρ∈(0,1), until condition satisfies;
	- 3.2 Convergence of Line Search
		- Theo-3.2 ∑cosθk^2 |∇fk|^2 < ∞
		- cosθk ≥ δ > 0 (bounded away from 90◦), converge;
		- |Bk| |B^−1| ≤ M, cosθk ≥1/M, converge;
	- 3.3 Rate of Convergence
		- f(x) = 1/2 x'Qx - b'x
		- Theo 3.3 (**steepest descent**) with exact line search:
			- |xk+1−x∗|^2 ≤ (λn-λ1)^2/(λn+λ1)^2 |xk−x∗|^2
		- Newton's method: p = −(∇^2f)^−1 ∇f
		- Theo 3.5 (**Newton**) positive-definite, then rate of convergence is **quadratic**;
			- |xk+pk−x∗| ≤ L|∇^2f(x∗)|^−1|xk−x∗|^2
		- Quasi-Newton: pk = -Bk^-1 ∇fk
		- Theo-3.6 (**Quasi-Newton**) Wolfe, positive-definite:
			- αk=1 is admissible for k large enough;
			- if αk=1 for all k > k0, {xk} converges to x∗ **superlinearly**.
		- Super-linearity ⇔ |(Bk−∇2f(x∗))pk| / |pk| = 0
	- 3.4 Newton's method with Hessian Modification
		- Add a diagonal to make positive definite;
		- Eigenvalue
		- Modified Cholesky Factorization;
		- Modified Symmetric Indefinite factorization;
	- 3.5 Step-length selection algorithms
		- Convex quadratic: closed-form analytically
		- Interpolation;
		- Initial step length;
- NO-Chap-4: Trust-Region Methods
	- Approximate locally in a neighborhood:
		- min_p mk(p) = fk + gk' p + 1/2 p'Bkp s.t. |p| ≤ Δk
	- Evaluate performance:
		- ρk = (f(xk) − f(xk+pk)) / (mk(0) - mk(pk))
	- Cauchy-point;
	- 4.1 Algorithms based on **Cauchy point**
		- Approx cost by pk = argmin fk + gk'p s.t. |p| ≤ Δk
			- pk = - Δk/|gk| gk (steepest descent direction)
			- τk = 1 if gkBkgk ≤ 0;
			- min(|gk|^3/(Δkgk'Bkgk), 1), otherwise;
		- **Dogleg** method:
			- pB = −B^−1 g > Δk?
			- Two line segments:
				- First: pU = - (g'g)/(g'Bg) g
				- Second: pU to pB
	- 4.2 Global Convergence
		- Lemma 4.3 Cauchy point pkC satisifes:
			- mk(0) - mk(pkC) ≥ 1/2 |gk| min(Δk, |gk|/|Bk|)
	- 4.3 Iterative Solution of subproblem
		- Find optimal p(λ) = −(B + λI)−1g
- NO-Chap-5: Conjugate Gradient Methods
	- 5.1 Linear CG
		- Ax = b, A: symmetric positive definite;
			- minφ(x) = 1/2 x'Ax − b'x
			- ∇φ(x) = Ax − b = r(x)
		- Conjugate: pi'Apj = 0, i ≠ j
		- xk+1 = xk + αk pk
			- αk = -(rk'pk)/(pk'Apk), 1-dim minimizer of φ(.);
		- Theo-5.1 converges to solution in at most n steps.
		- rk+1 = rk + αkApk
		- Theo-5.2 (Expanding Subspace Minimization)
			- rk'pi = 0; for i=0,1,...,k-1
			- xk is minimizer of φ(.) in {x|x=x0 +span{p0,p1,...,pk−1}}
		- Algorithm:
			- Init: r0 = Ax0-b, p0 ← -r0, k = 0;
				- p0: initial direction as steepest descent;
				- Some proof won't hold
			- For new direction pk, get best step-length αk:
				- αk = -(rk'pk)/(pk'Apk)
				- xk+1 ← xk + αkpk; rk+1 ← Axk+1 - b;
			- Get new direction pk+1:
				- βk+1 = rk+1'Apk/(pk'Apk)
				- pk+1 ← -rk+1 + βk+1pk;
			- k ← k+1
		- Theo-5.4 r distinct eigenvalues -> terminate in at most r iterations;
		- Theo-5.5 A has eigenvalues λ1 ≤ λ2 ≤ ··· ≤ λn, then: |xk+1−x∗|^2 ≤ (λn-k − λ1)^2/ (λn-k + λ1)^2 |x0−x∗|^2;
		- Preconditioning: xˆ = Cx
	- 5.2 Nonlinear CG
		- Fletcher-Reeves Method:
			- Init: x0, f0=f(x0), ∇f0=∇f(x0), p0 ← −∇f0;
			- while ∇fk ≠ 0:
				- Get αk, xk+1 = xk + αk pk; (step length)
				- Eval ∇fk+1;
				- βk+1 = |∇fk+1|^2 / |∇fk|^2
				- pk+1 = -∇fk+1 + βk+1 pk; (direction)
		- Polak-Ribiere Method and variant:
			- Variants differ from each other by β;
			- P-R: βk+1 = ∇fk+1'(∇fk+1-∇fk) / |∇fk|^2
		- Global convergence:
			- Theo-5.7 lim inf|∇fk| = 0.
- NO-Chap-6: Quasi-Newton Methods
	- 6.1 BFGS:
		- Bk+1 αk pk = ∇fk+1 − ∇fk
		- Bk+1 sk = yk (**Secant Equation**)
			- sk = xk+1 - xk = αk pk
			- yk = ∇fk+1 − ∇fk
		- Bk+1 positive-definite: curvature condition
			- sk' yk > 0
		- If nonconvex, need Wolfe Condition to make curvature condition work;
		- Among all Bk+1 satisfying Secont equation, find the one closest to current Bk:
			- min|B-Bk| s.t. B=B', Bsk=yk
		- **DFP**: Bk+1 and inverse of Bk, Hk:
			- Bk+1 = (I − ρk yk sk')Bk(I − ρk sk yk') + ρk yk yk'
				- with ρk = 1/(yk'sk), rank-2 modification;
			- Hk+1 = Hk − Hkykyk'Hk/(yk'Hkyk) + (sksk')/(yk'sk)
		- **BFGS**: Hk+1yk = sk, find the Hk s.t.
			- min|H-Hk| s.t. H=H', Hyk = sk
			- Hk+1 = (I − ρkskyk')Hk(I − ρkyksk') + ρksksk'
			- Bk+1 = Bk - Bksksk'Bk/(sk'Bksk) + (ykyk')/(yk'sk)
		- DFP and BFGS: dual of each other;
		- BFGS + Wolfe-step-length
	- 6.2 SR1 Method
		- Bk+1 = Bk + σvv' with Sekant equation;
		- SR1: Bk+1 = Bk + (yk-Bsk)(yk-Bsk)' / (yk-Bksk)'sk
		- SR1: Hk+1 = Hk + (sk-Hkyk)(sk-Hkyk)' / (sk-Hksk)'yk
		- SR1 + Trust-region;
	- 6.3 The Broyden Class
		- Bk+1 = Bk - Bksksk'Bk/(sk'Bksk) + (ykyk')/(yk'sk) + φk(sk'Bksk)vkvk'
			- vk = yk/(yk'sk) - Bksk/(sk'Bksk)
		- BFGS: φk = 0;
		- DFP: φk = 1;
		- restricted Broyden class: φk in [0,1]
	- 6.4 Convergence Analysis
		- Theo-6.5 Converge;
		- Theo-6.6 superlinear rate;
- NO Chap-7 Large-Scale Unconstrained Optimization
	- 7.1 Inexact Newton
		- ∇^2fk pk = −∇fk
		- Iteratively solve pk (not exactly)
			- Stop when the residual small enough: rk = ∇^2fk pk + ∇fk
		- Theo-7.1 |rk| ≤ ηk |∇fk|, then
			- |∇2 f(x∗)(xk+1 − x∗)| ≤ ηˆ|∇2 f(x∗)(xk − x∗)|
		- Line-search Newton-CG (truncated Newton)
		- Trust-region Newton-CG (CG–Steihaug)
		- Trust-region Newton-Lanczos
			- Search after j stpes;
	- 7.2 Limited-Memory Quasi-Newton
		- L-BFGS:
			- xk+1 = xk − αk Hk ∇f
			- Hk+1 = Vk'HkVk + ρksksk'
			- L-BFGS: save (si, yi) pairs instead of Hk;
	- 7.3 Sparse Quasi-Newton
	- 7.4 Partially Separable Functions

## Convex Optimization
- **cvxlayers**: A Agrawal, B Amos, S Barratt, S Boyd, S Diamond, and Z Kolter. Differentiable Convex Optimization Layers. NIPS'19
	- https://github.com/cvxgrp/cvxpylayers (support both pytorch and tf)

## Variance Reduction
- UCLA. Stochastic Nested Variance Reduced Gradient Descent for Nonconvex Optimization. NIPS'18
- SEGA: Variance Reduction via Gradient Sketching. NIPS'18
- Stochastic Expectation Maximization with Variance Reduction. NIPS'18

## Learning to Optimize
- Petr Hruby, Timothy Duff, Anton Leykin, Tomas Pajdla. Learning To Solve Hard Minimal Problems. CVPR'22
	- homotopy continuation (HC) + ML
	- Learn to initialize g(.)
	- Applications:
		- 5pt

## Regularization
- M. Jordan. Stochastic Cubic Regularization for Fast Nonconvex Optimization. NIPS'18
- NIPS'19
	- Dennis Maximilian Elbrächter, Julius Berner, Philipp Grohs. How degenerate is the parametrization of neural networks with the ReLU activation function?
	- Sanjeev Arora, Nadav Cohen, Wei Hu, Yuping Luo. Implicit Regularization in Deep Matrix Factorization
	- Fariborz Salehi, Ehsan Abbasi, Babak Hassibi. The Impact of Regularization on High-dimensional Logistic Regression
	- Qian Qian, Xiaoyuan Qian. The Implicit Bias of AdaGrad on Separable Data
	- Aditya Sharad Golatkar, Alessandro Achille, Stefano Soatto. Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence

## SGD NIPS'19
- Dominic Richards, Patrick Rebeschini. Optimal Statistical Rates for Decentralised Non-Parametric Regression with Linear Speed-Up
- Robert Gower, Dmitry Koralev, Felix Lieder, Peter Richtarik. RSN: Randomized Subspace Newton
- Othmane Sebbouh, Nidham Gazagnadou, Samy Jelassi, Francis Bach, Robert Gower. Towards closing the gap between the theory and practice of SVRG
- Ali Kavis, Kfir Y. Levy, Francis Bach, Volkan Cevher. UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization
