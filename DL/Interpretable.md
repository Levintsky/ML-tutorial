# Explain NN

## Interpretability
- David Alvarez-Melis, Tommi S. Jaakkola. Towards Robust Interpretability with Self-Explaining Neural Networks. NIPS'18
- Wenbo Guo, Sui Huang, Yunzhe Tao, Xinyu Xing, Lin Lin. Explaining Deep Learning Models -- A Bayesian Non-parametric Approach. NIPS'18
- Liwei Wang, Lunjia Hu, Jiayuan Gu, Yue Wu, Zhiqiang Hu, Kun He, John Hopcroft. Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation. NIPS'18
- Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim. A Benchmark for Interpretability Methods in Deep Neural Networks. NIPS'19
- Patrick Schwab, Walter Karlen. CXPlain: Causal Explanations for Model Interpretation under Uncertainty. NIPS'19

## Analysis
- Matthew Olson, Abraham Wyner, Richard Berk. Modern Neural Networks Generalize on Small Data Sets. NIPS'18
- Ann-Kathrin Dombrowski, Maximillian Alber, Christopher Anders, Marcel Ackermann, Klaus-Robert Müller, Pan Kessel. Explanations can be manipulated and geometry is to blame. NIPS'19
- Randall Balestriero, Romain Cosentino, Behnaam Aazhang, Richard Baraniuk. The Geometry of Deep Networks: Power Diagram Subdivision. NIPS'19

## Visualization
- Pei Wang, Nuno Nvasconcelos. Deliberative Explanations: visualizing network insecurities. NIPS'19
- Suraj Srinivas, François Fleuret. Full-Gradient Representation for Neural Network Visualization. NIPS'19
- Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, Been Kim. Visualizing and Measuring the Geometry of BERT. NIPS'19

## NIPS'19
- Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim. A Benchmark for Interpretability Methods in Deep Neural Networks
- Wieland Brendel, Jonas Rauber, Matthias Kümmerer, Ivan Ustyuzhaninov, Matthias Bethge. Accurate, reliable and fast robustness evaluation
- Ke Li, Tianhao Zhang, Jitendra Malik. Approximate Feature Collisions in Neural Nets
- Matthew Sotoudeh, Aditya V Thakur. Computing Linear Restrictions of Neural Networks
- Patrick Schwab, Walter Karlen. CXPlain: Causal Explanations for Model Interpretation under Uncertainty
- Ann-Kathrin Dombrowski, Maximillian Alber, Christopher Anders, Marcel Ackermann, Klaus-Robert Müller, Pan Kessel. Explanations can be manipulated and geometry is to blame
- Juyeon Heo, Sunghwan Joo, Taesup Moon. Fooling Neural Network Interpretations via Adversarial Model Manipulation
- Lukas Hoyer, Mauricio Munoz, Prateek Katiyar, Anna Khoreva, Volker Fischer. Grid Saliency for Context Explanations of Semantic Segmentation
- Alessio Ansuini, Alessandro Laio, Jakob H Macke, Davide Zoccolan. Intrinsic dimension of data representations in deep neural networks
- Ari Morcos, Haonan Yu, Michela Paganini, Yuandong Tian. One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers
- Randall Balestriero, Romain Cosentino, Behnaam Aazhang, Richard Baraniuk. The Geometry of Deep Networks: Power Diagram Subdivision
- Scott Gigante, Adam S Charles, Smita Krishnaswamy, Gal Mishne. Visualizing the PHATE of Neural Networks
