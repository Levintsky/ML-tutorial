# Deep Learning Theories

## Unclassified
- LÃ©onard Blier, Yann Ollivier. The Description Length of Deep Learning models. NIPS'18

## Approximator
- Hongzhou Lin, Stefanie Jegelka. ResNet with one-neuron hidden layers is a Universal Approximator. NIPS'18

## Convergence
- Over-Parametrization:
	- Francis Bach. On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport. NIPS'18
	- Yuanzhi Li. Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data. NIPS'18
	- Lili Su, Pengkun Yang. On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective. NIPS'19
- SGD:
	- Boris Hanin. Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients. NIPS'18
	- Allen Zhu. Byzantine Stochastic Gradient Descent. NIPS'18
- Normalization:
	- Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry. How Does Batch Normalization Help Optimization? NIPS'18
	- Nils Bjorck, Carla P. Gomes, Bart Selman, Kilian Q. Weinberger. Understanding Batch Normalizations. NIPS'18
	- Yixi Xu, Xiao Wang. Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units. NIPS'18
- Paul Hand. Phase Retrieval Under a Generative Prior. NIPS'18
- Allen Zhu. NEON2: Finding Local Minima via First-Order Oracles. NIPS'18
- Are ResNets Provably Better than Linear Predictors? NIPS'18
- Simon Du, Jason Lee. Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced. NIPS'18

## Generalization
- PAC-Bayes:
	- John Langford and John Shawe-Taylor. Pac-bayes & margins. NIPS'02
	- David McAllester. Simplified pac-bayesian margin bounds. 2003
	- Vaishnavh Nagarajan and Zico Kolter. Deterministic PAC-bayesian generalization bounds for deep networks via generalizing noise-resilience. ICLR'19
	- Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Nonvacuous generalization bounds at the imagenet scale: a PAC-bayesian compression approach. ICLR'19
- Behnam Neyshabur, Ryota Tomioka, Nathan Srebro. In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning. ICLR'15
- Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, Nathan Srebro. Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks, 2018
- NeurIPS 2019 orals:
	- Uniform convergence may be unable to explain generalization in deep learning. NIPS'19
		- https://www.youtube.com/watch?v=o3GfnEjTdIQ
- Zeyuan Allen-Zhu, Yuanzhi Li. Can SGD Learn Recurrent Neural Networks with Provable Generalization? NIPS'19

## Capacity
- Pierre Baldi. Neuronal Capacity. NIPS'18

## Dimensionality
- Zi Yin, Yuanyuan Shen. On the Dimensionality of Word Embedding. NIPS'18
	- PIP loss, bias-variance trade-off

## Sample Complexity
- Simon Du. How Many Samples are Needed to Estimate a Convolutional Neural Network? NIPS'18
