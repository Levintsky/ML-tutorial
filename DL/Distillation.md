# Knowledge Distillation

## Basics
- Problem setup: distill the knowledge of a large (teacher) network to a student net
- Geoffrey Hinton, Oriol Vinyals, Jeff Dean. Distilling the Knowledge in a Neural Network. NIPS"14

## Good Summaries
- https://zhuanlan.zhihu.com/p/51563760

## Attention Transfer
- Sergey Zagoruyko, Nikos Komodakis. Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer. ICLR'17
	- Insight: same activation

## FSP Matrix
- A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning. CVPR'17
	- Insight: mimic the layer relation between student and teacher
- Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, Hang Li. Learning to Rank: From Pairwise Approach to Listwise Approach. 
