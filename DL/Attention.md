# Attention Model

## NIPS'19
- Paul Michel, Omer Levy, Graham Neubig. Are Sixteen Heads Really Better than One?
- Yi Tay, Anh Tuan Luu, Aston Zhang, Shuohang Wang, Siu Cheung Hui. Compositional De-Attention Networks
- Joshua Tobin, Wojciech Zaremba, Pieter Abbeel. Geometry-Aware Neural Rendering
- Simao Herdade, Armin Kappeler, Kofi Boakye, Joao Soares. Image Captioning: Transforming Objects into Words
- Drew Hudson, Christopher Manning. Learning by Abstraction: The Neural State Machine
- Karlis Freivalds, Emīls Ozoliņš, Agris Šostaks. Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time
- Vighnesh Shiv, Chris Quirk. Novel positional encodings to enable tree-based transformers
- Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan. Self-attention with Functional Time Representation Learning
- Boris Knyazev, Graham W Taylor, Mohamed Amer. Understanding Attention and Generalization in Graph Neural Networks
