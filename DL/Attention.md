# Attention Model

## Basics
- Vanilla:
	- A very good intro: http://jalammar.github.io/illustrated-transformer/
	- Explanation with Pytorch: http://nlp.seas.harvard.edu/2018/04/03/attention.html
	- https://github.com/Kyubyong/transformer
	- Pytorch: https://github.com/jadore801120/attention-is-all-you-need-pytorch
- Basics:
	- MHA: x = x + mha(x), followed by a layer-norm (optional);
		- k, q, v = linear(x)
		- attn = softmax(kq)
		- out = linear(attn v)
	- Feedforward: x = x + ff(x)
		- ff: x -> FC -> GeLU -> FC -> layer-norm;
- Extra:
	- Gating: **GTrXL**: DeepMind. Stabilizing Transformers for Reinforcement Learning. '20
		- Add a gating layer;
- Causal?
	- Masking the future;
- Efficient:
	- https://github.com/lucidrains/linear-attention-transformer
	- Reduce O(T^2) cost:
		- Efficient Attention: Attention with Linear Complexities. CoRR'18
		- Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma. Linformer: Self-Attention with Linear Complexity. '20
		- Reformer: The Efficient Transformer. ICLR'20
- Position encoding:
	- Important since set op shuffle-invariant;
	- Fourier; ()
		- k: position in seq;
		- n: preset by user;
		- i: 0 <= i <= d/2;
		- d: pos-encoding dimension;
		- p(k, 2i) = sin[k / n^(2i/d)]
		- p(k, 2i+1) = cos[k / n^(2i/d)]
	- Relative pos bias;
	- Could be added or concat:
		- x = x + pos-enc
			- e.g. Swin: Attn(K,Q,V) = softmax(qk/d+B)V
			- B: learnable bias;
		- x = [x; pos-enc]
	- Relative linear position attention:
		- Brain. Self-Attention with Relative Position Representations. NAACL'18
	- Dependency syntax-based position:
		- X Wang, Z Tu, L Wang, S Shi. Self-Attention with Structural Position Representations. EMNLP'19
	- Rotational: RoFormer
		- https://www.kexue.fm/archives/8265
		- 相对位置(m, n): f(q, m) f(k, n) = g(q, k, m-n)
- Task token:
	- Used in BERT and VIT;
- Acceleration: O(n^2) -> O(n)
	- Performer: Google. Rethinking Attention with Performers. ICLR'21
	- Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention. AAAI'21

## Attention Modules
- Attention layers:
	- A Vaswani, et. al. Attention is all you need. NIPS'17
	- **Multi-Query-Attn**: N. Shazeer. Fast transformer decoding: One write-head is all you need. '19
		- Key/value share same head;
- Cross attention:
	- **Perceiver**: DeepMind. Perceiver: General Perception with Iterative Attention. ICML'21
		- Q from low-dim latent space: (n, d)
		- KV from high-dim image space: (N, D)
		- kqv-step:
			- Map both to same dim C, (n,C), (N,D)
			- Attention softmax: (n, N)
			- Value: (n, C)
		- Linear (C, d): (n, d)
	- **LSTR**: M Xu, Y Xiong, H Chen, X Li, W Xia, Z Tu, S Soatto. Long Short-Term Transformer for Online Action Detection. NeurIPS'21
		- https://xumingze0308.github.io/projects/lstr/
- Position Encoding:
	- Original in transformer:\
		<img src="/NLP/images/pos-enc.png" alt="drawing" width="500"/>	
	- FAIR. Convolutional Sequence to Sequence Learning. 2017
		- Learned Position Encoding;
	- Brain. Image Transformer. ICML'18
		- Fourier;
	- X Chu, B Zhang, Z Tian, X Wei, and H Xia. Do we really need explicit position encodings for vision transformers? arxiv'21
		- https://github.com/Meituan-AutoML/CPVT
	- Relative position encoding (as in Swin):
		- H Hu, J Gu, Z Zhang, J Dai, and Y Wei. Relation networks for object detection. CVPR'18
		- H Hu, Z Zhang, Z Xie, and S Lin. Local relation networks for image recognition. ICCV'19
		- Brain. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR'20
		- H Bao, L Dong, F Wei, W Wang, N Yang, X Liu, Y Wang, J Gao, S Piao, M Zhou, et al. Unilmv2: Pseudo-masked language models for unified language model pre-training. ICML'20

## Analysis
- Tao Y, Sun Q, Du Q, et al. Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling. NIPS'18
	- Insight: non-local attention has damping effect (eigenvalue decreases during training);
	- Change formulation to make up the damping;
- **Scaling** study:
	- J Kaplan, S McCandlish, T Henighan, T Brown, B Chess, R Child, S Gray, A Radford, J Wu, and D Amodei. Scaling laws for neural language models. '20

## Efficient Inference
- P Michel, O Levy, Gm Neubig. Are Sixteen Heads Really Better than One?. NeurIPS'19
	- https://github.com/pmichel31415/are-16-heads-really-better-than-1
	- At test time, a lot of head can be removed;
	- Greedy alg for pruning;

## NLP
- Good Summaries
	- https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html
	- https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html
	- https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html
	- https://zhuanlan.zhihu.com/p/109992475
- Legacy:
	- D Bahdanau, K Cho, and Y Bengio. Neural machine translation by jointly learning to align and translate. arxiv'14
	- **ELMO**: M Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer. Deep contextualized word representations. NAACL'18
		- https://allennlp.org/elmo
		- ELMo (Embeddings from Language Models)
		- bidirectional LSTM, predict next/last word
- Self-supervised learning:
	- Bidiretional: BERT, RoBERTa, SpanBERT;
	- Causal: GTP-1,2,3;
	- Check SSL/NLP for details;
- Efficient:
	- Neil Houlsby, et. al. Parameter-Efficient Transfer Learning for NLP. ICML'19
		- Adapter module
	- **Reformer**: N Kitaev, Ł Kaiser, A Levskaya. Reformer: The Efficient Transformer. ICLR'20
		- LSH instead of fc for speed;
- **Transformer-XL**: Z Dai, et. al. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. 2019
	- https://github.com/kimiyoung/transformer-xl
	- Insight: improved on GPT for long sequence;
	- Dynamic masking;
	- Fixed-length segment; hidden state reuse (no-gradient)
	- Relative position encoding;
- **XLNet**: Z Yang, et. al. XLNet: Generalized Autoregressive Pretraining for Language Understanding. 2019
	- https://github.com/zihangdai/xlnet
- S Sukhbaatar, E Grave, P Bojanowski, A Joulin. Adaptive Attention Span in Transformers. 2019
- **Universal Transformers**: M Dehghani, S Gouws, O Vinyals, J Uszkoreit, Ł Kaiser. Universal Transformer. ICLR'19
	- Transformer + RNN;
- Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.
- **Google-T5**: https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html
	- Transfer learning;

## Vision: Backbone
- X Wang, R Girshick, A Gupta, and K He. Non-local neural networks. CVPR'18
	- Task: video classification;
	- CNN on each frame, then transformer to catch long-range dependency;
- CNN+Transformer for aggregation/attention/...;
	- Y Yuan and J Wang. Ocnet: Object context network for scene parsing. arxiv'18
		- Task: semantic segmentation;
		- Model: transformer for context aggregation;
			- Global attention: divide pixels into subgroups and self-attention in each subgroup;
			- Local attention: in each local grid;
	- H Hu, Z Zhang, Z Xie, and S Lin. Local relation networks for image recognition. ICCV'19
		- Transformer as **local aggregation** layer;
		- SA for the weights;
	- Y Cao, J Xu, S Lin, F Wei, and H Hu. Gcnet: Non-local networks meet squeeze-excitation networks and beyond. ICCVW'19
		- https://github.com/xvjiarui/GCNet
		- SENet like hard attention instead of soft-attention to save memory;
		- Traditional NL-Net: HW x HW; GCNet: HW x 1 x 1; learn to weight (excite);
	- DANet: J Fu, J Liu, H Tian, Y Li, Y Bao, Z Fang, and H Lu. Dual attention network for scene segmentation. CVPR'19
		- https://github.com/junfu1115/DANet/
		- Task: semantic segmenation;
		- Two channels of SA, then fuse: Position Attention + channel Attention;
	- M Yin, Z Yao, Y Cao, X Li, Z Zhang, S Lin, and H Hu. Disentangled non-local neural networks. ECCV'20
		- Unary attention (SE-Net based) + binary attention (pairwise);
	- **BoTNet**: A Srinivas, T Lin, N Parmar, J Shlens, P Abbeel, and A Vaswani. Bottleneck transformers for visual recognition. arxiv'21
	- T Xiao, M Singh, E Mintun, T Darrell, P Dollár, R Girshick. Early Convolutions Help Transformers See Better. NeurIPS'21
- Pure attention & ViT series (no CNN):
	- P Ramachandran, N Parmar, A Vaswani, I Bello, A Levskaya, and J Shlens. Stand-alone self-attention in vision models. NIPS'19
	- I Bello, B Zoph, A Vaswani, J Shlens, and Q Le. Attention augmented convolutional networks, ICCV'19
	- H Zhao, J Jia, and V Koltun. Exploring self-attention for image recognition. CVPR'20
		- Compare two SA:
			- Pairwise: yi = ∑α(xi, xj) β(xj), treat as set, permutation-invariant;
			- Patchwise: yi = ∑α(X_Ri)j β(xj), not set or perm-inv, strictly more powerful;
	- **ViT**: A Dosovitskiy, et. al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR'21
		- https://github.com/google-research/vision_transformer
		- Requires Large training dataset pretraining;
	- **DeiT**: H Touvron, M Cord, M Douze, F Massa, A Sablayrolles, and H Jegou. Training data-efficient image transformers & distillation through attention. ICML'21
		- Improved on ViT and train with one machine in 3 days;
		- Token-based distillation, teacher-student network;
			- Hard label distillation on class and token labels;
	- ViT-FRCNN: J Beal, E Kim, E Tzeng, D H Park, A Zhai, and D Kislyuk. Toward transformer-based object detection. arxiv'20
		- CNN/ViT-encoder - Reshape as feature map - RPN;
	- S Zheng, J Lu, H Zhao, X Zhu, Z Luo, Y Wang, Y Fu, J Feng, T Xiang, P Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. arxiv'20
		- https://github.com/fudan-zvg/SETR
		- ViT-backbone, then upsampling for finer-res like FPN;
	- L Yuan, Y Chen, T Wang, W Yu, Y Shi, F Tay, J Feng, and S Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arxiv'21
		- Make training-from-scratch work, not requiring pretraining;
	- K Han, A Xiao, E Wu, J Guo, C Xu, and Y Wang. Transformer in transformer. NeurIPS'21
		- Local smaller patches (word);
		- Larger patches (sentences);
	- **PVT**: W Wang, E Xie, X Li, D Fan, K Song, D Liang, T Lu, P Luo, and L Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arxiv'21
		- https://github.com/whai362/PVT
- CNN-style attention:
	- Cordonnier, J.-B., Loukas, A., and Jaggi, M. On the relationship between self-attention and convolutional layers. ICLR'20
	- **Swin Transformer**: Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. ICCV'21
		- MARR prize winner; beats old SOTA a lot on all detection and segmentation tasks;
		- Key insight:
			- Shifted window: always linear to image size (ViT square), cyclic shift to speed up;
			- Multi-scale by patch merging;
			- Relative position bias;
		- For object detection:
			- HTC++, instaboost, multi-scale training, soft-NMS;
		- https://github.com/microsoft/Swin-Transformer
		- https://youtu.be/udY0GlYXXbE
- **Vision: Head (Detection/Segmentation)**
	- H Hu, J Gu, Z Zhang, J Dai, and Y Wei. Relation networks for object detection. CVPR'18
		- https://github.com/msracver/Relation-Networks-for-Object-Detection
		- Relative position encoding: log(dx/w), log(dy/h)
	- J Gu, H Hu, L Wang, Y Wei, and J Dai. Learning region features for object detection. ECCV'18
	- N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, and S Zagoruyko. End-to-End Object Detection with Transformers. ECCV'20
		- https://github.com/facebookresearch/detr
		- CNN ResNet-50 backbone 
		- encoder (with modified position enc) 
		- decoder (learned-fixed-queries) - FFN;
	- C Chi, F Wei, and H Hu. Relationnet++: Bridging visual representations for object detection via transformer decoder. NeurIPS'20
	- H Zhang, F Li, S Liu, L Zhang, H Su, J Zhu, L Ni, and H Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.
	- F Li, H Zhang, H Xu, S Liu, L Zhang, L Ni, H Shum. Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation.
		- https://github.com/IDEACVR/MaskDINO

## Performance
- Pure attention (ImageNet):
	- [P Ramachandran, NIPS'19]: 77%;
	- [H Zhao, CVPR'20]: 77.1% patchwise;
	- ViT: top-1 77.9%(ViT-B), 76.5%(ViT-L);
	- DeiT: top-1 79.8%(DeiT-S), 83.1%(DeiT-B);
	- Swin: top-1 81.3%(Swin-T), 84.5%(Swin-L), better than EffNet, RegNet, ViT, DeiT;
- Pure attention (detection):
	- ViT-FRCNN: Low perf;
	- Swin (COCO-mini-val): AP-box 58.0%, AP-Mask 50.4%, better than Cascade R-CNN, ATSS, RepPoint, Sparse R-CNN;
	- Swin (COCO-test): AP-box 58.7%, AP-Mask 51.1%
- Segmentation (segmentation ADE20k);
	- SETR: (low perf); ADE20K: 50.28% mIoU
	- Swin: 53.5 mIoU, 62.8 test score, based on UperNet (mmseg);

## Vision: others
- Rendering:
	- J Tobin, W Zaremba, P Abbeel. Geometry-Aware Neural Rendering. NeurIPS'19
- Recurrent attention:
	- V Mnih, N Heess, A Graves, et al. Recurrent models of visual attention. NIPS'14
	- J Fu, H Zheng, and T Mei. Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. CVPR'17
- Unclassified:
	- J Ba, V Mnih, and K Kavukcuoglu. Multiple object recognition with visual attention. arxiv'14
	- T Xiao, Y Xu, K Yang, J Zhang, Y Peng, and Z Zhang. The application of two-level attention models in deep convolutional neural network for fine-grained image classification. CVPR'15
	- M Chung, S Cho. Cram: Clued recurrent attention model. arxiv'18
	- A Ablavatski, S Lu, and J Cai. Enriched deep recurrent visual attention model for multiple object recognition. WACV'17
	- H Zheng, J Fu, T Mei, and J Luo. Learning multi-attention convolutional neural network for fine-grained image recognition. ICCV'17
	- F Wang, M Jiang, C Qian, S Yang, C Li, H Zhang, X Wang, and X Tang. Residual attention network for image classification. arxiv'17
	- Y Chen, Y Kalantidis, J Li, S Yan, J Feng. A2-Nets: Double Attention Networks. NIPS'18

## Bioinformatics
- Jumper'21 AlphaFold2

## Multimodal Attention
- Arandjelovic, R. and Zisserman, A. Objects that sound. ECCV'18
- DeepMind. Self-supervised multimodal versatile networks. NeurIPS'20
- UNITER: Learning UNiversal
Image-TExt Representations. ECCV'20