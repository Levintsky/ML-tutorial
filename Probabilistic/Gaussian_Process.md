# Gaussian Process

## Good Summaries
- https://katbailey.github.io/post/gaussian-processes-for-dummies/
- http://www.gaussianprocess.org/
- https://distill.pub/2019/visual-exploration-gaussian-processes/
- Tutorials
	- MacKay, D. J. C. (1998). Introduction to Gaussian processes. In C. M. Bishop (Ed.), Neural Networks and Machine Learning, pp. 133–166. Springer. 1998
	- Williams, C. K. I. (1999). Prediction with Gaussian processes: from linear regression to linear prediction and beyond. In M. I. Jordan (Ed.), Learning in Graphical Models, pp. 599–621. MIT Press. 1999
	- MacKay, D. J. C. Information Theory, Inference and Learning Algorithms. 2003
- GP for Optimization
	- https://krasserm.github.io/2018/03/19/gaussian-processes/
	- https://zhuanlan.zhihu.com/p/86386926
	- BoTorch (Bayesian Optimzation for Pytorch): https://botorch.org/

## Textbooks 
- Kevin Murphy's textbook
	- A GP defines a **prior over functions**, which can be converted into a posterior over functions once we have seen some data. Although it might seem difficult to represent a distrubtion over a function, it turns out that we only need to be able to define a distribution over the function’s values at a finite, but arbitrary, set of points, say (x1,...,xN). 
	- A GP assumes that p(f(x1),...,f(xN)) is jointly Gaussian, with some mean μ(x) and covariance ∑(x) given by ∑ij=k(xi,xj)
	, where k is a positive definite kernel function. The key idea is that if 
	xi and xj are deemed by the kernel to be similar, then we expect the output of the function at those points to be similar, too.
	- https://github.com/probml/pmtk3
- C Rasmussen, C Williams. Gaussian Processes for Machine Learning. 2006

## PRML Bishop, Chap 6.4
- Applications:
	- kriging (Cressie, 1993)
	- ARMA (autoregressive moving average) models, Kalman filters, and radial basis function network;
- Notations: y(x) = wφ(x), p(w) ~ N(0, α^(-1)I):
	- E[y] = ΦE[w]=0
	- cov[y] = E[yy'] = 1/α ΦΦ'
- GP for regression with noise: tn = yn + εn, y is noise free, t: noisy observation; marginal distribution of t:
	- p(t|y) = N(t|y, β^(-1)I)
	- C(xn,xm) = k(xn, xm) + β^(-1)δnm
- Predict t-N+1 with x-N+1:
	- p(t_N+1) = N(t_N+1|0, C_N+1)
	- C_N+1 = [CN k; k' c]; (covariance)
	- p(aN+1|tN) = ∫p(aN+1|a_1:N)p(aN|tN)da1:N
- Hyper-paramter learning: e.g., noise β;
	- lnp(t|θ) = -1/2ln|C| - 1/2 tC^(-1)t - N/2ln(2π)
	- ∂lnp(t|θ)/∂θi = -1/2 Tr(C^(-1)∂C/∂θi) + 1/2tC^(-1)∂C/∂θiC^(-1)
- ARD (Automatic relevance determination):
	- k(x,x') = θ0 exp{-1/2 Σηi(xi-xi')^2}
	- k(x,x') = θ0 exp{-1/2 Σ_i=1..D ηi(xi-xi')^2} + θ2 + θ3Σ_i=1..D xnixmi
- GP for classification:
	- Notation: logit a(x), logistic sigmoid y=σ(a):
		- p(t|a) = σ(a)^t (1-σ(a))^(1-t)
		- p(a_N+1) = N(a_N+1|0, C_N+1)
		- C(xn, xm) = k(xn,xm) + νδnm
		- p(t_N+1=1|tn) = ∫p(t_N+1=1|a_N+1)p(a_N+1)da_N+1, with p(t_N+1=1|a_N+1) = σ(a_N+1)
	- Another way: Laplacian approximation:\
		<img src="/Bayes/images/gp/gp-cls-6.png" alt="drawing" width="400"/>\
		<img src="/Bayes/images/gp/gp-cls-7.png" alt="drawing" width="400"/>\
		<img src="/Bayes/images/gp/gp-cls-8.png" alt="drawing" width="400"/>
- Connection with NN: for a broad class of prior distributions over w, the distribution of functions generated by a neural network will tend to a Gaussian process in the limit M approaches infitnity;
	- Neal, R. M. Bayesian Learning for Neural Networks. Springer. Lecture Notes in Statistics 118. 1996

## Theory
- D R. Burt, C E. Rasmussen, M van der Wilk. Rates of Convergence for Sparse Variational Gaussian Process Regression. ICML'19 best paper
	- Reduce O(NM^2+M^3) to  and O(NM+M^2)

## Deep GP
- Neil D Lawrence and Andrew J Moore. Hierarchical gaussian process latent variable models. ICML'07
- Andreas Damianou and Neil Lawrence. Deep gaussian processes. AISTATS'13
- David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding pathologies in very deep networks. AISTATS'14
- Thang Bui, Daniel Herna ́ndez-Lobato, Jose Hernandez-Lobato, Yingzhen Li, and Richard Turner. Deep gaussian processes for regression using approximate expectation propagation. ICML'16
- J. H. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. ICLR'18
- **Gpytorch**: Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration.

## Connection with Neural Network
- Radford M. Neal. Priors for infinite networks. 1994
	- Single hidden layer
- Radford M. Neal. Bayesian Learning for Neural Networks. 1994
- Christopher KI Williams. Computing with infinite networks. NIPS'97
- Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein. Deep Neural Networks as Gaussian Processes. ICLR'18
	- Exact equivalence between infinitely wide deep networks and GPs
	- Focus on exact Bayesian inference for regression tasks
	- https://github.com/brain-research/nngp
- Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohl-dickstein. Deep neural networks as gaussian processes. ICLR'18
- Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arxiv'18
- **CNN**: Adrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional networks as shallow gaussian processes. ICLR'19
- Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. ICLR'19

## Unclassified
- Orthogonally Decoupled Variational Gaussian Processes. NIPS'18
- Infinite-Horizon Gaussian Processes. NIPS'18
- Learning Gaussian Processes by Minimizing PAC-Bayesian Generalization Bounds. NIPS'18

## NIPS'19
- Haibin YU, Yizhou Chen, Bryan Kian Hsiang Low, Patrick Jaillet, Zhongxiang Dai. Implicit Posterior Variational Inference for Deep Gaussian Processes
- Rui Li. Multivariate Sparse Coding of Nonstationary Covariances with Gaussian Processes
- Siqi Liu, Milos Hauskrecht. Nonparametric Regressive Point Processes Based on Conditional Gaussian Processes
- Ian Char, Youngseog Chung, Willie Neiswanger, Kirthevasan Kandasamy, Oak Nelson, Mark Boyer, Egemen Kolemen, Jeff Schneider. Offline Contextual Bayesian Optimization
- Creighton Heaukulani, Mark van der Wilk. Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes
- Yusuke Tanaka, Toshiyuki Tanaka, Tomoharu Iwata, Takeshi Kurashima, Maya Okawa, Yasunori Akagi, Hiroyuki Toda. Spatially Aggregated Gaussian Processes with Multivariate Areal Outputs
- Armin Lederer, Jonas Umlauft, Sandra Hirche. Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control
