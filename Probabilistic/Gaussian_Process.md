# Gaussian Process

## Basics
- Problem: p(y∗|x∗,X,y) with Gaussian kernel priors;
- GP for regression:
	- Conditional Guassian: μ1|2 = Σ1|2(Λ11μ1 - Λ12(x2-μ2))
		- μ ~ k∗K^(-1)y
	- f∗ = k∗Ky^(-1)y = Σαiκ(xi, x∗), with α=Ky^(-1)y
	- Parameter learning: posterior with GD;
- GP for classification:
	- p(yi|xi) = σ(yif(xi)), f ∼ GP(0, κ)
	- Extra term of logp(y|f) in NLL;
- Connections:
	- GP ~ Bayesian linear learning with same kernel;
	- GP ~ NN with infinite width;
	- RKHS
- Learning: GP SVI;

## Good Summaries
- Tutorials
	- D. MacKay. Introduction to Gaussian processes. 1998
	- C. Williams. Prediction with Gaussian processes: from linear regression to linear prediction and beyond. 1999
	- D. MacKay. Information Theory, Inference and Learning Algorithms. 2003
	- C Rasmussen, C Williams. Gaussian Processes for Machine Learning. 2006
- GP for Optimization
	- https://krasserm.github.io/2018/03/19/gaussian-processes/
	- https://zhuanlan.zhihu.com/p/86386926
	- BoTorch (Bayesian Optimzation for Pytorch): https://botorch.org/
- Kevin Murphy's textbook
	- https://github.com/probml/pmtk3
- Good resources:
	- https://katbailey.github.io/post/gaussian-processes-for-dummies/
	- http://www.gaussianprocess.org/
	- https://distill.pub/2019/visual-exploration-gaussian-processes/

## GP (K-Murphy-15, PRML-6.4)
- 15.1 Introduction
	- p(y∗|x∗,X,y) = ∫p(y∗|f,x∗)p(f|X,y)df
	- A GP defines a **prior over functions**;
	- Assumption: p(f(x1),...,f(xN)) ~ N(μ(x), ∑), ∑ij=k(xi,xj)

## GPs for regression
- K-Murphy-15.2
- f(x) ∼ GP(m(x), κ(x, x'))
- m(x) = E[f(x)]
- κ(x,x') = E[(f(x)−m(x))(f(x')−m(x'))']
- p(f|X) = N(f|μ, K), with Kij = κ(xi,xj) and μ = (m(x1),...,m(xN)).
- 15.2.1 Predictions using noise-free observations
	- f ~ N(μ,  [K   K∗])
	- f∗   (μ∗, [K∗† K∗∗])
	- Posterior: conditional Gaussian
		- p(f∗|X∗,X,f) = N(f∗|μ∗,Σ∗)
		- μ∗ = μ(X∗) + K∗† K−inv (f−μ(X))
		- Σ∗ = K∗∗ - K∗† Kinv K∗
	- SE (squared exponential) kernel:
		- κ(x, x′) = σf^2 exp(−1/2l^2(x−x′)^2)
- 15.2.2 Predictions using noisy observations
	- Observed y is noisy: y = f(x) + ε
	- cov[y|X] = K + σy^2I := Ky;
		- [Ky  K∗]
		- [K∗† K∗∗]
	- p(f∗|x∗,X,y) = N(f|k∗† Ky-inv y, k∗∗−k∗† Kyinv k∗)
	- Estimated f∗ = k∗ Ky-inv
		- Equivalent to y = Σαiκ(xi, x∗), with α=Ky-inv y
- 15.2.3 Effect of the kernel parameters
- 15.2.4 Estimating the kernel parameters
	- MLE for marginal: p(y|X) = ∫p(y|f,X)p(f|X)df
	- Prior: p(y|X) ~ N(f|0, K)
	- p(y|f) = Πi N(yi|fi, σi^2)
	- logp(y|X)=logN(y|0,K) = −1/2yKy^(−1)y − 1/2log|K| − Nlog(2π)
	- First term: data fit; 2nd: complexity
	- ∂logp(y|X)/∂θj = 1/2 tr((αα'-Ky^(-1))∂Ky/∂θj)
- 15.2.5 Computational and numerical issues
	- Input: x, y, K, k∗, σy, estimate f∗, var[f∗], logp(y)
		- Cholesky to speedup [k∗† Ky-inv y], [k∗∗−k∗† Kyinv k∗]
	- L = Cholesky(Ky)
	- α = Linv† Linv y
	- E[f∗] = k∗† α
	- v = Linv k∗;
	- var[f∗] = κ(x∗,x∗) − v†v;
	- logp(y|X) = −1/2y†α − Σ_i logLii − N/2log(2π)
- 15.2.6 Semi-parametric GPs*
	- f(x) = β†φ(x)+r(x), with r(x) ∼ GP(0,κ(x,x'))

## GPs meet GLMs
- K-Murphy-15.3
- 15.3.1 Binary classification
	- p(yi|xi) = σ(yif(xi)), f ∼ GP(0, κ)
	- Posterior: l(f) = logp(y|f) + logp(f|X) 
	-      = logp(y|f) − 1/2y Kinv y − 1/2log|K| − Nlog(2π)
	- Loss: J(f) := −l(f)
		- g = −∇logp(y|f) + Kinv f
		- H = −∇∇logp(y|f) + Kinv = W + Kinv
		- p(f|X,y) ≈ N(fˆ, (Kinv +W)inv)
	- Computing the posterior predictive
		- E[f∗|x∗,X,y] = k∗†Kinv E[f|X, y] ≈ k∗† Kinv fˆ
	- Marginal likelihood:
		- logp(y|X) ≈ logp(y|fˆ) − 1/2fˆ† Kinv fˆ − 1/2log|K| − 1/2log|Kinv+W|
	- Numerically stable computation
- 15.3.2 Multi-class classification

## Connection with other methods
- K-Murphy-15.4
- 15.4.1 Linear models compared to GPs
	- Prior: w ~ N(0, Σ)
	- Bayesian linear regression ⇔ a GP with covariance κ(x, x′) = x†Σx′
- 15.4.2 Linear smoothers compared to GPs
	- f(x∗) = ∑wi(x∗)yi
	- GP: wi(x∗) = [Kyinv k∗]i
- 15.4.3 SVMs compared to GPs
	- Loss: J(f) = 2f†f + CΣ.i (1−yifi)+
		-        = 2f†f - Σ.i logp(yi|fi)
	- No such likelihood (KM-14.5.5)
- 15.4.4 L1VM and RVMs compared to GPs
- 15.4.5 Neural networks compared to GPs
	- p(y|x, θ) = Ber(y|σ(w† σ(Vx)))
	- Prior (i.i.d. Gaussian): b ~ N(0, σb^2) v ~ N(vj|0, σw^2)
	- Eθ[f(x)] = 0
	- Eθ[f(x)f(x′)] = σb^2 + H σv^2 Eu[g(x;u)g(x′;u)]
	- H → ∞, ~ GP
- 15.4.6 Smoothing splines compared to GPs
	- Optimization problem: J(f)=1/2σy^2 Σ(yi-f(xi))^2 + 1/2|f|H^2
		- with Hilbert function norm prior;
	- Solution must have the form f(x)=Σαiκ(xi, x) (representation theorem);
- 15.5 GP latent variable model
	- GP-LVM
- 15.6 Approximation methods for large datasets
- Applications:
	- kriging (Cressie, 1993)
	- ARMA (autoregressive moving average) models, Kalman filters, and radial basis function network;
- ARD (Automatic relevance determination):
	- k(x,x') = θ0 exp{-1/2 Σηi(xi-xi')^2}
	- k(x,x') = θ0 exp{-1/2 Σ_i=1..D ηi(xi-xi')^2} + θ2 + θ3Σ_i=1..D xnixmi

## Learning
- J. Hensman and N. Lawrence. Gaussian processes for big data through stochastic variational inference. NIPS Workshop'12/UAI'13
	- SVI with introduced u (m points in the same space of x) to decouple between examples;
		- Denoiting ⟨.⟩p expectation under p.
		- Noisy obs: p(y|f) ~ N(f, β^−1 I)
		- p(f|u) ~ N(Knm Kmm^-1 u, Knn - Knm Kmm^-1 Kmn)
		- p(u) ~ N(0, Kmm)
		- logp(y|u) = log⟨p(y|f)⟩p(f|u) ≥ ⟨logp(y|f)⟩p(f|u) := L1
		- logp(y|X) = log∫p(y|u)p(u)du ≥ log∫exp(L1)p(u)du := L2
		- μ.u = β Λ^-1 Kmm^-1 Kmn y
		- Λ.u = β K.mm^-1 Kmn Knm Kmm^-1 + Kmm^-1
	- Global var u, approx q(u)
		- logp(y|X) ≥ ⟨L1 + logp(u) - logq(u)⟩q(u) := L3
		- q(u) ~ N(u|m, S)
		- Natural gradient to optimize {m, S}.

## Theory
- D Burt, C Rasmussen, M v d Wilk. Rates of Convergence for Sparse Variational Gaussian Process Regression. ICML'19 best paper
	- Reduce O(NM^2+M^3) to  and O(NM+M^2)

## Deep GP
- Derive analytical GP kernels for 1-layer NN;
	- Infinite width N → ∞
		- z ~ GP(0, K)
		- K(x, x′) = σb^2 + σw^2 C(x, x′)
	- R Neal. Priors for infinite networks. 1994
		- Single hidden layer
	- R Neal. Bayesian Learning for Neural Networks. 1994
	- C Williams. Computing with infinite networks. NIPS'97
- GP-LVM:
	- Setup: unsupervised learning, obs Y ∈ R.d, from latent X ∈ R.q
	- N Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. NIPS'04
	- N Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. JMLR'05
	- N Lawrence and A Moore. Hierarchical gaussian process latent variable models. ICML'07
	- M Titsias. Variational learning of inducing variables in sparse Gaussian processes. JMLR'09
	- M Titsias and N Lawrence. Bayesian Gaussian process latent variable model. AISTATS'10
		- Introduce auxiliary u to handle non-linearity;
	- A Damianou and N Lawrence. Deep gaussian processes. AISTATS'13
		- Stacked-GP-LVM, introduce U to handle non-linearity;
	- D Duvenaud, O Rippel, R Adams, and Z Ghahramani. Avoiding pathologies in very deep networks. AISTATS'14
	- T Bui, D Hernandez-Lobato, J Hernandez-Lobato, Y Li, and R Turner. Deep gaussian processes for regression using approximate expectation propagation. ICML'16
- NNGP: J Lee, Y Bahri, R Novak, S Schoenholz, J Pennington, and J Sohl-dickstein. Deep neural networks as gaussian processes. ICLR'18
	- Exact equivalence between infinitely wide deep networks and GPs
	- Focus on exact Bayesian inference for regression tasks
	- Setup: zi(x) = bi + ∑.j Wij xj; xj = φ(zj)
	- Recursive kernels:
		- K0(x,x′) = σb^2 + σw^2(xx′/d.in)
		- Kl(x,x′) = σb^2 + σw^2 F.φ[Kl-1(x,x′), Kl-1(x,x), Kl-1(x′,x′)]
	- Predictive: D=P{(x1,t1),...(xn,tn)}, predict x∗ ~ N(μ∗, K∗)
		- K = [K.DD  Kx∗,D†]
		-     [Kx∗,D Kx∗,x∗]
		- μ∗ = Kx∗,D(K.DD + σε^2I)inv t
		- K∗ = Kx∗,x∗ - Kx∗,D (K.DD + σε^2I)inv Kx∗,D†
	- Numeric implementation: refer to SVI;
		- Generate pre-act [-umax,...,umax], var [0,...,smax]
		- Populate F with a lut for F.φ in grids;
		- For every pair (x, x′), bilinear interpolate lut;
	- https://github.com/brain-research/nngp
- Deep kernel learning:
	- M Al-Shedivat, A Wilson, Y Saatchi, Z Hu, and E Xing. Learning scalable deep kernels with recurrent structure. JMLR'17
	- A Wilson, Z Hu, R Salakhutdinov, and E Xing. Deep kernel learning. AISTATS'17
- Gpytorch: J R Gardner, G Pleiss, D Bindel, K Weinberger, and A Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. NeurIPS'18
	- https://gpytorch.ai/
- A Matthews, M Rowland, J Hron, R Turner, and Z Ghahramani. Gaussian process behaviour in wide deep neural networks. arxiv'18
- **CNN**: A Garriga-Alonso, C Rasmussen, and L Aitchison. Deep convolutional networks as shallow gaussian processes. ICLR'19
- R Novak, L Xiao, J Lee, Y Bahri, G Yang, J Hron, D Abolafia, J Pennington, and J Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. ICLR'19
