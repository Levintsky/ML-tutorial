# Graphical Models

## Basics
- DGM: (PRML-8, K Murphy-10)
	- Bayesian net:
	- Inference: counting p(x_h|x_v,θ) = p(x_h,x_v|θ)/p(x_v|θ)
	- Learning: NLL, non-convex if latent;
		- p(θ|D) ∝ p(D|θ)p(θ) = ∏p(D|θ) p(θ)
- UGM: MRF/CRF (PRML-8, K Murphy-19)
	- MRF: p(y|θ) = 1/Z(θ) exp(−∑E(yc|θc)), Gibbs distribution;
	- Learning:
		- p(y|θ)= 1/Z(θ) exp[Σ_c θc† φc(y)]
		- ∂l/∂θc = [1/N Σ_i φ_c(yi)] - E[φ_c(yi)]
	- Inference:
	- CRF (discriminative MRF): y: label, connected, latent; x: observed;
		- p(y|x, w) = 1/Z(x,w) ∏c ψc(yc|x, w)
		- Usually assume log-linear clique: ψc(yc|x, w) = exp(wc† φ(x, yc))
	- Learning:
		- l(w) = 1/N Σlogp(yi|xi, w) = 1/N Σi[Σc wc' φc(yi, xi) − log Z(w, xi)]
		- ∂l/∂wc = 1/N Σi[φc(yi, xi) - E[φc(y, xi)]]
	- Structured/Latent SVM;
- Inference: (PRML-8, K Murphy-20)
	- Exact inference:
		- VE;
		- Factor graph;
		- Sum-product/max-product: for posterior and MAP;
	- Variational inference (check VI);
- Examples:
	- Markov-HMM: (K Murphy-17)
		- Inference:
			- Maginal each state: apply junction-tree, a.k.a. forward/backward, (message-passing)
			- Most likely whole sequence: Viterbi (DP)
		- Learning: Baum-Welch
	- Spate Space Model: (K Murphy-18)
		- Inference: Kalman Filter, ADF,
		- Learning: Baum-Welch

## Directed Graphical Models (PRML: Chap 8.1, 8.2; Kevin Murphy Chap-10)
- 10.1 Introduction
	- 10.1.1 Chain rule
	- 10.1.2 Conditional independence
		- X⊥Y|Z ⇐⇒ p(X, Y |Z) = p(X|Z)p(Y|Z)
	- 10.1.3 Graphical models
	- 10.1.4 Graph terminology
	- 10.1.5 Directed graphical models
- 10.2 Examples
	- 10.2.1 Naive Bayes Classifier;
	- 10.2.2 MM, HMM;
	- 10.2.3 QMR (quick medical reference);
	- 10.2.4 Genetic linkage analysis
	- 10.2.5 Directed Gaussian graphical models
- 10.3 Inference:
	- p(x_h|x_v,θ) = p(x_h,x_v|θ)/p(x_v|θ) 
	- = p(x_h,x_v|θ) / Σx_h' p(x_h',x_v|θ); (normalize)
- 10.4 Learning:
	- θ = argmax log p(x_i,v|θ) + logp(θ)
	- 10.4.1 Plate notation
	- 10.4.2 With complete data:
		- Prior: p(θ) = ∏t=1..V p(θt)
		- Posterior: p(θ|D) = ∏t=1..V p(Dt|θt)p(θt)
	- 10.4.3 With missing data/latent, not convex;
- 10.5 Conditional independence properties of DGMs
	- I-map;
	- 10.5.1 d-separation and the Bayes Ball algorithm (global Markov properties)
		- 1. P contains a chain, s→m→t or s←m←t, where m ∈ E.
		- 2. P contains a tent or fork, s↙m↘t, where m ∈ E.
		- 3. P contains a collider or v-structure, s ↘m↙ t, where m ∉ E and nor is any descendant of m.
	- 10.5.2 Other Markov properties of DGMs
		- t ⊥ nd(t) \ pa(t)|pa(t)
		- t ⊥ pred(t) \ pa(t)|pa(t)
	- 10.5.3 Markov blanket and full conditionals
- 10.6 Influence (decision) diagrams

## Undirected Graphical Models (PRML, Chap 8.3, Kevin Murphy, Chap 19)
- PRML, Chap 8.3
	- MRF: p(x) = 1/Z ∏ ψc(xc)
	- CRF
	- M. Schmidt. UGM: A Matlab toolbox for probabilistic undirected graphical models.
		- http://www.cs.ubc.ca/~schmidtm/Software/UGM.html. 
- Kevin Murphy, Chap 19
- 19.1 Introduction
- 19.2 Conditional independence properties of UGMs
	- 19.2.1 Key properties
		- The set of nodes that renders a node t conditionally independent of all the other nodes in the graph is called t's Markov blanket: mb(t)
		- t ⊥ V \ cl(t)|mb(t)
		- cl(t) := mb(t) ∪ {t} is the closure of node t;
	- 19.2.2 An undirected alternative to d-separation
	- 19.2.3 Comparing directed and undirected graphical models
- 19.3 Parameterization of MRFs
	- 19.3.1 Hammersley-Clifford Theorem
	- 19.3.2 Representing potential functions
- 19.4 Examples of MRFs
	- 19.4.1 Ising model
		- logp(y) = −Σwst ys yt = −1/2y'Wy
		- logp(y) = −Σwst ys yt −Σbsys = −1/2y'Wy + by (bias term, external field)
	- 19.4.2 Hopfield networks
	- 19.4.3 Potts model
		- generalized Ising to multiple states
	- 19.4.4 Gaussian MRFs
		- p(y|θ) ∝ exp[ηy − 1/2y'Λy]
	- 19.4.5 Markov logic networks
- 19.5 Learning
	- 19.5.1 Training maxent models using gradient methods
		- p(y|θ)= 1/Z(θ) exp[Σ_c θc'φc(y)]
		- ∂l/∂θc = [1/N Σ_i φ_c(yi)] - E[φ_c(yi)]
	- 19.5.2 Training partially observed maxent models
		- p(y, h|θ) = 1/Z(θ) exp[Σ_c θc' φc(h, y)]
	- 19.5.3 Approximate methods for computing the MLEs of MRFs
	- 19.5.4 Pseudo-likelihood:
		- Product of full conditionals (compositive likelihood)
	- 19.5.5 Stochastic maximum likelihood: (minibatch)
		- ∇l(θ) = 1/N Σ_i[φ(yi) − E[φ(y)]] with batchsize B;
		- For E[φ(y)] sample y from p(y|θk), then MC estimator with S samples;
	- 19.5.6 Feature induction for maxent models
	- 19.5.7 Iterative proportional fitting (IPF)
- 19.6 Conditional random fields (CRFs)
	- Also a discriminative random field (Kumar and Hebert 2003), is just a version of an MRF where all the clique potentials are conditioned on input features
		- p(y|x, w) = 1/Z(x,w) ∏c ψc(yc|x, w)
		- x: feature, y: label;
	- 19.6.2 Applications of CRFs
		- Pos-tagging: ψt(yt|xt) discrimininative (NN, RVM); ψst(ys,yt): language bigram;
		- Noun phrase chunking;
		- NER;
		- Stereo vision;
	- 19.6.3 CRF training
		- l(w) = 1/N Σlogp(yi|xi, w) = 1/N Σi[Σc wc' φc(yi, xi) − log Z(w, xi)]
		- ∂l/∂wc = 1/N Σi[φc(yi, xi) - E[φc(y, xi)]]
- 19.7 Structural SVMs
	- 19.7.1 SSVMs: a probabilistic view
		- REL(w) = −logp(w) + Σi[log[Σy L(yi, y)p(y|xi, w)]]
		- p(y|x,w) = exp(wφ(x,y)) / Z(x,w)
		- p(w) = exp[-E(w)]/Z
		- Regard sum-log as a softmax
		- REL(w) ~ −E(w) + Σi[max_y[L(yi,y)+wφ(xi,y)] - wφ(xi,yi)]
	- 19.7.3 Cutting plane methods for fitting SSVMs
	- 19.7.4 Online algorithms for fitting SSVMs
	- 19.7.5 Latent structural SVMs
		- p(y,h|x,w) = exp(wφ(x,y,h)) / Z(x,w)
		- Z(x,w) = Σy,h exp[wφ(x,y,h)]
		- REL(w) ~ −E(w) + Σi[max_y,h[L(yi,y,h)+wφ(xi,y,h)]] - Σi max_h[wφ(xi,yi,h)]
		- Solvable by CCCP;

## Inference in Graphical Models (PRML, Chap 8.4, Kevin Murphy Chap-20)
- 20.1 Introduction
- 20.2 Belief propagation for trees
	- 20.2.1 Serial protocol
		- p(x|v) = 1/Z(v) ∏s ψs(xs) ∏s,t ψs,t(xs, xt)
		- Collect evidence bottom-up;
			- bel-t(xt) = p(xt|vt−) = 1/Zt ψt(xt) ∏c m−c→t(xt)
			- m−s→t(xt) = Σs ψst(xs, xt) bel−s(xs)
		- Root (with all the evidence)
			- bel(xr) = p(xr|v) = p(xt|vr−) ∝ ψr(xr) ∏c m−c→r(xr)
		- Message down:
			- bel(xs) = p(xs|v) ∝ bel−s (xs) m+t→s(xt)
		- Also: sum-product;
	- 20.2.2 Parallel protocol
	- 20.2.3 Gaussian BP
	- 20.2.4 Other BP variants
- 20.3 The variable elimination algorithm
	- Max-product
- 20.5 Computational intractability of exact inference in the worst case
	- 20.5.1 Approximate inference
- Exact inference:
	- Variable elimination;
	- Factor graph: clique ψ(a,b,c) like;
	- **Sum-product** algorithm (same as **belief propagation**, variable elimination):
		- Used to find marginal of a node p(x):
		- Belief propagation (Pearl, 1988) is a special case; sum-product for a node x first does sum of neighbor xb, then x takes all sum and do product;
	- Max-product: find x maximize the joint distribution;
- Exact inference:
	- Junction tree: sum-product, max-product produces efficient, exact inference on tree, so make graph a tree;
- Loopy BP;
- Learning grah structure;

## CRF (Directed or Undirected)
- Inference Proposals
	- MAP inference:
		- alpha-expansion
		- TRW-S
	- M-Best-MAP:
		- D. Nilsson. An efficient algorithm for finding the m most probable configurations in probabilistic expert systems. Statistics and Computing, vol. 8, pp. 159–173, 1998.
		- C. Yanover and Y. Weiss, Finding the m most probable configurations using loopy belief propagation. NIPS'03
		- D. Batra. An Efficient Message-Passing Algorithm for the M-Best MAP Problem. UAI'12
	- Generate a diverse set of proposals:
		- D. Batra, P. Yadollahpour, A. Guzman-Rivera, and G. Shakhnarovich. Diverse M-Best Solutions in Markov Random Fields. ECCV'12
		- **PDivMAP**: F. Meier, A. Globerson, and F. Sha. The More the Merrier: Parameter Learning for Graphical Models with Multiple MAPs. in ICML Workshop on Inferning: Interactions between Inference and Learning, 2013
		- C. Chen, V. Kolmogorov, Y. Zhu, D. Metaxas, and C. H. Lampert. Computing the m most probable modes of a graphical model. AISTATS'13
- Dense CRF:
	- P Krähenbühl, V Koltun. Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials. NIPS'12
	- DeepLab;
- Uncertainty measure:
	- G. Papandreou and A. L. Yuille, Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models. ICCV'11
	- D. Tarlow, R. P. Adams, and R. S. Zemel. Randomized optimum models for structured prediction. AISTATS'12
	- **MAP-perturbation**: S. Maji, T. Hazan, and T. Jaakkola. Active boundary annotation using random map perturbations. AAAI'14

## Exact Inference: Junction Tree
- Def: Clique tree with RIP;
- K.Murphy-20.4: The junction tree algorithm
	- Triangulated ⇔ junction tree;
- Inference: run message passing on the junction tree; (between clique)

## Example: Tree
- Model: p(x1,x2,...,xm) = ∏.s ψs(xs) ∏.s,t ψst(xs,xt)
- Marginal: μs(xs) = ∑.xs'=xs p(x1',x2',...,xm')
- Message Passing: (Jordan-08-2.5.1)
	- Sum-product: dynamic programming;
		- μs(xs) = κ ψs(xs) ∏.t∈N(s) M∗ts(xs)
		- Subtree: M∗ts(xs) = ∑.xVt' ψst(xs,xt) p(xVt';Tt)
		- Full graph: Mts(xs) ← κ ∑[ψst(xs,x′t)ψt(x′t) ∏.u∈N(t)/s Mut(x′t)]
		- Insight; message t to s: local clique, message from N(t) except s;
	- Full-graph:
		- Replace sum with max;

## Markov and HMM (Kevin Murphy Chap 17)
- 17.1 Intro;
- 17.2 Markov models:
	- p(X1:T) = p(X1) ∏p(Xt|Xt-1)
	- 2.1 Transition matrix A;
	- n-step: A(n), A(1)=A, A(m+n)=A(m)A(n);
	- 2.2 Language modeling:
		- p(Xt=k) unigram;
		- p(Xt=k|Xt-1=j): bigram;
		- p(Xt=k|Xt-1=j, Xt-2=i): trigram;
		- n-gram;
		- MLE: π^ = Nj/ΣNj, Ajk^ = Njk/Σ_k Njk;
		- Backoff smoothing: Dirichlet;
	- 2.3 stationary distribution
		- π=πA
- 17.3 HMM
	- Hidden state z1:T
	- p(z1:T,x1:T) = [p(z1)∏p(zt|zt-1)] [∏p(xt|zt)]
	- Obs model:
		- B(k,l) = p(xt=l|zt=k,θ); discrete
		- p(xt|z=k,θ) = N(xt|μk, Σk)
- 17.4 HMM inference
	- 17.4.2 Forward algorithm
		- p(zt=j|x1:t−1) = Σp(zt=j|zt−1=i)p(zt−1=i|x1:t−1)
		- Forward posterior:
			- αt(j) = p(zt=j|x1:t) = p(zt=j|xt, x1:t−1) 
			- = 1/Z p(xt|zt=j)p(zt=j|x1:t−1)
			- αt ∝ ψt ⊙ (Ψ αt−1)
			- with ψt(j) = p(xt|zt = j) as local evidence, Ψ(i,j) = p(zt = j|zt−1 = i)
	- 17.4.3 Forward-backward algorithm
		- Posterior of a state given all obs:
			- p(z=j|x1:T) ∝ p(zt=j,x_t+1:T|x1:t) 
			- ∝ p(zt=j|x1:t) p(x_t+1:T|zt=j)
		- First item: αt
		- Second item: βt(j) := p(xt+1:T|zt=j)
		- Posterior: γt(j) ∝ αt(j)βt(j)
		- βt−1 = Ψ(ψt ⊙ βt)
	- 17.4.4 Viterbi (most probable sequence)
		- z∗ = argmax_z2:T p(z1:T|x1:T)
		- Still forward-backward, but do dynamic programming;
		- Keep track of best so far up to t-1;
		- δt(j) = max δt−1(i)ψ(i, j)φt(j)
	- 17.4.5 Forwards filtering, backwards sampling
		- zs1:T ∼ p(z1:T|x1:T)
		- One-step back posterior:
			- p(zt=i|zt+1=j,x1:T)=φt+1(j)ψ(i,j)αt(i)/αt+1(j)
- 17.5 Learning for HMMs
	- 17.5.1 Training with fully obs data
	- 17.5.2 EM for HMMs (the Baum-Welch algorithm)
		- E-step: latent zt marginal:
			- γi,t(j) = p(zt=j|xi,1:Ti, θ)
			- ξi,t(j, k) = p(zt−1=j, zt=k|xi,1:Ti, θ)
		- M-step:
			- Transition for discrete: Ajk = E[N_jk]/ΣE_k'[N_jk']
			- μ, Σ for Gaussian;
	- 17.5.3 Bayesian fitting
		- VBEM: posterior rather than MAP
	- 17.5.4 Discriminative training
		- Implicit from generative model
	- 17.5.5 Model selection
		- Choosing the number of hidden states
		- Structure learning: learning a **sparse** transition matrix
- 17.6 Generalizations of HMMs
	- 17.6.1 Variable duration (semi-Markov) HMMs
		- 17.6.1.1 HSMM as augmented HMMs
	- 17.6.2 Hierarchical HMMs
	- 17.6.3 Input-output HMMs
	- 17.6.4 Auto-regressive and buried HMMs
	- 17.6.5 Factorial HMM
	- 17.6.6 Coupled HMM and the influence model
	- 17.6.7 Dynamic Bayesian networks (DBNs)

## 18 State space models (Kevin Murphy Chap 18)
- 18.1 Introduction
	- zt = g(at, zt−1, εt)
	- ot = h(zt, at, δt)
	- Important special case: all linear-Gaussian (LG-SSM):
		- Transition: zt = Atzt−1 + Btat + εt
		- Observation: ot = Ctzt + Dtat + δt
		- εt ∼ N(0,Qt), δt ∼ N(0,Rt)
- 18.2 Applications of SSMs
	- 18.2.1 SSMs for object tracking
	- 18.2.2 Robotic SLAM
	- 18.2.3 Online parameter learning using recursive least squares
	- 18.2.4 SSM for time series forecasting
		- ARMA
- 18.3 Inference in LG-SSM
	- 18.3.1 The Kalman filtering algorithm
		- p(zt|o1:t, a1:t) = N(zt|μt, Σt)
		- Prediction step: p(zt|o1:t-1, a1:t) ~ N(zt|μt|t-1, Σt|t-1)
			- zt|μt|t-1 = Atμt−1 + Btat
			- Σt|t-1 = AtΣt−1ATt + Qt
		- Measurement step
			- p(zt|o1:t, a1:t) ∝ p(ot|zt, at)p(zt|o1:t−1, a1:t) ~ N(zt|μt, Σt)
			- μt = μt|t−1 + Kt rt,
				- Observaton residualrt=ot−oˆt
				- o^ = E[ot|o1:t−1, a1:t] = Ctμt|t−1 + Dtut
			- Kt: Kalman gain matrix
				- Kt = Σt|t-1 Ct St^(-1)
				- St = Ct Σt|t-1 Ct' + Rt
		- Marginal likelihood of the sequence:
			- log p(o1:T|a1:T ) = Σ_t log p(ot|o1:t−1, a1:t)
			- p(ot|o1:t−1,a1:t) = N(ot|Ctμt|t−1, St)
		- Predictive: p(ot|o1:t−1, a1:t) ~ N(yt|Cμt|t−1 , CΣt|t−1 CT + R)
	- 18.3.2 The Kalman smoothing algorithm
		- Consider the future to reduce uncertainty
		- p(zt|o1:T) ~ N(μt|T, Σt|T)
		- μt|T = μt|t + Jt(μt+1|T − μt+1|t)
		- Σt|T = Σt|t + Jt(Σt+1|T − Σt+1|t)JTt
		- with Jt = Σt|t A't+1 Σt+1|t^(−1)
- 18.4 Learning for LG-SSM
	- Also systems identification in control theory
	- 18.4.1 Identifiability and numerical stability
	- 18.4.2 Training with fully observed data
	- 18.4.3 EM for LG-SSM
		- Baum-Welch
- 18.5 Approximate online inference for non-linear, non-Gaussian SSMs
	- 18.5.1 Extended Kalman filter (EKF)
		- Nonlinear models, Gaussian noise;
		- zt = g(at, zt−1) + N(0, Qt)
		- ot = h(zt) + N(0, Rt)
		- EKF: linearize g and h with 1st-order Taylor
	- 18.5.2 Unscented Kalman filter (UKF)
	- 18.5.3 Assumed density filtering (ADF)
		- Boyen-Koller algorithm for online inference in DBNs
		- Gaussian approximation for online inference in GLMs
- 18.6 Hybrid discrete/continuous SSMs
