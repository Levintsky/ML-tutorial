# EM, Variational Inference

## Basics
- Background:
	- Exponential family (Jordan-08-3);
	- Conjugate dual ;
- Exact inference:
	- Junction tree;
	- Variable elimination;
- Belief propagation: (Jordan-08-4, K-Murphy-22);
	- Bethe approximation;
	- Expectation propagation;
- Mean field method; (Jordan-08-5, PRML-10, K-Murphy-21);
- Parameter Estimation in VI; (Jordan-08-6);
- Convex relaxation; (Jordan-08-7);
- Mode computation; (Jordan-08-8);
- Conic programming; (Jordan-08-9);
- Techniques:
	- Loopy-BP:
		- Init ms→t=1, bel(xs)=1;
		- ms→t(xt) = Σs[ψs(xs)ψst(xs, xt) ∏u∉N(t) mu→s(xs)]
		- bel(xs) ∝ ψs(xs) ∏t mt→s(xs)
	- VI, ELBO:
		- KL(q(z) | p(z|x)) = E(log q(z)) - E(log p(z,x)) + log p(x)
		- logp(x) <= ELBO(q) = ⟨log p(z, x)⟩.q(z) - H(q)
			- logp(x) = log[∫p(x,z)dz]
			- = log[∫p(x,z)q(z)/q(z)dz]
			- = log[⟨logp(x,z)/q(z)⟩q(z)]
			- >= ⟨log[p(x,z)/q(z)⟩q(z) (Jensen, swap E[.] and log)
		- Special case: mean field;
			- Problem: known θ, infer z;
			- logqj(xj) ~ E.-qj[logp(x)]; marginalize out other var xi with qi(.);
			- qj(xj) = exp(E.-qj[lnp(x,z)]) / ∫exp(E.-qj[lnp(x,z)])dzj
	- VB: Learn θ;
		- p(θ|D) ~ Πqi(θi)
	- EM: infer z and learn θ; q(θ) approx, p(z) exact;
		- E-Step: posterior for z, p(z|θ)
		- M-step: fix p(z|θ), optimize θ;
	- VBEM/EM: both q(θ) q(z) approx;
		- p(θ,z1..N|D) ~ q(θ)Πqi(zi)
		- zi → xi ← θ.
		- Variational E-step: logq(z) by ∫q(θ)
		- Variational M-step: logq(θ) by ∫q(z)
	- CCCP:
		- Optimize minF(x) = f(x) - h(x), both f and h convex;
		- h(x) ≥ h(y) + ⟨∇h(y), x−y⟩. (h convex)
		- F(x) ≤ f(x) − h(y) − ⟨∇h(y), x−y⟩ =: G(x, y); upper bound
		- Alg:
			- xk+1 = argmin.x G(x, xk)
			- ∇f(xk+1) = ∇h(xk) each iteration;
		- EM: a special case of CCCP;
	- Sampling:
		- MCMC: Hard to do m-step, sample some z and average;
		- Gibbs sampling:
	- Black-box VI: used in VAE;
		- MC sample instead of integral to compute gradient;
- SVI:
	- p(z|x) = p(x|z)p(x) / ∫p(x|z)p(x)dz
	- Generally, the normalizer: ∫p(x|z)p(x)dz is not tractable;
	- Approx with q(z), minimize KL(q(z)|p(z|x))
	- KL = ⟨log[q(z)/p(z|x)]⟩q(z) = ⟨logq(z)⟩q - ⟨logp(z|x)⟩q
	-    = ⟨logq(z)⟩q(z) - ⟨log[p(x|z)p(z)]⟩q(z) + logp(x)
	- 3rd term logp(x) not related, dropped, we got negative ELBO;
	- SVI: sample a subset of dataset (Monte Carlo ∫)
	- Gradient of ELBO, two approaches to calculate E.q[∇.θ f(z)], with f(z) as ELBO in our case:
		- Reparametrization trick [ADVI] MC ∫
			- q(z; θ) -> q(g(z); ξ)
			- ⟨∇.θ f(ξ)⟩ξ
		- REINFORCE [BBVI] MC ∫
			- E.q(z;θ)[∇.θ logq(z;θ)[logp(x|z)p(z)-logq(z;θ)]]
- Examples:
	- Mixture model (PRML-9, K Murphy-11)
		- ln(∑(exp(.))) is **convex**, so Z(w) is always convex;
	- Gaussian;
	- Linear-regression;
	- LDA;
- Tutorials
	- M Wainwright and M Jordan, Graphical models, exponential families, and variational inference, Foundations and Trends in Machine Learning. 2008
	- D Blei,  M Jordan and J Paisley. Variational Bayesian inference with Stochastic Search. ICML'12
	- M Hoffman, D Blei, C Wang, and J Paisley. Stochastic variational inference. JMLR'13
	- D Blei. Variational Inference: A Review for Statisticians, 2018
		- ELBO (Evidence Lower-Bound)

## Sum-Product, Bethe–Kikuchi, and Expectation-Propagation
- Jordan-08-4
- Problem setup: pairwise-MRF + discrete
	- pθ(x) ∝ exp{∑θs(xs)+ ∑θst(xs,xt)} 
	- μs(xs) := ∑.j μs;j Is;j(xs);
	- μst(xs,xt) := ∑.jk μst;jk Ist;jk(xs, xt);
	- marginal polytope M(G)
		- The space of allowable μ: M(G)
		- where G is the structure of the graph defining UGM.
- Normalization condition:
	- ∑τs(xs) = 1
	- ∑.t τst(xs,xt) = τs
	- ∑.s τst(xs,xt) = τt
	- τ of L(G) as pseudomarginals;
- L(G): local consistent, M(G) ⊆ L(G)
- Bethe:
	- H(pμ) = −A∗(μ) = ∑Hs(μs) - ∑Ist(μst)
	- Bethe Approx: −A∗(τ) = ∑Hs(τs) - ∑Ist(τst)
- BVP (Bethe variational problem):
	- max.τ{⟨θ, τ⟩ + ∑Hs(τs) − ∑Ist(τst)}, Bethe free energy;
	- s.t. Css(τ) := 1 − ∑τs(xs)
	- s.t. Cts(xs,τ) := τs(xs) − ∑τst(xs,xt)
	- Lagrange with λss, λts(xs)
- Theo: Sum-Product and the Bethe Problem
	- a. fixed-point: ∇τ L(τ∗,λ∗;θ) = 0, and ∇λ L(τ∗,λ∗;θ) = 0;
	- b. tree MRF: exact solution and optimal A(θ).
- Two approximations:
	- Bethe entropy;
	- L(G) outer bound of M(G);
- Expectation-Propagation [T. Minka]:
	- Sufficient stat decoupling:
		- Tractable: φ := (φ1 ,φ2 ,...,φdT)
		- Intractable: Φ := (Φ1,Φ2,...,ΦdI)
	- Reformulate: p(x,θ,θ') = f0(x)exp⟨θ, φ(x)⟩∏exp(⟨θi, Φi(x)⟩).
	- base model: p(x;θ,0) ∝ f0(x)exp⟨θ, φ(x)⟩, no intractable part;
	- Φi-augmented: p(x;θ,θi) ∝ f0(x)exp⟨θ, φ(x)⟩ exp(⟨θi, Φi(x)⟩)
	- EP: marginalize base model, augment 1 at a time;
- K.Murphy
- 22.2 Loopy belief propagation: algorithmic issues
	- 22.2.2 LBP on pairwise models
		- Initialize message ms→t=1, beliefs bel(xs)=1;
		- ms→t(xt) = Σs[ψs(xs)ψst(xs, xt) ∏u mu→s(xs)]
		- bel(xs) ∝ ψs(xs) ∏t mt→s(xs)
	- 22.2.3 LBP on a factor graph
	- 22.2.4 Convergence
		- Damping: M̃k(xs) = λM(xs)+(1−λ)M̃k−1(xs)
		- TRP: pick a set of spanning tree, only up-down sweep update on tree, keep others fixed;
		- TRW:
	- 22.2.5 Accuracy of LBP
- 22.3 Loopy belief propagation: theoretical issues
	- 22.3.3 Exact inference as a variational optimization problem
		- L(q) = −KL(q||p)+logZ = Eq[logp'(x)]+H(q) ≤ logZ
		- max.μ∈M θ'μ + H(μ)
	- 22.3.4 Mean field as a variational optimization problem
		- inner approximation: MF(G) ⊆ M(G), some edges (sufficient stat) are set as zero;
	- 22.3.5 LBP as a variational optimization problem
		- Σx τs(xs) = 1; normalization constraint;
		- Σt τs(xs, xt) = τs(xs); marginalization constraint.
		- L(G) := {τ ≥ 0 : (1) holds ∀s ∈ V and (2) holds ∀(s, t) ∈ E}
	- 22.3.6 Loopy BP vs mean field
		- Mean field: concave functional on non-convex set;
			- max.μ θ†μ + H(μ) ≤ logZ(θ)
			- max.μ ∑.s∑.xs θ(xs)μ(xs) + ∑.E∑.xs,xt θ(xs,xt)μ(xs)μ(xt) + ∑.s H(μs) ≤ logZ(θ)
		- Message passing: non-concave functional on convex set;
			- L(τ,λ;θ) = θ†μ + H.Bethe(τ) + λss...
- 22.4 Extensions of belief propagation
	- 22.4.1 Generalized belief propagation
	- 22.4.2 Convex belief propagation
		- Tree-reweighted BP: weight Ist with ρst;
- 22.5 Expectation propagation
	- 22.5.1 EP as a variational inference problem
	- 22.5.2 Optimizing the EP objective using moment matching
	- 22.5.3 EP for the clutter problem
	- Expectation propagation: the **reverse form KL(p, q)**:
		- We use an exponential family q(z) to approx p(z):
			- q(z) = h(z)g(η)exp(η†u(z))
			- KL(p|q) = -lng(η) - η†E.p(z)[u(z)] + const
		- E.g. Moment matching; the cost:
			- KL(p|q) = KL(1/p(D) ∏fi(θi)|1/Z ∏fi'(θi))
- 22.6 MAP state estimation
	- 22.6.1 Linear programming relaxation
		- argmax_x∈X m θφ(x) = argmax_μ∈M(G) θ'μ
	- 22.6.2 Max-product belief propagation
	- 22.6.3 Graphcuts
		- Submodular: Euv(1, 1) + Euv(0, 0) ≤ Euv(1, 0) + Euv(0, 1)
	- 22.6.4 Experimental comparison of graphcuts and BP
	- 22.6.5 Dual decomposition

## Mixture Models/EM, (PRML-Chap-9, Kevin Murphy Chap-11)
- {X, Z}, posterior of z (E-step);
	- Q(θ,θ-old) = Σ_z p(z|X,θ-old)p(X,Z|θ)
	- θ-new = argmax_θ Q(θ,θ-old)
	- Similar to ELBO, q = p(z|X,θ-old) to approx p(z|X,θ)
- EM for Bayesian linear regression: converge to the same result with direct method:
	- logp(t,w|α,β) = logp(t|w,β) + logp(w|α)
	- p(α) ~ Gam(α|a0, b0)
	- p(t,w,α) = p(t|w)p(w|α)p(α)
- General EM:
	- p(X|θ) = Σ_z p(X,Z|θ); intractable;
	- logp(X|θ) = L(q,θ) + KL(q||p)
	- L(q,θ) = Σ_z q(z)log[p(x,z|θ)/q(z)]
	- KL(q||p) = Σ_z q(z)log[p(z|x,θ)/q(z)]
- 11.3 Parameter estimation for mixture models
	- 11.3.1 Unidentifiability
	- 11.3.2 Computing a MAP estimate is **non-convex**
		- log p(D|θ) = Σ_x log[Σ_z p(xi,zi|θ)]
- 11.4 The EM algorithm
	- 11.4.1 Basic idea
		- Complete likelihood: lc=Σlogp(xi,zi|θ)
		- Q(θ, θ-old) = E[lc(θ)|D, θ-old]
		- M-step: θ-new = argmax_θ Q(θ, θ-old)+logp(θ)
	- 11.4.7 Theoretical basis for EM
		- Lower bound: Jensen's inequality
		- ΣlogΣ > ΣΣlog, b/c log(.) concave;
		- ELBO: L(θ, qi) = -KL(qi(zi)||p(zi|xi,θ)) + logp(xi|θ)
	- 11.4.8 Online EM
		- Batch-EM: sgd-style parameter update;
		- Incremental EM: optimize q1, then q2, ...;
		- Stepwise-EM: paramter Polyak averaging;
	- 11.4.9 Other EM variants
		- Annealed EM: smooth the posterior by raising temperature, then gradually cooling;
		- Variational EM: q(zi) could be approximate;
		- Monte Carlo EM: draw samples, then sufficient statistics;
		- Generalized EM: partial M-step, increase ELBO rahter than maximization;
		- ECEM: conditional;
		- Over-relaxed EM: θt+1 = θt + η(M(θt)−θt), with aggressive η;
- 11.5 Model selection for latent variable models
	- K∗ = argmax.k p(D|K).
- 11.6 Fitting models with missing data
- Modern:
	- J Xu, D Hsu, A Maleki. Benefits of over-parameterization with EM. NIPS'18
	- W Lin, M Khan, M Schmid. Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-family Approximations. ICML'19
	- B Karimi, H Wai, E Moulines, M Lavielle. On the Global Convergence of (Fast) Incremental Expectation Maximization Methods. NIPS'19

## VI (Kevin Murphy-21)
- 21.1 Intro
- 21.2 VI
	- KL(p|q) = Σ_x p(x)log(p/q); (intractable, b/c Expectation over p(x))
	- KL(q|p) = Σ_x q(x)log(q/p);
	- Main goal: J(q) = KL(q||p)
	- Generally unnormalized p^(x)=p(x,D)=p(x)Z is tractable, in practice:
		- KL(q|p^) = KL(q|p) - logZ
	- 21.2.1 Alternative interpretation
		- J(q) = -H(q) + Eq[E(x)], E(x)=-logp^(x) energy
	- 21.2.2 Forward or reverse KL
		- KL(q|p): I-projection, zero avoiding
		- KL(p|q): mode-covering
- 21.3 Mean field
	- q(x) = Πqi(xi)
	- Goal: min_q1,... KL(q∥p)
		- D(θ1∥θ2) = A(θ2) - A(θ1) - ⟨μ1, θ2-θ1⟩
		- D(θ1∥θ2) ≡ D(μ1∥θ2) = A(θ2) + A∗(μ1) − ⟨μ1, θ2⟩
		- D(θ1∥θ2) ≡ A∗(μ1) − A∗(μ2) − ⟨θ2, μ1 − μ2⟩
	- logqj(xj) = E.-qj[logp(x)] = Σ_x-jΠ_i≠j qi(xi)logp(x)
	- 21.3.1 Derivation
		- L(q) = -J(q) = Σq(x)log(p^(x)/q(x))
		- qj(xj) ~ exp(E_-qj[logp(x)])
- 21.4 Structured Mean field
	- Exploit tractable substructure (no need to fully factorize)
- 21.7 Variational message passing and VIBES
	- One can then sweep over the graph, updating nodes one at a time, in a manner similar to Gibbs sampling. This is known as variational message passing or VMP (Winn and Bishop 2005)
	- Preferred in continuous z; discrete, use loopy-bp;
- 21.8 Local variational bounds
	- Simpler function to replace a specific term;

## Examples: Exponential Families
- e.g.1 Factorized Gaussian: (PRML-10)
	- To approx a coupled Gaussian:
		- μ = [μ1,μ2];
		- Λ=[Λ11 Λ12;Λ21 Λ22]
	- With factorized Gaussian:
		- q(z1) = N(z1|m1, Λ11^-1)
		- m1 = μ1 - Λ11^-1Λ12[E[z2]-μ2]
		- m2 = μ2 - Λ22^-1Λ21[E[z1]-μ1]
- e.g.2: VB for a univariate Gaussian (K.Murphy-25.1)
	- p(μ,λ) = N(μ|μ0,(κ0λ)−1)Ga(λ|a0,b0)
	- Approx with: q(μ, λ) = qμ(μ)qλ(λ)
- E.g.2: Gaussian, factorized prior (mean, precision) for conjugate prior:
	- Assume q(μ,τ)=q(μ)q(τ), we have
- E.g.3: exponential family;
	- Model: (x, z) with latent z, conjagate prior on η;
		- p(x,z|η) = h(x,z)g(η)exp(η†u(x,z))
		- p(η|ν0, v0) = f(ν0, x0)g(η)^ν0 exp(ν0 η†x)
	- Approx: q(z,η)=q(z)q(η)
	- q(zi) = h(xi,zi)g(E[η])exp(E[η†]u(x,z))
	- p(η) = f(νN, xN)g(η)^νN exp(η†xN)
		- νN = ν0 + N
		- xN = x0 + ∑E.z[u(x,z)]
- 11.4.5 EM for the Student distribution

## Examples: MRF
- E.g. Ising model: (K-Murphy-21)
	- Problem: x: hidden clean image; y: obs;
	- p(x) = 1/Z0 exp(-E0(x)), where E0(x)=-ΣΣWijxixj;
	- Likelihood: p(y|x) = Πp(yi|xi) = Σexp(-Li(xi))
	- Posterior: p(x|y) = 1/Z exp(-E0(x)+Li(x))
	- Approx: q(x) = Πqi(xi, μi), assuming μi as mean of xi;
	- q(xi) ∝ exp[xiΣWijμj + Li(xi)]

## Examples: MM/HMM
- e.g.: factorial HMM (K-Murphy-21.4.1)

## Examples: Mixture Model (K-Murphy-21)
- Generally: p(x|θ) = Σk πk pk(xi|θ)
- E.g. EM of GMM with Dirichlet prior on π (PRML 10.2),
	- Gaussian-Wishart prior on mean, precision;
- e.g. EM for GMM (K.Murphy-11)
	- p(x|θ) = Σk πk N(xi|μk,Σk)
- 11.2.2 Mixture of multinomial
	- p(xi|zi=k,θ) = Π_k Ber(xij|μjk)
- 11.2.3 Mixture for clustering
	- To infer p(zi=k|xi,θ)
	- Posterior: p(zi)p(x|zi,θ) / Σp(zi)p(x|zi,θ); soft
	- MAP: maxp(zi)p(x|zi,θ); hard clustering
- 11.2.4 Mixture for experts
	- Graphical model: xi->zi->yi (zi controls different experts); generative
	- p(yi|xi,zi=k,θ) = N(yi|wkxi,σk^2)
	- p(zi|xi,θ) = Cat(zi|S(Vxi)), zi posterior by generative likelihood
- 11.4.3 EM for mixture of experts
- 11.4.4 EM for DGM

## Generalizd Linear Model
- 11.4.6 EM for probit regression
- 21.5.2 Example: VBEM for linear regression
- 21.8.1.1. Variational logistic regression
	- p(y|X,w) = Πexp(yiηi-lse(ηi)), with ηi=Xiwi
		- log-sum-exp or lse(ηi)=log(1+Σexp(Σηim))
	- Not conjugate to Gaussian prior;
	- Approx with simpler function;

## Approximate Inference (PRML-Chap-10, Kevin-Murphy-Chap-21)
- Local variational method, **conjugate function**:\
	- For concave funtions, we could get **upper bound**:\
		<img src="/Bayes/images/VI/local-vi-3.png" alt="drawing" width="400"/>
	- Log(logistic) is concave, with upper bound:\
		<img src="/Bayes/images/VI/local-vi-4.png" alt="drawing" width="400"/>
	- Log(logistic) lower bound:\
		<img src="/Bayes/images/VI/local-vi-5.png" alt="drawing" width="400"/>
		<img src="/Bayes/images/VI/local-vi-6.png" alt="drawing" width="400"/>
		<img src="/Bayes/images/VI/local-vi-7.png" alt="drawing" width="400"/>

## Black-box/Stochastic VI
- BBVI: R Ranganath, S Gerrish, D Blei. Black Box Variational Inference. AISTATS'14
	- Basics behind VAE;
	- Assume q(z|λ) with λ as parameter;
	- ELBO: L(λ) = E_q(λ)[logp(x,z)-logq(z)]
	- ∇λL = Eq[∇λ logq(z|λ)(logp(x,z) − logq(z|λ))].
	- Noisy unbiased gradient of ELBO by MC:
		- Sample some z[s] ~ q(z|λ)
		- ∇λL ≈ 1/S Σ.s ∇λ logq(zs|λ)(logp(x, zs) − logq(zs|λ)),
- M Hoffman, D Blei, C Wang, J Paisley. Stochastic variational inference. JMLR'13
	- Insight: case of mini-batch, a global variable β to guarantee correctness; otherwise, x1, x2 is not independent of x3; minibatch won't work;
- ADVI: A Kucukelbir, D Tran, R Ranganath, A Gelman, D Blei. Automatic Differentiation Variational Inference. JMLR'17

## Conjugate Prior
- M Khan, W Lin. Conjugate-Computation Variational Inference: Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models. AISTATS'17

## Latent Models
- Binary latent factors:
	- Z Ghahramani and T Griffiths. Infinite latent feature models and the indian buffet process. NIPS'06
- Matrix Factorization:
	- A Mnih and R Salakhutdinov. Probabilistic matrix factorization. NIPS'08
- Clustering, co-clustering:
	- C Kemp, J Tenenbaum, T Griffiths, T Yamada, and N Ueda. Learning systems of concepts with an infinite relational model. AAAI'06
- Sparse coding:
- Dependent GSM:
	- Y Karklin and M Lewicki. Emergence of complex cell properties by learning to generalize in natural scenes. Nature'09
- Structure discovery (recursive search-based):
	- C Kemp and J Tenenbaum. The discovery of structural form. PNAS'08
	- R Grosse, R Salakhutdinov, W Freeman, and J Tenenbaum. Exploiting compositionality to explore a large space of model structures. UAI'12
