# Probability

## Basics
- Discrete
	- Bernoulli
	- Binomial <> Beta
	- Multinomial <> Dirichlet
	- Poisson
	- Empirical
- Continuous
	- Uniform
	- Gaussian
		- Conditional, marginal, linear Gaussian;
		- Learning: Gaussian-Gamma/IW;
	- Laplace
	- Gamma (Exponential, Erlang, χ-Square), Inverse-Gamma;
	- Beta
	- student-t: if var is unknown and estimated, then (x-μ)/σˆ ~ t()
	- F-distribution;
- Joint
- Conjugacy: posterior of the same functional form with prior
- Gaussian:
	- Posterior (single variable):
		- μ with known variance, Gaussian
		- μ known, precision λ=1/σ^2 ~ Gamma
		- Mean and precision unknown: Gaussian-Gamma
		- x ~ N(μ, τ^-1) with Gam(τ|a, b),
		- with unknown precision/covariance integrated out: student-t;
	- Posterior (multi-variable):
		- Mean and precision unknown: Gaussian-Wishart
		- covariance: inverse-Wishart
- Exponential family (GLM), check Linear-Model

## Preliminaries
- Joint Distribution (K-Murphy-2.5)
	- 2.5.1 Covariance and correlation
		- cov[X,Y] = E[(X−E[X])(Y−E[Y])] = E[XY]−E[X]E[Y]
	- Pearson correlation coefficient
		- ρ(X, Y) = cov(X, Y) / std(X)std(Y)
	- Marginal distribution
	- Conditional distribution
- Distribution
	- Z=X+Y (Convolution): fx * fy
	- Y/X: ∫ |x|fx(x)fy(xz) dx
	- XY: ∫ 1/|x| fx(x)fy(z/x) dx
	- max(X,Y): Fz(z)=Fx(z)Fy(z)
- Transformation (K-Murphy-2.6)
	- 2.6.1 Linear Transform
	- 2.6.2 General transformations
		- p(y)=p(x)|dx/dy|
		- Multivar: Jacobi p(y)=p(x)det(∂x/∂y)
- Theory:
	- 2.6.3 Central Limit Theorm:
		- X i.i.d., with mean=μ, std=σ, not necessarily Gaussian;
		- (Σx-nμ)/√(n)σ close to N(0, 1)
		- Proof: MGF same for the two; (Taylor)
	- Weak law of large numbers (Khinchin's law)
		- the sample average converges in probability towards the expected value: lim.n→∞ P(|X-μ|>ε) = 0
	- Chebyshev: P(|X-μ|>ε) ≤ σ^2/ε^2

## Exponential Family (Jordan-08-3, K.Murphy-9)
- Principle of max entropy with empirical expectation:
	- μαˆ := 1/n ∑i=1.n φα(Xi),
- Model expectation:
	- Ep[φα(X)] := ∫φα(x)p(x)ν(dx)
- Maximize Shannon entropy:
	- H(p) := -∫[logp(x)]p(x)ν(dx)
- Proof: (K-Murphy-9.2.6)
	- Maximize entropy s.t. Σ_x fk(x)p(x) = Fk (moment matching)
	- J(p, λ) = -Σp(x)log(x) + λ0(1-Σp(x)) + Σ_k λk(Fk-Σp(x)fk(x))
	- ∂J/∂p(x) = -1 - logp(x) - λ0 - Σ_k λkfk(x) = 0
	- p(x) ∝ exp(-Σλkfk(x)): Gibbs distribution;
- Examples:
	- Bernoulli, Multinoulli, Gaussian;
	- Ising model; (pairwise)
	- Potts model
- Potential functions or sufficient statistics: φα : Xm → R
	- Minimal
	- Overcomplete;
- pθ(x1,x2,...,xm) = exp{⟨θ, φ(x)⟩ − A(θ)}
	- In K.Murphy-9.2 as: p(x|θ) = 1/Z(θ) h(x) exp[θφ(x)] = h(x)exp[θφ(x)-A(θ)]
- A(θ) = logZ(θ); Log-Partition function, cumulant function;
	- A(θ) = log∫.x ⟨θ, φ(x)⟩ν(dx)
	- ∂A/∂θ = E[φ(x)]
	- ∇^2 A(θ) = cov[φ(x)]
	- ∇A : Ω → M bijective ⇔ if representation is minimal;
	- pair (θ,μ) is dually coupled if μ = ∇A(θ)
	- Theo: ∇A is onto the interior M◦, ∀ μ ∈ M◦, ∃ θ = θ(μ) ∈ Ω s.t. Eθ[φ(X)] = μ
- Mean parameters and Marginal Polytopes:
	- Mean parameters: μα = Ep[φα(X)] = φα(x)p(x)ν(dx)
	- All realizable: M := {μ∈Rd |∃p s.t. Ep[φα(X)]=μα ∀α∈I}
	- Convex polytope; Minkowski-Weyl theorem;
- Forward mapping: θ ∈ Ω to μ ∈ M;
- Backward mapping: μ ∈ M to θ ∈ Ω;
- Log-likehood: l(θ;X1n) := 1/nΣlogpθ(Xi) = ⟨θ, μ^⟩ − A(θ)
- Conjugate dual:
	- A∗(μ) := sup.θ {⟨μ, θ⟩ − A(θ)}
	- E.θ(μ)[φ(X)] = ∇A(θ(μ)) = μ
	- Theo: A∗(μ) = H(pθ(μ)) if μ ∈ M◦
		- A∗(μ) = +∞ if μ ∉ M
	- A(θ) = sup.μ ⟨θ, μ⟩ − A∗(μ)
	- bijective between θ ∈ Ω and μ ∈ M;

## Discrete (K.Murphy-2.3, 3.3, 3.4)
- 2.3.1 Binomial and Bernoulli
	- Bernoulli: Ber(x|θ) = θ^I(x=1) (1−θ)^I(x=0)
		- E(x) = p
		- V(x) = p(1-p)
	- Binomial: conjugacy prior is Beta;
		- Bin(m|N,μ) = C(N,m)μ^m(1-μ)^(N-m)
		- E(x) = np
		- V(x) = np(1-p)
		- Beta(a, b) ∝ x^(a-1) (1-x)^(b-1)
	- Learning:
		- Prior: p(θ) ∝ θ^γ1 (1−θ)^γ2 ~ Beta(θ|a, b)
		- Likelihood: p(D|θ) = θ^N1 (1−θ)^N0
		- Posterior: p(θ|N0, N1) ∝ Beta(θ|N1+a, N0+b)
		- Predictive: p(x^=1|D) = a/a+b
- 2.3.2 Multinomial: K-sided die (extending binomial)
	- Mu(x|n,θ) = (n,x1,...,xK)∏k θj^xj
	- Conjugate prior: Dirichlet
	- 3.4 Learning
		- Likelihood: p(D|θ) = ∏k θk^Njk
		- Prior: Dir(θ|α) = 1/B(α) ∏k θk^(αk-1)
			- C = 1/B(α) = Γ(α0)/Γ(α1)..Γ(αK)
		- Posterior: Dir(θ|α1 + N1,...,αK + NK)
- 2.3.3 Poisson:
	- Poi(x|λ) = exp(-λ) λ^x/x!
	- E[x] = Var(x) = λ
	- Physical meaning: x ~ Bin(n, p), n very large, np=λ, then k appearancd observes Poisson;
- 2.3.4 Empirical distribution;
	- p.emp(A) = 1/N Σδxi(A)

## Gaussian (2.4）
- 1d:
	- pdf: N(x|μ, σ^2) ~ 1/√(2πσ) exp(-1/2σ^2 (x-μ)^2)
	- cdf: Φ(x; μ, σ^2) := ∫N(z|μ, σ^2)dz
- Multi:
	- N(x|μ, Σ) = (2π)^(-d/2)|Σ|^(-1/2) exp[-1/2(x-μ)Σ^-1(x-μ)]
	- Precision matrix: Λ = inv(Σ);
- Eigen interpretation:
	- Σ = Σi λi uiui† (SVD)
	- Σ^-1 = Σi 1/λi uiui†
	- p(y) = p(x)|J| = ∏1/(2πλi)^1/2 exp(-yi^2/2λi)
- MLE:
	- μˆ = 1/N Σx
	- Σˆ = 1/N (Σxx') - μˆμˆ†
- **Conditional**
	- μ = [μ1]; Σ=[Σ11 Σ12]
	-     [μ2];   [Σ21 Σ22]
	- Λ = Σ^(-1) = [Λ11 Λ12]
	- 			   [Λ21 Λ22]
	- Posterior:
		- x1 ~ N(x1|μ1, Σ11)
		- x2 ~ N(x2|μ2, Σ22)
		- p(x1|x2) ~ N(x1|μ1|2, Σ1|2)
		- μ1|2 = Σ1|2(Λ11μ1 - Λ12(x2-μ2))
		- Σ1|2 = Σ11 - Σ12 Σ22^-1 Σ21 = Λ11^(-1)
	- Proof of Posterior
		- Theo 4.3.2: inverse of a partitioned matrix with Schur complements;
		- p(x1,x2) = p(x2)p(x1|x2)
- Linear Gaussian (Application of conditional)
	- p(x) ~ N(x|μ, Σx)
	- p(y|x) ~ N(y|Ax+b, Σy) noisy obs of x;
	- Posterior conditional: p(x|y) ~ N(x|μx|y, Σx|y)
		- (Σx|y)^-1 = Σx^-1 + A'Σy^(-1)A
		- μx|y = Σx|y[A†Σy^(-1)(y-b) + Σx^(-1)μx]
	- Marginal: p(y) ~ N(y|Aμx + b, Σy + AΣxA†)
- MAP, 1-dim (PRML):
	- σ2 known, prior μ ~ N(μ0, σ0):
		- Posterior: Gaussian μ ~ N(μN, σN)
	- μ known, precision λ=1/σ^2 ~ Gam(a0,b0)
		- p(λ|X) ~ Gam(a0-1+N/2, λ)
	- Integrate out precision λ or σ
		- (x-μ)/(s/√n) ~ t
	- μ, λ unknown: Gaussian-Gamma
		- p(μ, λ) ~ N(μ0,(βλ)^-1)Gam(λ|a,b)
		- p(μ, λ|X) Gaussian-Gamma
- MAP, multi-dim:
	- Wishart: known mean, unknown precision matrix Λ: (K-4.5)
		- Insight: multivar of Gamma
		- Wi(Λ|S,ν) = 1/Z |Λ|^(ν−D−1)/2 exp(−1/2 tr(ΛS^−1))
		- Normalizer: ZWi = 2^(νD/2) ΓD(ν/2) |S|ν/2
		- 1-d: Wi(λ|s^−1,ν) = Ga(λ|ν/2, s/2)
	- Inverse-Wishart: known mean, unknown covariance:
		- λ ∼ Ga(a, b), then 1/λ ∼ IG(a, b)
		- Σ^−1 ∼ Wi(S, ν) then Σ ∼ IW(S−1, ν + D + 1)
	- Student t: p(x) = T(x|μ, Σ, ν)
		- p(x) = C [1+1/ν (x-μ)Σ^-1(x-μ))]^(-(ν+d)/2)
			- V = νΣ
			- C = Γ((ν+d)/2)/Γ(ν/2) |πV|^(-1/2)
	- 4.6 Infer/MAP
		- Likelihood: p(D|μ) = N(x|μ, 1/NΣ)
		- Posterior:
			- p(μ|D, Σ) = N(μ|mN, VN)
			- VN^−1 = V0^(−1) + NΣ^(−1)
			- m = VN[Σ^(−1)(Nx) + V0^(−1)m0]
		- Posterior of Σ: IW
		- 4.6.3 Posterior distribution of μ and Σ
			- Likelihood: Gaussian p(D|μ,Σ)
			- Prior: p(μ,Σ) = N(μ|m0,V0)IW(Σ|S0,ν0)
			- Posterior: p(μ, Σ|D) = NIW(μ, Σ|mN, κN, νN, SN)
		- 4.6.3.8 Bayesian t-test
			- p(μ>μ0|D) = ∫μ0..∞ p(μ|D)dμ
			- p(μ|D) = T(μ|x, s^2/N, N−1)
			- t statistic: t=(x-μ0)/(s/√(N))

## Gamma Distribution (K-Murphy-2.4.4)
- Ga(T|a, b) = b^a/Γ(a) T^(a-1) exp(-Tb)
	- Gamma function: Γ(x) := ∫u^(x-1)exp(-u)du
	- mean = a/b, mode = a−1/b, var = a/b^2
- Special case: Exponential, Erlang and χ-square
	- Exponential: p(x) ~ Ga(x|1, λ) = exp(-x/θ)/θ
		- E(x) = θ, V(x) = θ^2
	- Erlang: Ga(x|2, λ)
	- χ-square: Ga(x|ν/2, 1/2)
		- zi i.i.d normal, Q = Σ_k zi^2
		- Q ~ χ^2(k)
- Inverse Gamma: X ~ Ga(a,b), then 1/X ~ IG(a,b)
	- IG(x|a,b) = b^a/Γ(a) x^(-a-1) exp(-b/x)
	- mean = b/a−1, mode = b/a+1,var = b^2/(a−1)^2(a−2),

## Other-Continuous (K.Murphy-2.4)
- Uniform U(a, b)
	- E(x) = (a+b)/2
	- V(x) = (b-a)^2/12
- Dirac delta δ(x)
- Laplace:
	- Lap(x|μ, b) = 1/2b exp(-|x-μ|/b)
	- mean = μ, mode = μ, var = 2b2
- Beta (K-2.4.5)
	- Beta(x|a,b) = 1/B(a,b) x^(a-1)(1-x)^(b-1)
	- B(a,b) = Γ(a)Γ(b)/Γ(a+b)
	- mean = a/a+b, mode = a−1/a+b-2, var= ab/(a+b)^2(a+b+1)
- Pareto (K-2.4.6)
	- Pareto(x|k,m) = k m^k x^(-k-1) I(x>=m)
- Standard t-distribution:
	- Definition: p(x) = C(1+x^2/ν)^(-(ν+1)/2)
	- C = Γ((ν+1)/2) / √(πν)Γ(ν/2)
- F distribution:
	- U1, U2 χ-square with dof d1, d2
	- X = (U1/d1) / (U2/d2)
- Periodic (PRML-2.3.8)
	- On a circle: average angle (with period 2π)
	- von Mises distribution: periodic generalization of Gaussian;
