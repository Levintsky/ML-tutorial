# View Synthesis

## Basics
- Task definition:
	- Synthesize a novel view from a different view point;
	- More general: 3D-aware 2D generation;
	- Roughly 3D Representation + Rendering; (a lot of trickes shared by 3d-representation);
- Common techniques:
	- Ray tracing (a framework, not an algorithm);
	- IBMR (Image-based modeling and rendering): Render a new view from some images;
	- Reflection, refraction:
- Approaches of novel view synthesis:
	- Direct output or from optical flow;
	- 3D voxel or mesh first;
	- Single depth;
	- Multiple plane (MPI)/Layered depth (LDI);
- Generally 2D rendered by volumetric rendering;
	- J Kajiya and B Herzen. Ray tracing volume densities. ACM SIGGRAPH'84
	- R Drebin, L Carpenter, and P Hanrahan. Volume rendering. SIGGRAPH'88
	- https://www.zhihu.com/search?type=content&q=volumetric%20rendering
- Good Tutorials:
	- CVPR'20:
		- https://youtu.be/LCTYRqW-ne8
		- https://youtu.be/JlyGNvbGKB8
	- SIGGRAPH'21:
		- https://youtu.be/otly9jcZ0Jg
		- https://youtu.be/aboFl5ozImM
	- ICCV'21 workshop: https://unsup3d.github.io/
- Neural rendering basics:
	- Regress it: code - (2dnet) - 2d-img; e.g.GQN
	- Make it real: code/3d-mesh/points - (CG) - 2d - (encoder-decoder)- 2d-img; DVP or DNR;
	- Regress & render: code - (2dnet) - 3dmesh/points/volume - (cg) - 2d img
	- step, sample & blend: 3d-space - (mlp) - (cg) - 2d-img;
	- Loss design:
		- Statistical: perceptual loss;
		- Adversarial;

## Rendering Basics
- Ray tracing:
	- Path tracing: ray tracing + Monte Carlo;
		- Sample according to BRDF;
	- Ray Casting:
		- First step of ray tracing;
		- also used in volumetric rendering;
	- Ray marching: always used in volumetric;
		- https://www.zhihu.com/search?type=content&q=ray%20marching
	- https://www.zhihu.com/question/29863225/answer/70728387
	- James T. Kajiya. Rendering equation. SIGGRAPH'83
	- D Immel, M Cohen, D Greenberg. A radiosity method for non-diffuse environments. SIGGRAPH'86
	- Systematically sample light sources at each hit
		- Don't just wait the rays will hit it by chance
	- Photon mapping: https://www.zhihu.com/search?type=content&q=photon%20map
	- E Veach, L Guibas. Optimally combining sampling techniques for Monte Carlo rendering. SIGGRAPH'95
- Color
	- Lambertian: (ideal matte, diffusely reflecting surface), same color regardless of observer's angle of view;
	- Reflection, refraction:
		- Douglas Enright, Stephen Marschner, Ronald Fedkiw. Animation and Rendering of Complex Water Surfaces. SIGGRAPH'02
		- Glossy Reflection: multiple reflection rays; polished surface;
- Hierarchical (kd-tree):
	- BVH (Bounding Volume Hierarchy);
	- Pros and cons of kd-tree:
		- Pros: Simple code, Efficient traversal, Can conform to data;
		- Cons: costly construction, not great if you work with moving objects;
	- Most people use the Surface Area Heuristic (SAH)
		- MacDonald and Booth. Heuristic for ray tracing using space subdivision. Visual Computer'90
	- W Hunt, W Mark, G Stoll. Fast kd-tree Construction with an Adaptive Error-Bounded Heuristic. IRT'06
	- K Zhou, Q Hou, R Wang, B Guo. Real-Time KD-Tree Construction on Graphics Hardware. SIGGRAPH Asia'08
	- Hard core efficiency: Ingo Wald's PhD thesis: http://www.sci.utah.edu/~wald/PhD/
- Differentiable:
	- Resources:
		- **Kaolin**: https://github.com/NVIDIAGameWorks/kaolin
		- **Tensorflow**: https://www.tensorflow.org/graphics/api_docs/python/tfg/rendering
		- **pytorch3d**: https://github.com/facebookresearch/pytorch3d
	- **Neural-Renderer**: H Kato, Y Ushiku, and T Harada. Neural 3D Mesh Renderer. CVPR'18
		- Problem: 1. single 2D image to mesh; 2. Mesh Editing;
		- Insight: differentiable; approximate GD; first mesh generative model;
		- http://hiroharu-kato.com/projects_en/neural_renderer.html
		- https://github.com/daniilidis-group/neural_renderer
		- Supervision: silhouette loss + smoothness loss;
		- Assumption: deform an existing mesh (not from scratch); preprocessed segmentation;
	- **Soft-Ras**: S Liu, T Li, W Chen, and H Li. Soft rasterizer: A differentiable renderer for image-based 3d reasoning. ICCV'19
		- https://github.com/ShichenLiu/SoftRas
		- Insight: soft assign pixel to faces, making the rasterization differentiable by soft aggregation;
	- DIB-R: NVIDIA. Learning to predict 3d objects with an interpolation-based differentiable renderer. NIPS'19
		- https://nv-tlabs.github.io/DIB-R/
		- Insight: Barycentric interpolation based;
		- Framework:
			- im -> CNN -> mesh, light, texture (3-heads);
			- mesh/light/texture -> DIB-R -> rendered image;
		- Loss: mask, rgb;
		- Applied Soft-Ras weighting to soft assign pixel;
		- Lighting: support Phong, Lambertian, Sphere Harmonics;
		- Application: 3D objects from single images; 3D-GAN;

## Mesh-based (Surface) View-Synthesis
- Topology-preserving;
- Pros and Cons:
	- Easy to render;
	- Hard to represent complex scenes;
- Lambertian:
	- M Waechter, N Moehrle, and M Goesele. Let there be color! large-scale texturing of 3d reconstructions. ECCV'14
- Non-Lambertian:
	- P Debevec, C Taylor, and J Malik. Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach. SIGGRAPH'96
	- D Wood, D Azuma, K Aldinger, B Curless, T Duchamp, D Salesin, and W Stuetzle. Surface light fields for 3d photography. SIGGRAPH'00
	- C Buehler, M Bosse, L McMillan, S Gortler, and M Cohen. Unstructured lumigraph rendering. SIGGRAPH'01

## Image Based Rendering: Multi Images/Warping/Alpha-Blending
- Insight: Rely on a set of two-dimensional images of a scene to generate a three-dimensional model and then render some novel views of this scene;
- Legacy:
	- Alpha-Compositing: Thomas Porter and Tom Duff. Compositing digital images. Computer graphics and interactive techniques, 1984.
	- S Chen and L Williams. View interpolation for image synthesis. SIGGRAPH'93.
	- S. E. Chen and L. Williams. View interpolation for image synthesis. 1993
	- P. E. Debevec, C. J. Taylor, and J. Malik. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. 1996
	- S. M. Seitz and C. R. Dyer. View morphing. 1996
	- M Levoy and P Hanrahan. Light field rendering. SIGGRAPH'96
		- Blending weight based on ray proximity;
	- J Shade, S Gortler, L He, and R Szeliski. Layered depth images. SIGGRAPH'98
	- D N Wood, D Azuma, K Aldinger, B Curless, T Duchamp, D Salesin, and W Stuetzle. Surface light fields for 3d photography. SIGGRAPH'00
	- A. Fitzgibbon, Y. Wexler, and A. Zisserman. Image-based rendering using image-based priors. IJCV'05
- Proxy geometry:
	- G Chaurasia, S Duchene, O Sorkine-Hornung, and G Drettakis. Depth synthesis and local warps for plausible image-based navigation. TOG'13
	- P Hedman, T Ritschel, G Drettakis, and G Brostow. Scalable inside-out image-based rendering. TOG'16
- Optical flow:
	- M Eisemann, B Decker, M Magnor, P Bekaert, E Aguiar, N Ahmed, C Theobalt, and A Sellent. Floating textures. CGF'08
	- D Casas, C Richardt, J Collomosse, C Theobalt, and A Hilton. 4d model flow: Precomputed appearance alignment for real-time 4d video interpolation. CGF'15
	- S Sun, M Huh, Y Liao, N Zhang, and J J Lim. Multi-view to novel view: Synthesizing novel views with self-learned confidence. ECCV'18
		- https://shaohua0116.github.io/Multiview2Novelview/
	- R Du, M Chuang, W Chang, H Hoppe,and A Varshney. Montage4d: Interactive seamless fusion of multiview video textures. ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, 2018
- Soft blending:
	- Eric Penner and Li Zhang. Soft 3D reconstruction for view synthesis. SIGGRAPH Asia, 2017
	- Gernot Riegler and Vladlen Koltun. Free view synthesis. ECCV'20
	- SVS: Gernot Riegler Vladlen Koltun. Stable view synthesis. ICCV'21
		- https://github.com/isl-org/StableViewSynthesis
		- Preprocess with COLMAP to get 3d-scaffold;
			- Intrinsics K, camera pose R, translation t;
		- Learn to fuse:
			- for a pixel, get direction and feature (vk, fk) in visible frames with UNet;
			- for a new direction u, aggregate aggr(u, {vk, fk}k)
- Mesh surface:
	- P Debevec, Y Yu, and G Borshukov. Efficient view-dependent image-based rendering with projective texture-mapping. Eurographics W'98
	- J Huang, J Thies, A Dai, A Kundu, C Jiang, L Guibas, M Nießner, and Thomas Funkhouser. Adversarial texture optimization from rgb-d scans. CVPR, 2020.
	- J Thies, M Zollhofer, and M Nießner. Deferred neural rendering: Image synthesis using neural textures. ACM TOG'19
- P Hedman, J Philip, T Price, J Frahm, G Drettakis, and G Brostow. Deep blending for free-viewpoint image-based rendering. SIGGRAPH Asia'18
	- Generate 2 MVS, then CNN to blend;
		- Michal Jancosek and Tomas Pajdla. Multi-view reconstruction preserving weakly-supported surfaces. CVPR'11
		- Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. CVPR'16
- **IBRNet**: Q Wang, T Funkhouser. Ibrnet: Learning multi-view image-based rendering. CVPR'21

## Unclassified
- **GQN**: Neural scene representation and rendering. Science 2018
	- https://deepmind.com/blog/neural-scene-representation-and-rendering/
	- Problem setup: novel view synthesis;
	- Encode: v1, v2, ... -> r1, r2;
		- Additive r = r1 + r2;
	- Novel view:
		- Latent z, query vi -> h
		- Render: h -> new view;
	- g(x|vq, r) = ∫g(x,z|vq, r)dz

## Explicit Voxel/PC/...
- Also refer to 3D-representation;
	- Explicit, topology-free;
- Voxels:
	- **Refer to Voxels in 3D-represenation**;
	- Octree:
		- Aaron Knoll. A survey of octree volume rendering methods. GI'06
	- N Kalantari, T Wang, and R Ramamoorthi. Learning-based view synthesis for light field cameras. ACM TOG, 2016
	- E Penner and L Zhang. Soft 3D reconstruction for view synthesis. SIGGRAPH Asia, 2017
	- S Tulsiani, T Zhou, A Efros,and J Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. CVPR'17
	- A Kar, C Hane, and J Malik. Learning a multi-view stereo machine. NeurIPS'17
	- DeepMind. Neural scene representation and rendering. Science'18
	- V Sitzmann, J Thies, F Heide, M Nießner, G Wetzstein, and M Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. CVPR'19
		- https://vsitzmann.github.io/deepvoxels/
		- Assume: camera intrinsics, poses known;
		- 2D feat extract;
		- Lifting layer -> GRU (state shared across training) -> Hole filling (UNet)
		- Projection -> occlusion net -> render
		- Supervision: L1-loss, adv-loss;
	- S Lombardi, T Simon, J Saragih, G Schwartz, A Lehrmann, and Y Sheikh. Neural volumes: Learning dynamic renderable volumes from images. TOG'19
		- encoder-decoder network end-to-end using a differentiable ray marching algorithm;
		- Encoder: three views (center, leftmost, rightmost) go through CNN to get 3,072dim, concat, fc, VAE+sample -> 256-dim + condition-var;
		- Decoder: Voxel V(x;z)=S(x-x0/W/2, g(z))
		- Warping field: to generate finer resolution;
	- K Olszewski, S Tulyakov, O Woodford, H Li, and L Luo. Transformable bottleneck networks. ICCV'19
		- https://kyleolsz.github.io/TB-Networks/
		- Problem: 3D manipulation;
		- Source -> 3D-recon voxel -> novel view synthesis;
		- Encoder: 2D - 3D;
		- Transformer-bottleneck: Target-pose -> resampling layer -> aggregated voxel feature;
		- Decoder: 2D + occupancy -> image;
	- P Henzler, N Mitra, and T Ritschel. Learning a neural 3d texture space from 2d exemplars. CVPR'20
		- https://geometry.cs.ucl.ac.uk/projects/2020/neuraltexture
- Layered/Multiplane:
	- Basics:
		- Represent the 3d scene with a few different layers with depth d1, d2, d3, ...;
	- C. L. Zitnick, S. B. Kang, M. Uyttendaele, S. Winder, and R. Szeliski. High-quality video view interpolation using a layered representation. TOG'04
	- J Flynn, I Neulander, J Philbin, and N Snavely. Deepstereo: Learning to predict new views from the world's imagery. CVPR'16
		- PSV: Plane sweep volume;
		- Each image (selection tower as depth prediction): conv-relu -> per-pixel softmax;
	- **LDI**: S Tulsiani, R Tucker, N Snavely. Layer-structured 3D Scene Inference via View Synthesis. ECCV'18
		- https://shubhtuls.github.io/lsi/
		- https://github.com/google/layered-scene-inference
	- T Zhou, R Tucker, J Flynn, G Fyffe, N Snavely. Stereo Magnification: Learning View Synthesis using Multiplane Images. SIGGRAPH'18
		- https://people.eecs.berkeley.edu/~tinghuiz/projects/mpi/
	- **MPI**: P Srinivasan, R Tucker, J Barron, R Ramamoorthi, R Ng, N Snavely. Pushing the Boundaries of View Extrapolation with Multiplane Images. CVPR'19
		- https://github.com/google-research/google-research/tree/master/mpi_extrapolation
	- J Flynn, M Broxton, P Debevec, M DuVall, G Fyffe, R Overbeck, N Snavely, R Tucker. DeepView: View synthesis with learned gradient descent. CVPR'19
		- https://augmentedperception.github.io/deepview/
	- B Mildenhall, P Srinivasan, R Ortiz-Cayon, N Kalantari, R Ramamoorthi, R Ng, and A Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. TOG'19
	- P Dai, Y Zhang, Z Li, S Liu, and B Zeng. Neural point cloud rendering via multi-plane projection. CVPR'20
	- Zhengqi Li, Wenqi Xian, Abe Davis, and Noah Snavely. Crowdsampling the plenoptic function. ECCV'20
- Point cloud:
	- K Aliev, A Sevastopolsky, M Kolos, D Ulyanov, and V Lempitsky. Neural point-based graphics. arxiv'19
	- M Meshry, D Goldman, S Khamis, H Hoppe, R Pandey, N Snavely, and R Brualla. Neural rerendering in the wild. CVPR'19
	- F Pittaluga, S Koppal, S B Kang, and S Sinha. Revealing scenes by inverting structure from motion reconstructions. CVPR'19

## Implicit Radience Field
- Basic idea:
	- Use a MLP to overfit color and occupancy of a scene, supervised by each image;
- X Chen, J Song, and O Hilliges. Monocular neural image based rendering with continuous view control. ICCV'19
	- Analysis
- NERF:
	- **SRN**: V Sitzmann, M Zollhofer, and G Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. NeurIPS'19
		- https://github.com/vsitzmann/scene-representation-networks
		- Implicit function: Phi(R3) -> Rn;
		- Neural render: Psi, ray-marching lstm;
	- **NERF**: B Mildenhall, P Srinivasan, M Tancik, J Barron, R Ramamoorthi, and R Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. ECCV'20
		- Insight: overfit a function mlp(x,y,z,θ,φ) to explain the scene;
		- Key techniques to improve novel view synthesis performance:
			- Positional encoding;
			- Multi-resolution;
	- K Park, U Sinha, J Barron, S Bouaziz, D Goldman, S Seitz, and R Martin-Brualla. Deformable neural radiance fields. arxiv'20
		- https://nerfies.github.io/
		- Problem setup:
			- Input: selfie videos; (people can move)
		- Insight: Optimize an additional continuous volumetric deformation field;
			- (x,y,z) -> MLP -> **deformation** field (x',y',z')
			- Nerf on the deformed view;
	- R Martin-Brualla, N Radwan, M Sajjadi, J Barron, A Dosovitskiy, and D Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. CVPR'21
		- https://nerf-w.github.io/
		- Problem setup:
			- Input: unstructured collected images from internet;
		- Embedding:
			- Static embedding;
			- **Transient** embedding; (moving human, ...)
			- Reconstruction + uncertainty; (to ignore transient parts)
	- P Srinivasan, B Deng, X Zhang, M Tancik, B Mildenhall, and J Barron. Nerv: Neural reflectance and visibility fields for relighting and view synthesis. CVPR'21
		- Problem setup: to enable **relighting**;
		- Neural Visibility Field;
	- M Guo, A Fathi, J Wu, T Funkhouser. Object-Centric Neural Scene Rendering. arxiv'20
		- https://www.shellguo.com/osf/
- NERF acceleration:
	- Add a prior (image cnn features!):
		- **GRF**: A Trevithick and B Yang. Grf: Learning a general radiance field for 3d scene representation and rendering. ICCV'21
			- https://github.com/alextrevithick/GRF
			- Similar to IBRNet, but use absolute coord;
			- Each image -> CNN -> feature;
			- Reproject feature -> (xp, yp, zp) -> Attention aggregation;
			- Volumetric rendering and supervision;
		- A Yu, V Ye, M Tancik, and A Kanazawa. pixelnerf: Neural radiance fields from one or few images. CVPR'21
			- Similar to IBRNet, with abs coord；
			- (x,y,z) + image-feat at (u,v) -> MLP -> color + opacity;
		- M Tancik, B Mildenhall, T Wang, D Schmidt, P Srinivasan, J Barron, and R Ng. Learned initializations for optimizing coordinate-based neural representations. CVPR'21
			- **Meta learning**: learn to initialize the network;
			- https://www.matthewtancik.com/learnit
	- D Rebain, W Jiang, S Yazdani, K Li, K M Yi, and A Tagliasacchi. Derf: Decomposed radiance fields. CVPR'20
		- Decompose into several small net;
		- Occupancy: σ(x)=∑ w(x) sig(x)
		- Color: c(x, d) = ∑ w(x) c(x,d)
	- **NSVF**: L Liu, J Gu, K Z Lin, T Chua, and C Theobalt. Neural sparse voxel fields. NeurIPS'20
		- https://lingjie0206.github.io/papers/NSVF/
		- https://github.com/facebookresearch/NSVF
		- Insight: Voxel bounded implicit field; progressive training;
		- Skip empty, 10x faster;
	- D Lindell, J Martel, and G Wetzstein. Autoint: Automatic integration for fast neural volume rendering. CVPR'21
		- http://www.computationalimaging.org/publications/
		- Modifies Nerf s.t. fewer samples required;
			- Grad net for fast integral;
		- Lower quality;
	- T Takikawa, J Litalien, K Yin, K Kreis, C Loop, D Nowrouzezahrai, A Jacobson, M McGuire, and S Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. CVPR'21
		- octree;
- Fast rendering;
	- P Hedman, P Srinivasan, B Mildenhall, J Barron, and P Debevec. Baking neural radiance fields for real-time view synthesis. ICCV'21
		- https://youtu.be/5jKry8n5YO8
	- S Garbin, M Kowalski, M Johnson,
	J Shotton, and J Valentin. Fastnerf: High-fidelity neural rendering at 200fps. arxiv'21
		- https://microsoft.github.io/FastNeRF/
		- Cache instead of neural net: decompose to avoid 5-dim cache;
			- position dependent;
			- direction dependent;
	- C Reiser, S Peng, Y Liao, and A
	Geiger. Kilonerf: Speeding up neural radiance fields with
	thousands of tiny mlps. arxiv'21
		- https://github.com/creiser/kilonerf
		- Thousands of small MLP instead of a big MLP;
	- Real-time rendering of NeRFs with PlenOctrees - Angjoo Kanazawa
		- https://unsup3d.github.io/
- Z Li, S Niklaus, N Snavely, and O Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. arxiv'20
- K Rematas, R Martin-Brualla, and V Ferrari. Sharf: Shape-conditioned radiance fields from a single view. arxiv'21
- Generative
	- M Fraccaro, D J Rezende, Y Zwols, A Pritzel, Eslami, and F Viola. Generative temporal models with spatial memory for partially observed environments. arxiv'18
	- P Henzler, N Mitra, and T Ritschel. Escaping plato's cave: 3d shape from adversarial rendering. ICCV'19
	- T Nguyen-Phuoc, C Li, L Theis, C Richardt, and Y Yang. Hologan: Unsupervised learning of 3d representations from natural images. ICCV'19
	- E Chan, M Monteiro, P Kellnhofer, J Wu, and G Wetzstein. pi-GAN: Periodic implicit generative adversarial networks for 3d-aware image synthesis. arxiv'20
	- M Niemeyer and A Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. arxiv'20
	- GRAF: K Schwarz, Y Liao, M Niemeyer, and A Geiger. GRAF: Generative radiance fields for 3d-aware image synthesis. NeurIPS'20
		- Insight: Generative model + NERF
			- Traditional: z-sampling -> image/voxel;
			- Proposed: z-sampling -> RF, RF + camera-pose -> image;
	- **SGN**: T DeVries, M Angel Bautista, N Srivastava, G Taylor, J Susskind. Unconstrained Scene Generation with Locally Conditioned Radiance Fields. ICCV'21
		- Global generator from a latent code z: W=g(z); with StyleGAN2;
		- Local radience field from an angle p: (σ, color)=f(wij, p); modulated linear layers similar to CIPS;

## Real-Time
- S Wizadwongsa, P Phongthawee, J Yenphraphai, and S Suwajanakorn. NeX: Real-time view synthesis with neural basis expansion. arxiv'21
- MVP: S Lombardi, T Simon, G Schwartz, M Zollhoefer, Y Sheikh, and J Saragih. Mixture of volumetric primitives for efficient neural rendering. SIGGRAPH'21

## Extroplate, Large Angle
- I Choi, O Gallo, A Troccoli, M Kim, and J Kautz. Extreme view synthesis. ICCV'19
- A Liu, R Tucker, V Jampani, A Makadia, N Snavely, and A Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image. arxiv'20

## Unclassified
- H Su, F Wang, E Yi, L Guibas. 3D-Assisted Feature Synthesis for Novel Views of an Object. ICCV'15
- J Yang. Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis. NIPS 2015
	- Input/output: images
	- https://github.com/jimeiyang/deepRotator
- M Tatarchenko, A Dosovitskiy, T Brox. Multi-view 3D Models from Single Images with a Convolutional Network. ECCV'16
	- Encoder/Decoder
	- Problem setup: Input RGB, output RGB/D conditioned on any shape input (angle, ...)
	- Separate model for 3D reconstruction (Point-cloud) from multiple RGBD, then mesh;
	- https://github.com/lmb-freiburg/mv3d
- T. Zhou, S. Tulsiani, W. Sun, J. Malik and A. Efros. View synthesis by appearance flow. ECCV'16
	- https://github.com/tinghuiz/appearance-flow
- A Dosovitskiy, J Springenberg, M Tatarchenko, T Brox. Learning to Generate Chairs, Tables and Cars with Convolutional Networks. PAMI'17
	- Input: class c, view v, transfrom param θ;
	- Output: image;
	- Conv, deconv, upsampling;
- J Xie, R Girshick, and A Farhadi. Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks. ECCV'16
	- Key insight: Unsupervised;
	- Input: left view; Output: right view
	- Evaluation: Kitti, NYU;
- E Park, J Yang, E Yumer, D Ceylan, and A Berg. Transformation-Grounded Image Generation Network for Novel 3d View Synthesis. 2017
- J Delanoy, M Aubry, P Isola, A Efros, A Bousseau. 3D Sketching using Multi-View Deep Volumetric Prediction. CGIT'18
- S Sun, M Huh, Y Liao, N Zhang, and J Lim. Multi-view to Novel view: Synthesizing Novel Views with Self-Learned Confidence. ECCV'18
	- Problem: novel view synthesis
	- Input: many images;
	- https://github.com/shaohua0116/Multiview2Novelview
- D Shin, Z Ren, E Sudderth, C Fowlkes. 3D Scene Reconstruction with Multi-layer Depth and Epipolar Transformers. ICCV'19
	- https://research.dshin.org/iccv19/multi-layer-depth/
- **SynSin**: O Wiles, G Gkioxari, R Szeliski, J Johnson. SynSin: End-to-end View Synthesis from a Single Image. CVPR'20 submission
	- http://www.robots.ox.ac.uk/~ow/synsin.html

## Disentangled
- **3D-SDN**: S Yao, H Hsu, J Zhu, J Wu, A Torralba, W Freeman, J Tenenbaum. 3D-Aware Scene Manipulation via Inverse Graphics. NIPS'18
	- http://3dsdn.csail.mit.edu/
	- https://github.com/ysymyth/3D-SDN
	- Algorithm:\
		<img src = '/Composition/images/2d/3d-sdn.png' width = '400'>
