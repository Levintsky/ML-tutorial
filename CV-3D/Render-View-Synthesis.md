# View Synthesis

## Basics
- Task definition:
	- Synthesize a novel view from a different view point;
	- More general: 3D-aware 2D generation;
	- Roughly 3D Representation + Rendering; (a lot of trickes shared by 3d-representation);
- Common techniques:
	- Ray tracing (a framework, not an algorithm);
	- IBMR (Image-based modeling and rendering): Render a new view from some images;
	- Reflection, refraction:
- Approaches of novel view synthesis:
	- Direct output or from optical flow;
	- 3D voxel or mesh first;
	- Single depth;
	- Multiple plane (MPI)/Layered depth (LDI);
- Generally 2D rendered by volumetric rendering;
	- James T Kajiya and Brian P Von Herzen. Ray tracing volume densities. ACM SIGGRAPH'84
	- Robert A Drebin, Loren Carpenter, and Pat Hanrahan. Volume rendering. SIGGRAPH'88
	- https://www.zhihu.com/search?type=content&q=volumetric%20rendering
- Good Tutorials:
	- CVPR'20:
		- https://youtu.be/LCTYRqW-ne8
		- https://youtu.be/JlyGNvbGKB8
	- SIGGRAPH'21:
		- https://youtu.be/otly9jcZ0Jg
		- https://youtu.be/aboFl5ozImM
	- ICCV'21 workshop: https://unsup3d.github.io/
- Neural rendering basics:
	- Regress it: code - (2dnet) - 2d-img; e.g.GQN
	- Make it real: code/3d-mesh/points - (CG) - 2d - (encoder-decoder)- 2d-img; DVP or DNR;
	- Regress & render: code - (2dnet) - 3dmesh/points/volume - (cg) - 2d img
	- step, sample & blend: 3d-space - (mlp) - (cg) - 2d-img;
	- Loss design:
		- Statistical: perceptual loss;
		- Adversarial;

## Rendering Basics
- Ray tracing:
	- Path tracing: ray tracing + Monte Carlo;
		- Sample according to BRDF;
	- Ray Casting:
		- First step of ray tracing;
		- also used in volumetric rendering;
	- Ray marching: always used in volumetric;
		- https://www.zhihu.com/search?type=content&q=ray%20marching
	- https://www.zhihu.com/question/29863225/answer/70728387
	- James T. Kajiya. Rendering equation. SIGGRAPH'83
	- David S. Immel, Michael F. Cohen, Donald P Greenberg. A radiosity method for non-diffuse environments. SIGGRAPH'86
	- Systematically sample light sources at each hit
		- Don't just wait the rays will hit it by chance
	- Photon mapping: https://www.zhihu.com/search?type=content&q=photon%20map
	- Eric Veach, Leonidas John Guibas. Optimally combining sampling techniques for Monte Carlo rendering. SIGGRAPH'95
- Color
	- Lambertian: (ideal matte, diffusely reflecting surface), same color regardless of observer's angle of view;
	- Reflection, refraction:
		- Douglas Enright, Stephen Marschner, Ronald Fedkiw. Animation and Rendering of Complex Water Surfaces. SIGGRAPH'02
		- Glossy Reflection: multiple reflection rays; polished surface;
- Hierarchical (kd-tree):
	- BVH (Bounding Volume Hierarchy);
	- Pros and cons of kd-tree:
		- Pros: Simple code, Efficient traversal, Can conform to data;
		- Cons: costly construction, not great if you work with moving objects;
	- Most people use the Surface Area Heuristic (SAH)
		- MacDonald and Booth. Heuristic for ray tracing using space subdivision. Visual Computer'90
	- Warren Hunt, William R. Mark, Gordon Stoll. Fast kd-tree Construction with an Adaptive Error-Bounded Heuristic. IRT'06
	- Kun Zhou, Qiming Hou, Rui Wang, Baining Guo. Real-Time KD-Tree Construction on Graphics Hardware. SIGGRAPH Asia'08
	- Hard core efficiency: Ingo Wald's PhD thesis: http://www.sci.utah.edu/~wald/PhD/
- Differentiable:
	- Resources:
		- **Kaolin**: https://github.com/NVIDIAGameWorks/kaolin
		- **Tensorflow**: https://www.tensorflow.org/graphics/api_docs/python/tfg/rendering
		- **pytorch3d**: https://github.com/facebookresearch/pytorch3d
	- **Neural-Renderer**: Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3D Mesh Renderer. CVPR'18
		- Problem: 1. single 2D image to mesh; 2. Mesh Editing;
		- Insight: differentiable; approximate GD; first mesh generative model;
		- http://hiroharu-kato.com/projects_en/neural_renderer.html
		- https://github.com/daniilidis-group/neural_renderer
		- Supervision: silhouette loss + smoothness loss;
		- Assumption: deform an existing mesh (not from scratch); preprocessed segmentation;
	- **Soft-Ras**: Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for image-based 3d reasoning. ICCV'19
		- https://github.com/ShichenLiu/SoftRas
		- Insight: soft assign pixel to faces, making the rasterization differentiable by soft aggregation;
	- **DIB-R**: Wenzheng Chen, Jun Gao, Huan Ling, Edward Smith, Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learning to predict 3d objects with an interpolation-based differentiable renderer. NIPS'19
		- https://nv-tlabs.github.io/DIB-R/
		- Insight: Barycentric interpolation based;
		- Applied Soft-Ras weighting to soft assign pixel;
		- Lighting: support Phong, Lambertian, Sphere Harmonics;
		- Application: 3D objects from single images; 3D-GAN;

## Mesh-based (Surface) View-Synthesis
- Topology-preserving;
- Pros and Cons:
	- Easy to render;
	- Hard to represent complex scenes;
- Lambertian:
	- Michael Waechter, Nils Moehrle, and Michael Goesele. Let there be color! large-scale texturing of 3d reconstructions. ECCV'14
- Non-Lambertian:
	- Paul E Debevec, Camillo J Taylor, and Jitendra Malik. Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach. SIGGRAPH'96
	- Daniel N Wood, Daniel I Azuma, Ken Aldinger, Brian Curless, Tom Duchamp, David H Salesin, and Werner Stuetzle. Surface light fields for 3d photography. SIGGRAPH'00
	- Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured lumigraph rendering. SIGGRAPH'01

## Image Based Rendering: Multi Images/Warping/Alpha-Blending
- Insight: Rely on a set of two-dimensional images of a scene to generate a three-dimensional model and then render some novel views of this scene;
- Legacy:
	- Alpha-Compositing: Thomas Porter and Tom Duff. Compositing digital images. Computer graphics and interactive techniques, 1984.
	- Shenchang Eric Chen and Lance Williams. View interpolation for image synthesis. SIGGRAPH'93.
	- S. E. Chen and L. Williams. View interpolation for image synthesis. 1993
	- P. E. Debevec, C. J. Taylor, and J. Malik. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. 1996
	- S. M. Seitz and C. R. Dyer. View morphing. 1996
	- M Levoy and P Hanrahan. Light field rendering. SIGGRAPH'96
		- Blending weight based on ray proximity;
	- J Shade, S Gortler, L He, and R Szeliski. Layered depth images. SIGGRAPH'98
	- D N Wood, D Azuma, K Aldinger, B Curless, T Duchamp, D Salesin, and W Stuetzle. Surface light fields for 3d photography. SIGGRAPH'00
	- A. Fitzgibbon, Y. Wexler, and A. Zisserman. Image-based rendering using image-based priors. IJCV'05
- Proxy geometry:
	- G Chaurasia, S Duchene, O Sorkine-Hornung, and G Drettakis. Depth synthesis and local warps for plausible image-based navigation. TOG'13
	- P Hedman, T Ritschel, G Drettakis, and G Brostow. Scalable inside-out image-based rendering. TOG'16
- Optical flow:
	- M Eisemann, B Decker, M Magnor, P Bekaert, E Aguiar, N Ahmed, C Theobalt, and A Sellent. Floating textures. CGF'08
	- D Casas, C Richardt, J Collomosse, C Theobalt, and A Hilton. 4d model flow: Precomputed appearance alignment for real-time 4d video interpolation. CGF'15
	- S Sun, M Huh, Y Liao, N Zhang, and J J Lim. Multi-view to novel view: Synthesizing novel views with self-learned confidence. ECCV'18
		- https://shaohua0116.github.io/Multiview2Novelview/
	- R Du, M Chuang, W Chang, H Hoppe,and A Varshney. Montage4d: Interactive seamless fusion of multiview video textures. ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, 2018
- Soft blending:
	- Eric Penner and Li Zhang. Soft 3D reconstruction for view synthesis. SIGGRAPH Asia, 2017
	- Gernot Riegler and Vladlen Koltun. Free view synthesis. ECCV'20
	- SVS: Gernot Riegler Vladlen Koltun. Stable view synthesis. ICCV'21
		- https://github.com/isl-org/StableViewSynthesis
		- Preprocess with COLMAP to get 3d-scaffold;
			- Intrinsics K, camera pose R, translation t;
		- Learn to fuse:
			- for a pixel, get direction and feature (vk, fk) in visible frames with UNet;
			- for a new direction u, aggregate aggr(u, {vk, fk}k)
- Mesh surface:
	- P Debevec, Y Yu, and G Borshukov. Efficient view-dependent image-based rendering with projective texture-mapping. Eurographics W'98
	- J Huang, J Thies, A Dai, A Kundu, C Jiang, L Guibas, M Nießner, and Thomas Funkhouser. Adversarial texture optimization from rgb-d scans. CVPR, 2020.
	- J Thies, M Zollhofer, and M Nießner. Deferred neural rendering: Image synthesis using neural textures. ACM TOG'19
- P Hedman, J Philip, T Price, J Frahm, G Drettakis, and G Brostow. Deep blending for free-viewpoint image-based rendering. SIGGRAPH Asia'18
	- Generate 2 MVS, then CNN to blend;
		- Michal Jancosek and Tomas Pajdla. Multi-view reconstruction preserving weakly-supported surfaces. CVPR'11
		- Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. CVPR'16
- **IBRNet**: Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. CVPR'21

## Unclassified
- **GQN**: Neural scene representation and rendering. Science 2018
	- https://deepmind.com/blog/neural-scene-representation-and-rendering/
	- Problem setup: novel view synthesis;
	- Encode: v1, v2, ... -> r1, r2;
		- Additive r = r1 + r2;
	- Novel view:
		- Latent z, query vi -> h
		- Render: h -> new view;
	- g(x|vq, r) = ∫g(x,z|vq, r)dz

## Explicit Voxel/PC/...
- Also refer to 3D-representation;
	- Explicit, topology-free;
- Voxels:
	- **Refer to Voxels in 3D-represenation**;
	- Octree:
		- Aaron Knoll. A survey of octree volume rendering methods. GI'06
	- Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi. Learning-based view synthesis for light field cameras. ACM TOG, 2016
	- Eric Penner and Li Zhang. Soft 3D reconstruction for view synthesis. SIGGRAPH Asia, 2017
	- Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros,and Jitendra Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. CVPR'17
	- Abhishek Kar, Christian Hane, and Jitendra Malik. Learning a multi-view stereo machine. NeurIPS'17
	- SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Garnelo, Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene representation and rendering. Science'18
	- Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. CVPR'19
		- https://vsitzmann.github.io/deepvoxels/
		- Assume: camera intrinsics, poses known;
		- 2D feat extract;
		- Lifting layer -> GRU (state shared across training) -> Hole filling (UNet)
		- Projection -> occlusion net -> render
		- Supervision: L1-loss, adv-loss;
	- Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. TOG'19
		- encoder-decoder network end-to-end using a differentiable ray marching algorithm;
		- Encoder: three views (center, leftmost, rightmost) go through CNN to get 3,072dim, concat, fc, VAE+sample -> 256-dim + condition-var;
		- Decoder: Voxel V(x;z)=S(x-x0/W/2, g(z))
		- Warping field: to generate finer resolution;
	- Kyle Olszewski, Sergey Tulyakov, Oliver Woodford, Hao Li, and Linjie Luo. Transformable bottleneck networks. ICCV'19
		- https://kyleolsz.github.io/TB-Networks/
		- Problem: 3D manipulation;
		- Source -> 3D-recon voxel -> novel view synthesis;
		- Encoder: 2D - 3D;
		- Transformer-bottleneck: Target-pose -> resampling layer -> aggregated voxel feature;
		- Decoder: 2D + occupancy -> image;
	- P Henzler, N Mitra, and T Ritschel. Learning a neural 3d texture space from 2d exemplars. CVPR'20
		- https://geometry.cs.ucl.ac.uk/projects/2020/neuraltexture
- Layered/Multiplane:
	- Basics:
		- Represent the 3d scene with a few different layers with depth d1, d2, d3, ...;
	- C. L. Zitnick, S. B. Kang, M. Uyttendaele, S. Winder, and R. Szeliski. High-quality video view interpolation using a layered representation. TOG'04
	- J Flynn, I Neulander, J Philbin, and N Snavely. Deepstereo: Learning to predict new views from the world's imagery. CVPR'16
		- PSV: Plane sweep volume;
		- Each image (selection tower as depth prediction): conv-relu -> per-pixel softmax;
	- **LDI**: Shubham Tulsiani, Richard Tucker, Noah Snavely. Layer-structured 3D Scene Inference via View Synthesis. ECCV'18
		- https://shubhtuls.github.io/lsi/
		- https://github.com/google/layered-scene-inference
	- Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely. Stereo Magnification: Learning View Synthesis using Multiplane Images. SIGGRAPH'18
		- https://people.eecs.berkeley.edu/~tinghuiz/projects/mpi/
	- **MPI**: Pratul P. Srinivasan, Richard Tucker, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng, Noah Snavely. Pushing the Boundaries of View Extrapolation with Multiplane Images. CVPR'19
		- https://github.com/google-research/google-research/tree/master/mpi_extrapolation
	- John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, Richard Tucker. DeepView: View synthesis with learned gradient descent. CVPR'19
		- https://augmentedperception.github.io/deepview/
	- Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. TOG'19
	- Peng Dai, Yinda Zhang, Zhuwen Li, Shuaicheng Liu, and Bing Zeng. Neural point cloud rendering via multi-plane projection. CVPR'20
	- Zhengqi Li, Wenqi Xian, Abe Davis, and Noah Snavely. Crowdsampling the plenoptic function. ECCV'20
- Point cloud:
	- K Aliev, A Sevastopolsky, M Kolos, D Ulyanov, and V Lempitsky. Neural point-based graphics. arxiv'19
	- M Meshry, D Goldman, S Khamis, H Hoppe, R Pandey, N Snavely, and R Brualla. Neural rerendering in the wild. CVPR'19
	- F Pittaluga, S Koppal, S B Kang, and S Sinha. Revealing scenes by inverting structure from motion reconstructions. CVPR'19

## Implicit Radience Field
- Basic idea:
	- Use a MLP to overfit color and occupancy of a scene, supervised by each image;
- Xu Chen, Jie Song, and Otmar Hilliges. Monocular neural image based rendering with continuous view control. ICCV'19
	- Analysis
- NERF:
	- **SRN**: Vincent Sitzmann, Michael Zollhofer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. NeurIPS'19
		- https://github.com/vsitzmann/scene-representation-networks
		- Implicit function: Phi(R3) -> Rn;
		- Neural render: Psi, ray-marching lstm;
	- **NERF**: Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. ECCV'20
		- Insight: overfit a function mlp(x,y,z,θ,φ) to explain the scene;
		- Key techniques to improve novel view synthesis performance:
			- Positional encoding;
			- Multi-resolution;
	- Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Deformable neural radiance fields. arxiv'20
		- https://nerfies.github.io/
		- Problem setup:
			- Input: selfie videos; (people can move)
		- Insight: Optimize an additional continuous volumetric deformation field;
			- (x,y,z) -> MLP -> **deformation** field (x',y',z')
			- Nerf on the deformed view;
	- Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. CVPR'21
		- https://nerf-w.github.io/
		- Problem setup:
			- Input: unstructured collected images from internet;
		- Embedding:
			- Static embedding;
			- **Transient** embedding; (moving human, ...)
			- Reconstruction + uncertainty; (to ignore transient parts)
	- Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. Nerv: Neural reflectance and visibility fields for relighting and view synthesis. CVPR'21
		- Problem setup: to enable **relighting**;
		- Neural Visibility Field;
	- Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser. Object-Centric Neural Scene Rendering. arxiv'20
		- https://www.shellguo.com/osf/
- NERF acceleration:
	- Add a prior (image cnn features!):
		- **GRF**: Alex Trevithick and Bo Yang. Grf: Learning a general radiance field for 3d scene representation and rendering. ICCV'21
			- https://github.com/alextrevithick/GRF
			- Similar to IBRNet, but use absolute coord;
			- Each image -> CNN -> feature;
			- Reproject feature -> (xp, yp, zp) -> Attention aggregation;
			- Volumetric rendering and supervision;
		- Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. CVPR'21
			- Similar to IBRNet, with abs coord；
			- (x,y,z) + image-feat at (u,v) -> MLP -> color + opacity;
		- Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. CVPR'21
			- **Meta learning**: learn to initialize the network;
			- https://www.matthewtancik.com/learnit
	- Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasacchi. Derf: Decomposed radiance fields. CVPR'20
		- Decompose into several small net;
		- Occupancy: sigma(x)=sum w(x) sig(x)
		- Color: c(x, d) = sum w(x) c(x,d)
	- **NSVF**: Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. NeurIPS'20
		- https://lingjie0206.github.io/papers/NSVF/
		- https://github.com/facebookresearch/NSVF
		- Insight: Voxel bounded implicit field; progressive training;
		- Skip empty, 10x faster;
	- David B Lindell, Julien NP Martel, and Gordon Wetzstein. Autoint: Automatic integration for fast neural volume rendering. CVPR'21
		- http://www.computationalimaging.org/publications/
		- Modifies Nerf s.t. fewer samples required;
			- Grad net for fast integral;
		- Lower quality;
	- Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten	Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. CVPR'21
		- octree;
- Fast rendering;
	- Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. ICCV'21
		- https://youtu.be/5jKry8n5YO8
		- Stephan J. Garbin, Marek Kowalski, Matthew Johnson,
	Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity
	neural rendering at 200fps. arxiv'21
		- https://microsoft.github.io/FastNeRF/
		- Cache instead of neural net: decompose to avoid 5-dim cache;
			- position dependent;
			- direction dependent;
	- Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas
	Geiger. Kilonerf: Speeding up neural radiance fields with
	thousands of tiny mlps. arxiv'21
		- https://github.com/creiser/kilonerf
		- Thousands of small MLP instead of a big MLP;
	- Real-time rendering of NeRFs with PlenOctrees - Angjoo Kanazawa
		- https://unsup3d.github.io/
- Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. arxiv'20
- Konstantinos Rematas, Ricardo Martin-Brualla, and Vittorio Ferrari. Sharf: Shape-conditioned radiance fields from a single view. arxiv'21
- Generative
	- Marco Fraccaro, Danilo Jimenez Rezende, Yori Zwols, Alexander Pritzel, SM Eslami, and Fabio Viola. Generative temporal models with spatial memory for partially observed environments. arxiv'18
	- Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escaping plato's cave: 3d shape from adversarial rendering. ICCV'19
	- Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan: Unsupervised learning of 3d representations from natural images. ICCV'19
	- Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-GAN: Periodic implicit generative adversarial networks for 3d-aware image synthesis. arxiv'20
	- Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. arxiv'20
	- GRAF: Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. GRAF: Generative radiance fields for 3d-aware image synthesis. NeurIPS'20
		- Insight: Generative model + NERF
			- Traditional: z-sampling -> image/voxel;
			- Proposed: z-sampling -> RF, RF + camera-pose -> image;
	- **SGN**: Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind. Unconstrained Scene Generation with Locally Conditioned Radiance Fields. ICCV'21
		- Global generator from a latent code z: W=g(z); with StyleGAN2;
		- Local radience field from an angle p: (sigma, color)=f(wij, p); modulated linear layers similar to CIPS;

## Real-Time
- Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon
Yenphraphai, and Supasorn Suwajanakorn. NeX: Real-time
view synthesis with neural basis expansion. arxiv'21
- MVP: Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mixture of volumetric primitives for efficient neural rendering. SIGGRAPH'21

## Extroplate, Large Angle
- Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H Kim, and Jan Kautz. Extreme view synthesis. ICCV'19
- Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image. arxiv'20

## Unclassified
- Hao Su, Fan Wang, Eric Yi, Leonidas Guibas. 3D-Assisted Feature Synthesis for Novel Views of an Object. ICCV'15
- J Yang. Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis. NIPS 2015
	- Input/output: images
	- https://github.com/jimeiyang/deepRotator
- M Tatarchenko, A Dosovitskiy, T Brox. Multi-view 3D Models from Single Images with a Convolutional Network. ECCV'16
	- Encoder/Decoder
	- Problem setup: Input RGB, output RGB/D conditioned on any shape input (angle, ...)
	- Separate model for 3D reconstruction (Point-cloud) from multiple RGBD, then mesh;
	- https://github.com/lmb-freiburg/mv3d
- T. Zhou, S. Tulsiani, W. Sun, J. Malik and A. Efros. View synthesis by appearance flow. ECCV'16
	- https://github.com/tinghuiz/appearance-flow
- A Dosovitskiy, J Springenberg, M Tatarchenko, T Brox. Learning to Generate Chairs, Tables and Cars with Convolutional Networks. PAMI'17
	- Input: class c, view v, transfrom param θ;
	- Output: image;
	- Conv, deconv, upsampling;
- J. Xie, R. B. Girshick, and A. Farhadi. Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks. ECCV'16
	- Key insight: Unsupervised;
	- Input: left view; Output: right view
	- Evaluation: Kitti, NYU;\
		<img src="/CV-3D/images/depth-est/deep3d.png" alt="drawing" width="500"/>
- E Park, J Yang, E Yumer, D Ceylan, and A Berg. Transformation-Grounded Image Generation Network for Novel 3d View Synthesis. 2017
- Johanna Delanoy, Mathieu Aubry, Phillip Isola, Alexei A. Efros, Adrien Bousseau. 3D Sketching using Multi-View Deep Volumetric Prediction. CGIT'18
- S Sun, M Huh, Y Liao, N Zhang, and J Lim. Multi-view to Novel view: Synthesizing Novel Views with Self-Learned Confidence. ECCV'18
	- Problem: novel view synthesis
	- Input: many images;
	- https://github.com/shaohua0116/Multiview2Novelview
- Daeyun Shin, Zhile Ren, Erik B. Sudderth, Charless C. Fowlkes. 3D Scene Reconstruction with Multi-layer Depth and Epipolar Transformers. ICCV'19
	- https://research.dshin.org/iccv19/multi-layer-depth/
- **SynSin**: Olivia Wiles, Georgia Gkioxari, Richard Szeliski, Justin Johnson. SynSin: End-to-end View Synthesis from a Single Image. CVPR'20 submission
	- http://www.robots.ox.ac.uk/~ow/synsin.html

## Disentangled
- **3D-SDN**: Shunyu Yao, Tzu Ming Harry Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio Torralba, William T. Freeman, Joshua B. Tenenbaum. 3D-Aware Scene Manipulation via Inverse Graphics. NIPS'18
	- http://3dsdn.csail.mit.edu/
	- https://github.com/ysymyth/3D-SDN
	- Algorithm:\
		<img src = '/Composition/images/2d/3d-sdn.png' width = '400'>
