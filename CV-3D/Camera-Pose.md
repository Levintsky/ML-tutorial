# 3D Camera Pose Estimation

## Basics
- Perspective-n-Point
	- Given: camera coord uv-c, 3D world coord XYZ-w, intrinsics K;
	- Output: 6-DOF camera pose;
- Tutorials
	- VO: https://avisingh599.github.io/vision/visual-odometry-full/
	- S. Savarese and L. Fei-Fei. 3d generic object categorization, localization and pose estimation, ICCV 2017

## PnP
- https://zhuanlan.zhihu.com/p/399140251
- DLT:
	- https://zhuanlan.zhihu.com/p/58648937?edition=yidianzixun
	- s[u,v,1] = P3x4 [X,Y,Z,1]
	- Let P3x4 := [t1; t2; t3]
	- Because s = (t3, XYZ1)
	- u = t1 XYZ1/t3 XYZ1 -> t1 XYZ1 - t3 XYZ1 u =0
	- v = t2 XYZ1/t3 XYZ1 -> t2 XYZ1 - t3 XYZ1 v =0
	- [XYZ1' 0 -u XYZ1'] [t1;t2;t3] = 0
	- [0 XYZ1' -v XYZ1']
	- With all points, SVD;
- P3P: Law of cosines;
	- https://www.cnblogs.com/mafuqiang/p/8302663.html
	- Camera center: O;
	- Camera 3 pts: oa, ob, oc;
	- World: OA, OB, OC;
	- Let x=OA/OC, y=OB/OC, v=AB^2/OC^2, uv=BC^2/OC^2, wv=AC^2/OC^2
	- By Law of cosines:
		- x^2 + y^2 - 2xycos(a,b) = v (1)
		- y^2 + 1 - 2ycos(b,c) = uv
		- x^2 + 1 - 2xcos(c,a) = wv
	- Eliminate v with (1):
		- (1-u)y^2 - ux^2 -cos(b,c)y + 2uxycos(a,b) + 1 = 0
		- (1-w)x^2 - wy^2 -cos(c,a)x + 2wxycos(a,b) + 1 = 0
	- Known:
		- cos(a,b), cos(b,c), cos(c,a): calibrated camera;
		- u = BC^2/AB^2, w = AC^2/AB^2: known world coord;
	- Solve x, y, we get XYZ in camera coord;
	- ICP to get R|t;
- EPnP:
	- Find 4 control points (by PCA?)
	- World coord: pi-w = Σj αij cj-w with Σj αij=1
	- Camera coord: pi-c = Σj αij cj-c with Σj αij=1
		- Requires Σj αij=1 to satisfy;
	- αij can be derived easily;
	- Solve: 4 control points in camera coord: xj-c, yj-c;
	- Σj αij fx xj-c + αij(cx-ui)zj-c = 0
	- Σj αij fy yj-c + αij(cx-ui)zj-c = 0
- BA:
	- T = argmin 1/2 Σ|pi-c - 1/wi KTP-w|^2

## BA
- Optimality in Noisy Real World Conditions
	- 8-points: closed form, elegant; not robust to noise;
- Basics: Jointly estimate camera parameters and 3D coordinates
	- Minimize reprojection error from 3D to each camera;
	- Given: 2D coord xij on each image i
	- Solve: 3D coord Xj; m camera pose: {Ri,Ti}i=1..m
	- E({Ri,Ti}, {Xj}) = Σi Σj θij|xij - π(Ri,Ti, Xj)|^2
		- θij: visibility for Xj in image i;
- Optimizer:
	- Gradient Descent
	- Least Squares Estimation
	- Newton Methods
	- The Gauss-Newton Algorithm
	- The Levenberg-Marquardt Algorithm: damped Gauss-Newton
		- xt+1 = xt + Δ
		- Levenberg: Δ = -(J'J + λI)^(-1) J'r
		- Marquardt: Δ = -(J'J + λdiag(J'J))^(-1) J'r
- Good summaries:
	- Snavely, Seitz, Szeliski, Modeling the world from Internet photo collections, IJCV 2008.
	- **PTAM**: Klein, Murray, Parallel Tracking and Mapping (PTAM) for Small AR Workspaces, ISMAR 2007

## Unclassified
- D Fouhey, A Gupta, and M Hebert. Data-driven 3D primitives for single image understanding. ICCV'13
- S Tulsiani and J Malik. Viewpoints and keypoints. CVPR'15
- D Hoiem, A Efros, and M Hebert. Geometric context from a single image. ICCV'15
- G Pavlakos, X Zhou, A Chan, K Derpanis, and K Daniilidis. 6-dof object pose from semantic keypoints. ICRA'17
- Dense RGB-D Tracking:
	- Benchmark: Sturm, Engelhard, Endres, Burgard, Cremers, IROS 2012
	- Steinbrücker, Sturm, Cremers (2011)
	- Direct stereo: Comport, Malis, Rives, ICRA 2007
	- Non-quadratic penalizers; Kerl, Sturm, Cremers, ICRA 2013
		- Combines color consistency and geometry consistency

## Visual Odometry/SLAM (Simultaneous Localization And Mapping)
- Basics:
	- Input: lidar (Lidar SLAM), one image (monocular SLAM), multi-image (stereo pair in Quadrifocal VO);
	- Output: 
		- Also do 3D reconstruction (very sparse);
		- Focus more on localizing ego-motion of a camera and robot;
	- General steps of Lidar-SLAM:
		- Step 1. Map initialization
		- Step 2. Pose Tracking: ICP (Iterative closest point), optimize for camera pose;
		- Step 3. Map optimization: occupancy grid map;
		- Iterate 2 and 3.
		<img src="/CV/images/low-level/icp.png" alt="drawing" width="450"/>
	- General steps of VSLAM:
		- Step 1. Initialization; (essential matrix, triangulation)
		- Step 2. Pose estimation: (feature tracking, pose-only BA)
		- Visual SLAM by SfM
		- PTAM (Parallel Tracking and Mapping)
			- Real time camera pose tracking
			- An offline thread for map maintenance: when key frame comes, do a BA
		- Relocalization: Tracking can lose due to various reasons
		- Robustness Techniques: Drifting
- SLAM:
	- SLAM: Orb-slam2, DSO as starting point for 3D reconstruction reading.
	- Filtering-based SLAM: Kalman/particle;
	- SSM (State-Space Method)
	- Key-frame based:
		- Bootstrap: an initial 3D map; (2-view geometry)
		- Normal mode: assume 3D map available, incremental camera motion; track points, PnP (Perspective n Points);
		- Recovery mode: assume 3D map available, but tracking failed; Relocalize camera pose w.r.t. Previously reconstructed map;
		- BA;
	- MonoFusion: real-time 3D reconstruction of small scene;
		- Dense 3D reconstruction by single camera;
		- Sparse feature tracking for 6DoF; (key frame based BA);
		- Dense stereo matching for each two key frames;
		- Depth fusion by SDF-based volumetric integration.
	- **PTAM** (Parallel Tracking and Mapping), Separate thread
		- Mapping thread: stereo-init; wait for key frames; add new map points; optimize map; map maintenance; (slow)
		- Tracking thread: Pre-process frame; project points; measure points; update camera pose; draw graphics (optional, only in fine stage);
	- DTAM (Dense tracking and mapping);
	- Semi-Dense SLAM;
	- LSD-SLAM;
	- S-LSD-SLAM;
	- MobileFusion:
		- Real-time volumetric surface reconstruction and dense 6DoF camera tracking running;
		- RGB camera, scan objects;
		- Point-based 3D models, 3D surface model;
		- Dense 6DoF tracking, key-frame selection, dense per-frame stereo matching;
		- Depth maps fused volumetrically akin to KinectFusion;
	- ORB-SLAM: real-time monocular SLAM;
		- Feature-based real-time mono SLAM;
		- Same feature for tracking/mapping/relocalization/loop-closing;
		- Robust to motion clutter, wide baseline, ...
		- Real-time loop closing based on optimization of pose graph called the Essential Graph;
	- ORB-SLAM2: stereo cameras (RGB-D); three threads
		- Tracking to localize the camera; minimize reprojection error by motion only BA;
		- Local mapping to manage local map and optimize it;
		- Loop closing by pose-graph optimization; launches 4th thread to perform full BA;
	- Application in AR/VR;
		- Project Tango (Google): Visual-inertial odometry; SLAM; RGB-D sensor;
		- Hololens (Microsoft); RGB-D sensor;
		- Magic Leap;
		- Voforia (Qualcomm); monocular SLAM;
		- Apple (Metaio)
		- Oculus;
- Classical:
	- feat, feat-match, motion-estimation, dense reconstruction
	- suboptimal, lack robustness
- Direct:
	- No feat abstraction;
	- Steinbrücker, Sturm, Cremers, 2011 and Kerl, Sturm, Cremers, 2013 the camera motion of an RGB-D camera without feature extraction;
	- Newcombe, Lovegrove, Davison, ICCV 2011; dense geometry + motion;
- Loop Closure and Global Consistency
- Dense Tracking and Mapping
- Large Scale Direct Monocular SLAM:
	- Engel, Sturm, Cremers ICCV 2013 and Engel, Schöps, Cremers ECCV 2014; camera motion + semi-dense geometry;

## Geometry-based
- Keypoint matching:
	- M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. CVPR'14
	- A. Collet, M. Martinez, and S. S. Srinivasa. The moped framework: Object recognition and pose estimation for manipulation. IJRS'11
	- M. Zhu, K. G. Derpanis, Y. Yang, S. Brahmbhatt, M. Zhang, C. Phillips, M. Lecce, and K. Daniilidis. Single image 3d object detection and pose estimation for grasping. ICRA'14
- Align with ground truth:
	- V. Ferrari, T. Tuytelaars, and L. Van Gool. Simultaneous object recognition and segmentation from single or multiple model views. IJCV'06
	- F. Rothganger, S. Lazebnik, C. Schmid, and J. Ponce. 3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints. IJCV'06
- ICP:
	- S. Gupta, P. Arbelaez, R. Girshick, and J. Malik. Aligning 3d models to rgb-d images of cluttered scenes. CVPR'15

## Learning-based
- H Su, C Qi, Y Li, and L Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. ICCV'15
- A. Kendall, M. Grimes, and R. Cipolla. PoseNet: A convolutional network for real-time 6-DOF camera relocalization. ICCV'15
- S Tulsiani, J Carreira and J Malik. Pose Induction for Novel Object Categories. ICCV'15
	- Input: images, output three Euler Angles
	- SCT (Similar Class Transfer): 
		- Train a CNN for each class (shared base layers, output heads)
		- |C| x Na x Nθ, class, euler angle, angle bin
		- For an unknown class c', find most similar class known
	- GC (Generalized Classifier):
		- VGG, Na x Nθ
- A Handa, M Bloesch, V Pătrăucean, S Stent, J McCormac, and A Davison. gvnn: Neural network library for geometric computer vision. ECCVW'16
- C Wang, Buenaposada, M Jose, R Zhu, and S Lucey. Learning depth from monocular videos using direct methods. CVPR'18
	- Make direct method differentiable.
- R Clark, M Bloesch, J Czarnowski, S Leutenegger, and A Davison. Learning to solve nonlinear least squares for monocular stereo. ECCV'18
	- solve nonlinear least squares in two-view SfM using a LSTM-RNN
- Z Lv, F Dellaert, J Rehg, A Geiger. Taking a Deeper Look at the Inverse Compositional Algorithm. 2019
- **SVD**: J Levinson, C Esteves, K Chen, N Snavely, A Kanazawa, A Rostamizadeh, A Makadia. An Analysis of SVD for Deep Rotation Estimation. NeurIPS'20
	- https://youtu.be/jgN2SJTVokI
