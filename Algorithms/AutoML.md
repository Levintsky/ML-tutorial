# AutoML, Architecture Search

## Basics
- Search-space:
	- Whole architecture;
	- Cell;
	- Data augmentation: successful nice ideas
	- Activation functions;
- Search approach:
	- RL-based; (long time)
	- Evolution, Bayesian optimization: 
		- e.g. SMAC, BOHB
	- ENAS/DART;
	- Random search;
- Common techniques:
	- Acceleration:
		- ENAS/DART; (weight-sharing, differentiable)
	- Surrogates: Evaluate a subset of architectures, fit a model and predict others;
		- XGBoost, GCN, ...
- Metric: final performance, learning curve, GPU-hours;
- Benchmark:
	- https://openml.github.io/automlbenchmark/results.html
	- **101**: C Ying, A Klein, E Christiansen, E Real, K Murphy, F Hutter. NAS-Bench-101: Towards Reproducible Neural Architecture Search. ICML'19
	- **1-Shot**: A Zela, J Siems, F Hutter. NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search. ICLR'20
	- **201**: X Dong, Y Yang. NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search. ICLR'20
		- Learning curve;
	- **NATS-Bench**: X Dong, L Liu, K Musial, B Gabrys. NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size. PAMI'21
		- Architecture size;
	- **301**: A Zela, J Siems, L Zimmer, J Lukasik, M Keuper, F Hutter. Surrogate NAS Benchmarks: Going Beyond the Limited Search Spaces of Tabular NAS Benchmarks. ICLR'22
	- Hardware cost:
		- LatBench. Ł Dudziak, T Chau, M Abdelfattah, R Lee, H Kim, N Lane. BRP-NAS: Prediction-based NAS using GCNs. NeurIPS'20
		- HW-NASBench. C Li, Z Yu, Y Fu, Y Zhang, Y Zhao, H You, Q Yu, Y Wang, C Hao, Y Lin. HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark
	- ASR: A Mehrotra, A Ramos, S Bhattacharya, Ł Dudziak, R Vipperla, T Chau, M Abdelfattah, S Ishtiaq, N Lane. NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition. ICLR'21
	- NLP: N Klyuchnikov, I Trofimov, E Artemova, M Salnikov, M Fedorov, E Burnaev. NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing. 
- Applications:
	- Most successful On mobile: **MnasNet** in industry;
		- MobileNetV3;
	- Song Han: ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware
	- FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search
- Tradeoff:
	- EfficientNet, EfficientDet;
- Resources:
	- https://www.automl.org/talks/
	- Book: Automated Machine Learning
	- https://github.com/hibayesian/awesome-automl-papers
	- https://github.com/markdtw/awesome-architecture-search
- Tutorial:
	- NIPS'18 (Frank Hutter): https://nips.cc/Conferences/2018/Schedule?showEvent=10979
	- CVPR'22: https://www.dropbox.com/s/wanxfwu0vsczvkw/Frank_Hutter_Keynote_CVPR_NAS_Workshop_2022__NAS_Benchmarks.pdf?dl=0
	- T Elsken, J Metzen, F Hutter. Simple and efficient architecture search for Convolutional Neural Networks. Invited to ICLR'18 Workshop
- Challenges:
	- L Li, A Talwalkar. Random Search and Reproducibility for Neural Architecture Search. UAI'20
		- Poor perf compared to random search;
		- Poor reproducibility;
	- A Yang, P Esperança, F Carlucci. NAS evaluation is frustratingly hard. ICLR'20
		- Training pipeline matters much more than architecture.
	- M Lindauer, F Hutter. Best Practices for Scientific Research on Neural Architecture Search. JMLR'20
		- Poor scientific practice:
			- Inavailability of code;
			- Incomparable training code, search space, eval schemes;

## Summary
- M Wistuba, A Rawat, T Pedapati. A Survey on Neural Architecture Search. 2019
- Zhihu:
	- https://zhuanlan.zhihu.com/p/71547478
	- https://zhuanlan.zhihu.com/p/42924585
	- https://zhuanlan.zhihu.com/p/57404166
- SMBO: https://zhuanlan.zhihu.com/p/53826787

## Performance
- Cifar:
	- EAS: 4.23%
	- [Y Junjie, AAAI'18] CIFAR-10: 3.54%;
	- ENAS: 2.89%
	- DARTS: 2.83%
- ImageNet:
	- DARTS: 73.1%
	- NasNet: 82.7% top-1, 96% top-5;
	- RandAugment: 85%
- Detection
	- DetAug: +2.3 mAP; SOTA 50.7 mAP;
- NLP (PTB Perplexity):
	- DARTS: 56.1;
	- ENAS: 55.8;
- NLP (WMT-14):
	- Evolved transformer: BLEU score 29.8;

## Search Space
- Search whole neural net: original-Nas [B Zoph, Q Le ICLR'17]
- Search cell/block:
	- NasNet [B Zoph CVPR'18]:
		- Op-level: 1x7, 3x3, 5x5, ... conv, pooling, ...
		- Graph-level: randomly select 1 or 2, and an op to combine them;
	- [Junjie Yan AAAI'18], ENAS [H Pham, ICML'18], AmoebaNet [E. Real, AAAI'19],
	- J Dong, Dpp-net: ECCV'18
	- Evolved Transformer [Chen Liang, 19]
		- 5 branch-level: input, norm, layer, output-dim and act
		- 1 block-level search field (combiner function)
		- 1 cell-level search field (number of cells).
- Seach a subpath from a supernet:
	- PathLevel-EAS [H Cai, S Han; ICML'18]
- Search across resolution (in detection):
	- Nas-fpn: G Ghiasi, T Lin, Q Le.  CVPR'19
		- Step 1. Select feat-layer hi;
		- Step 2. Select feat-layer hj;
		- Step 3. Select the output feat resolution;
		- Step 4. Select a binary op to comb(hi, hj) with resolution step-3.
- Search Data Augmentation:
	- RandAugment/AutoAugment: (parameters) [E Cubuk, 19]
		- 16 operations: ShearX/Y, TranslateX/Y, Cutout [12]；
		- Hyperparameters: discretize 10 bins;
	- DetAug: B Zoph, E Cubuk, G Ghiasi, T Lin, J Shlens, Q Le. Learning Data Augmentation Strategies for Object Detection. 2019
		- Color op: Distort color;
		- Geometric op: distort the image;
		- Bounding box op: only distort near boxes;
- Search activation function:
	- swish [Q Le; ICLR'18]: unary and binary op;
- H2O: https://github.com/h2oai/h2o-3
- Anti AutoML; (Insight: Search-space > Search-alg)
	- S Xie, K He. ICCV'19
		- Network generators: g(θ), g: residual, θ: layers, ...
		- Randomly wired NN (ER/BA/WS)
			- Aggregation: linear weight learned to combine
			- Transformation: ReLU - Conv - BN;
			- Distribution;
			- Single input/output;
			- Stages: progressive downsample;
	- S. Xie. On network design spaces for visual recognition. ICCV'19
	- **RegNet**: K He. CVPR'20
		- https://github.com/facebookresearch/pycls	
		- Search space, with control;
			- ResNet-like block: 4-dof, width, num-of-blocks, group size, bottleneck ratio);
			- AnyNet-B: all stage with same bottleneck ratio? no loss;
			- AnyNet-C: all stage with same group width? g>1 best;
			- AnyNet-D: each stage with increasing width;
			- AnyNet-E: each stage with increasing depth;
		- Search approach:
			- Grid search: 500 models, 10 epochs;
			- Draw EDF, fit the trend;
		- Does not use SE and Swish;
		- Conclusion:
			- Best depth: 20 blocks (60 layers);
			- Best bottlenck ratio: 1.0 (no bottleneck?)
			- Best width multiplier: 2.5;
- Non-DL:
	- autoxgboost: J Thomas, S Coors and B Bischl. Automatic Gradient Boosting. ICMLW'18
		- https://github.com/ja-thomas/autoxgboost

## Candidate Proposal Generation
- Exhaustive search:
	- Swish: unary
- Mutation and survive by performance: evolutionary
	- AmoebaNet: mutate of drop an op;
	- The Evolved Transformer '19:
		- warm start with transformer; (6 x enc, 8 x dec)
	- Start simple, split, get deeper: EAS, Path-EAS, PNAS:
- Keep all (and pick later): DNAS, ...
- Proposed by RNN: 
	- Vanilla-NAS [ICLR'17]
	- AutoAugment, DetAug: RNN-controler;
	- Nas-FPN;
- NasNet:

## Search Approach
- DNAS (differentiable):
	- Continuous relaxation to make differentiable;
	- DARTS: [H Liu, K Simonyan, Y Yang; ICLR'19]
	- GDAS: X Dong, Y Yang. Searching for A Robust Neural Architecture in Four GPU Hours. CVPR'19
		- **Gumbel-softmax** trick to sample from different op outputs!
		- https://github.com/D-X-Y/AutoDL-Projects
- RL: REINFORCE, Q-learning
	- REINFORCE
		- vanilla-NAS ICLR'17: 400 GPUs, 3-4 days;
		- [NasNet CVPR'18]
		- [Swish-ICLR'18] Exhaustive search + RL;
			- Discovery: swish op: f() = x σ(βx)
		- EAS, Path-EAS, ENAS: RL + RNN-Meta-Controller
		- AutoAugment CVPR'19: PPO;
		- B Zoph, E Cubuk, G Ghiasi, T Lin, J Shlens, Q Le. Learning Data Augmentation Strategies for Object Detection. 2019
			- 5K subset as proxy;
			- 20K augmentation policies. 400 TPUs + 48 hours;
		- Nas-fpn: cvpr'19 
	- Q-Learning:
		- MetaQNN: ICLR'17
			- https://github.com/bowenbaker/metaqnn
			- Q-learning based: e-greedy, experience replay;
			- Action space: adding different layers, stopping;
		- Zhong, Z, Yan, J, and Liu, C. Practical network blocks design with q-learning. AAAI'18
			- Experiments: 3 days, 32 GPUs; CIFAR-10: 3.54%;
- Evolution
	- Basics:
		- https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html
		- Component and Logics:
			- Initialization
			- Parent selection
			- Recombination
			- Mutation
			- Survivor Selection: remove old and low-perf;
	- Legacy:
		- K Stanley and R Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary Computation, 10(2):99–127, 2002. 2
		- D Floreano, P Durr, and C Mattiussi. Neuroevolution: from architectures to learning. Evolutionary Intelligence, 1(1):47–62, 2008
	- Real, E.; Large-scale evolution of image classifiers. ICML'17
	- AmoebaNet: AAAI'19
	- The Evolved Transformer: 2019	
	- TPOT: https://github.com/EpistasisLab/tpot
- Proxy/surrogate-based (SMBO) Search by Performance prediction:
	- B Baker, et.al. 2017; ICLR'18 Workshop;
	- L Xie and A Yuille. Genetic CNN. ICCV'17
		- https://github.com/aqibsaeed/Genetic-CNN
	- PNAS: 2017
	- Negrinho, R and Gordon, G: Deeparchitect: Automatically designing and training deep architectures, CVPR'17
		- MCTS
		- https://github.com/negrinho/deep_architect
	- Y Kim, B Reddy, S Yun, and C Seo. Nemo: Neuro-evolution with multiobjective optimization of deep neural network for speed and accuracy. ICML'17 Workshop
	- H Liu, K Simonyan, et.al. Hierarchical representations for efficient architecture search, ICLR'18
	- AutoKeras 2018: Efficient Neural Architecture Search with Network Morphism
	- R Luo, F Tian, T Qin, E Chen, and T Liu. Neural architecture optimization.
- Bayesian Optimization:
	- Auto-WEKA: L Kotthoff, C Thornton, H Hoos, F Hutter, and K Leyton-Brown. Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA. JMLR'17
		- https://github.com/automl/autoweka
	- Eric Xing: Neural architecture search with bayesian optimisation and optimal transport. NeurIPS'18
		- Gaussian Process to predict performance from history
		- Measure Network distance from graph theory
		- https://github.com/kirthevasank/nasbot
- One-Shot Architecture Search
	- SMASH: one-shot model architecture search through hypernetworks. ICLR'18.

## Tricks
- Parameter Sharing (Important Trick!)
	- EAS: H Cai, T Chen: AAAI'18
	- PathLevel-EAS: H Cai, S Han: ICML'18
	- ENAS: H Pham, ICML'18

## Applications
- K Ho, A Gilbert, H Jin, J Collomosse. Neural Architecture Search for Deep Image Prior. 2020
