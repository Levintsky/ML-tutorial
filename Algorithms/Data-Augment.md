# Data Augmentation/Generation

## Basics
- Augment or generate new data;
- Goal:
	- Not enough data;
- Data Augmentation
	- Image Augmentation
		- Basic Image Processing Operations
		- Task-Specific Augmentation Strategies:
			- AutoAugment (RL), RandAugment (reduced search space, magnitude only), Population based (evolutionary), UDA;
		- Image Mixture:
			- Mixup, Cutmix (cut paste), MoCHi (HNM)
	- Text Augmentation
		- Lexical Edits: synomym replacement, random swap/delete words;
		- Back-translation: translate to another language, then back;
			- CERT (Contrastive self-supervised Encoder Representations from Transformers; Fang et al. (2020);
		- Mix-up: Guo'19 on embedding space; adding adversarial;
	- Audio Augmentation
		- Wang & van den Oord (2021): mixup, masking, frequency masking, frequency shift;
	- Architectural Augmentation: dropout, SimCSE (Guo'21), cutoff (Shen'20)
- Data Synthesis
	- Language Model as Noisy Annotator: GPT-3 as a weak annotator (Wang'21)
	- Language Model as Data Generator: LAMBADA (Anaby-Tavor et al. 2019), Kumar et al. 2021
- How to Quantify Generated Data Quality? Gontijo-Lopes et al. (2020)
	- Affinity: distribution shift; KL(p(y|x), p(y|x')) model sensitive;
	- Diversity:
- Training with Noisy Data
	- Survey: Learning from Noisy Labels with Deep Neural Networks: A Survey. '21
	- Regularization and Robust Architecture: noisy adaptation layer;
	- Robust Learning Objective: L1 > L2;
	- Label Correction: F-correction (Patrini et al. 2017);
	- Sample Reweighting and Selection
- Tutorial: https://lilianweng.github.io/posts/2022-04-15-data-gen/

## Data Augmentation
- Legacy
	- J Bellegarda. Robust speaker adaptation using a piecewise linear acoustic mapping. ICASSP'92
- Mixup: interpolate between pairwise data/label;
	- Insight: convex linear combination:
		- x˜ = λxi + (1−λ)xj 
		- y˜ = λyi + (1−λ)yj 
	- **mixup**: H Zhang, M Cisse, Y Dauphin, and D Lopez-Paz. mixup: Beyond empirical risk minimization. ICLR'18
		- https://github.com/facebookresearch/mixup-cifar10
	- H Guo, Y Mao, and R Zhang. Augmenting data with
	mixup for sentence classification: An empirical study. arxiv'19
		- Two strategy proposed: Interpolate on word or sentence embedding;
	- Guo, H., Mao, Y., and Zhang, R. Mixup as locally linear out-of-manifold regularization. AAAI'19
		- H := argmin{LD(H) + LD′(H)}
	- V Verma et. al. Manifold mixup: Better representations by interpolating hidden states. ICLR'19
		- Predict less confidently on interpolations of hidden representations.
		- https://github.com/vikasverma1077/manifold_mixup
		- 1. Select a random layer k (could be input);
		- 2. Two minibatches (x,y), (x', y')
		- 3. Input mixup from layer k, mixing λ ∼ Beta(α, α);
		- 4. Forwarded (g˜k(x), y˜) used as output;
	- Guo, H. Nonlinear mixup: Out-of-manifold data augmentation for text classification. AAAI'20
		- Nonlinear interpolation for both data and label;
	- Zhang, L., Deng, Z., Kawaguchi, K., Ghorbani, A., and Zou, J. How does mixup help with robustness and generalization? ICLR'21
		- Insight: prove adversarial robustness and generalization;
- Vision
	- AlexNet;
	- **cutout**: T DeVries and G Taylor. Improved regularization of convolutional neural networks with cutout. arxiv'17
		- useful on CIFAR-10 and not on ImageNet
	- AutoAugment: CVPR'19
	- Google. Improving robustness without sacrificing accuracy with patch gaussian augmentation. arXiv'19
	- RandAugment: check autoML;
	- R Gontijo-Lopes, S] Smullin, E Cubuk, E Dyer. Affinity and Diversity: Quantifying Mechanisms of Data Augmentation. 2020
		- Define Affinity and Diversity:
			- affinity = accuracy gap on validation;
			- diversity = training loss gap;
		- Empiricaly study: the larger the affinity/diversity, the better
- NLP
	- A Yu, D Dohan, M Luong, R Zhao, K Chen, M Norouzi, and Q Le. Qanet: Combining local convolution with global self-attention for reading comprehension. 2018
- Speech: SpecAugment
	- Baidu: Deep speech.
	- SpecAugment: Google-Brain. 2019
- Dataset Reduction
	- T Wang, J Zhu, A Torralba, A Efros. Dataset Distillation. ICLR'19 reject
