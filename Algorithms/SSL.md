# Self-Supervised Learning

## Basics
- Labels...
	- Assumption: 1. Smoothness: smooth label; 2. Cluster; 3. Low-density separation; 4. Manifold;
- Supervision: Contrastive Training Objectives
	- Contrastive Loss (pairwise): (Chopra et al. 2005)  x
		- L(xi, xj, θ) = I(yi=yj)|f(xi;θ)-f(xj;θ)| + I(yi≠yj)max(0, ε-|f(xi;θ)-f(xj;θ)|)
	- Triplet Loss: (Chopra et al. 2005)
	- Lifted Structured Loss (Song et al. 2015):
		- generalize triplet, all pairs in a batch;
	- N-pair Loss: Multi-Class N-pair loss (Sohn 2016)
		- generalize triplet to multiple negatives;
		- L = -log[exp(f(xq)f(x+)) / ∑exp(f(x)f(xi))]
	- NCE (Noise Contrastive Estimation): Gutmann & Hyvarinen'10
		- Target: p(x|c=1;θ)
		- Noise: p(x|c=0)
		- Logit: logp(.;θ) - logq(.)
		- Loss = -log[p/p+q]
	- InfoNCE (CPC):
		- softmax categorical: L = -E[log(f(x,c)/∑f(x',c))]
		- f = exp(z'Wc) in CPC, W trainable;
	- Soft-Nearest Neighbors Loss: (Salakhutdinov & Hinton 2007, Frosst et al. 2019)
		- Extend to multiple positive;
	- Common Setup
- Supervision: other consistency
	- Π-model: (also used in SimCLR, BYOL, SimCSE, ...)
	- Temporal ensembling: EMA-target;
	- Mean teachers: mean-teacher with EMA-parameter;
	- noisy samples as learning targets: adversarial, VAT, ICT, MixUp, UDA,
- Superivision: Pseudo Label
	- Label propagation: similarity graph;
	- Self-Training; train a model, then convert most confident data into labeled ones;
	- Reducing confirmation bias: problem with wrong label from imperfect teacher;
		- Meta Pseudo Labels (Pham et al. 2021)
	- Pseudo Labeling with Consistency Regularization
		- MixMatch, ReMixMatch;
		- DivideMix: GMM to divide data;
		- FixMatch
- Key Ingredients
	- Heavy Data Augmentation (SimCLR)
	- Large Batch Size (SimCLR, CLIP)
	- Hard Negative Mining (SimCSE, DPR)
		- top incorrect;
- Evaluation:
	- Generally downstream task performance;
	- Google-Brain. Realistic Evaluation of Deep Semi-Supervised Learning Algorithms. NIPS'18
- Vision: Image Embedding
	- Image Augmentations (check Data-Augment.md)
		- Basic Image Augmentation
		- Augmentation Strategies
		- Image Mixture: MixUp
	- Parallel Augmentation: two noisy version of the same image;
		- SimCLR, Barlow Twins, BYOL
	- Memory Bank
		- Instance Discrimination with Memoy Bank
		- MoCo & MoCo-V2, CURL (RL)
	- Feature Clustering
		- DeepCluster, SwAV
	- Working with Supervised Datasets (Check Multimodal.md)
		- CLIP, Supervised Contrastive Learning
- Language: Sentence Embedding
	- Check NLP-LM for SSL;
	- Text Augmentation (Check Data-Augment.md)
	- Supervision from NLI
		- Sentence-BERT
		- BERT-flow
			- Transform embedding to smooth isotropic Gaussian via normalizing flows.
		- Whitening Operation
			- Make sentence vec 0 mean, identity covariance;
	- Unsupervised Sentence Embedding Learning
		- Context Prediction
		- Mutual Information Maximization
- Tutorials:
	- NIPS'18 Tutorial by M. A. Ranzato and A. Graves. Deep unsupervised learning.
		- https://media.neurips.cc/Conferences/NIPS2018/Slides/Visualization_for_ML.pdf
	- https://deepmind.com/blog/unsupervised-learning/
	- Re-imagining intelligence
		- https://deepmind.com/blog/agents-imagine-and-plan/
	- https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/
	- https://deepmind.com/research/publications/neural-predictive-belief-representations/	
	- **Tutorial**: ECCV'20
		- Deepak Pathak: https://youtu.be/fUMpC_hoedA
			- RL: intrinsic motivation/curiosity;
				- Predict consequences of action;
				- Bad prediction: high curiosity;
		- Alexei Efros: https://youtu.be/iTbfEXFwDJc
			- Optimistic view: PASCAL VOC -> ImageNet -> MS COCO -> LVIS;
			- No-fixed dataset: data-augmentation?
			- Continual learning? Catastrophic forgetting?
		- Stella Yu: https://youtu.be/F5mt4z-w_Mk
			- Metric learning?
			- Non-parametric softmax instance;
		- Ishan Misra: https://youtu.be/gbziPIn9uDI
			- Multi-view invariance, grouping;
			- **PIRL**: Ishan Misra, Laurens van der Maaten. Self-Supervised Learning of Pretext-Invariant Representations. CVPR'20
				- Invariant to pretext task;
			- AVID-CMA: Pedro Morgado, Nuno Vasconcelos, Ishan Misra. Audio-Visual Instance Discrimination with Cross-Modal Agreement. arxiv'20
				- Task: in same video, audio/video match in time?
				- https://github.com/facebookresearch/AVID-CMA
			- **SWAV**: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. NeurIPS'20
				- Task: same image, two augmentation zt, zs and their query codes qt, qs, loss as l(zt, qs) + l(zs, qt);
				- https://github.com/facebookresearch/swav
		- Carl Doersch: https://youtu.be/RWCc0nZOSBw
			- **BYOL**: DeepMind. Bootstrap your own latent: A new approach to self-supervised Learning. 2020
				- https://github.com/deepmind/deepmind-research/tree/master/byol
				- step 1: no-negative;
				- step 2: stop-gradient; (freeze target network)
					- import to avoid collapse;
				- step 3: prediction; **predict the other representation?**
			- **CrossTransformer**:
		- Paolo Favaro: https://youtu.be/APwHDZZcLuY
			- Short-cut: predict from local features;
			- Build global features: distinguish between objects;
- **Tutorial**: ICML'21
	- https://icml.cc/media/icml-2021/Slides/10843_QHaHBNU.pdf
- https://lilianweng.github.io/posts/2021-05-31-contrastive/

## NLP
- Unsupervised Translation:
	- A. Conneau et al. Word translation without parallel data, ICLR 2018
		- https://github.com/facebookresearch/MUSE
		- https://github.com/facebookresearch/fastText
		- W = argmin||Wx-y||^2
		- No pair data, so fool discrimitor for Wx and y
		- Nearest neighbor for refinements
		- Orthogonality
	- G. Lample et al. Phrase-based and neural unsupervised machine translation, EMNLP'18

## Vision Specific
- Goal: learn feature z from x, without label y;
- Tasks:
	- Autoencoder: reconstruct x from z;
	- Context prediction:
		- Jigsaw puzzle;
		- Colorization;
		- Context inpainting: (MAE)
		- Tokenized (VQ-VAE like): BEiT;
	- Pseudo labels:
		- Rotation: 0/90/180/270
		- Deep clustering;
	- Consistency:
		- Contrastive learning: MoCo, SimCLR, CPC, ...
		- Non-contrastive: swav, byol, SimSiam, DINO, ...
- Goal:
	- Pretraining with a lot of unlabeled data;
- Evaluation:
	- Linear probe/protocol;
	- Finetuning;
- Tutorials:
	- https://lilianweng.github.io/posts/2021-12-05-semi-supervised
	- Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer. Revisiting Self-Supervised Visual Representation Learning. CVPR'19
		- https://github.com/google/revisiting-self-supervised

## Unclassified
- Edwards & Storkey, Towards a Neural Statistician, (2017)
	- one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model.
- **Test-time training (TTT)**
	- A Jabri, A Owens, A Efros. Space-Time Correspondence as a Contrastive Random Walk. NeurIPS'20
		- Task: play video forward then backward, should end at the same point;
		- https://ajabri.github.io/videowalk/
		- https://github.com/ajabri/videowalk
- **ReMixMatch**: D Berthelot, N Carlini, E Cubuk, A Kurakin, H Zhang, C Raffel, K Sohn. ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring. 2019
	- CTAugment
	- https://github.com/google-research/remixmatch
	- CIFAR: 93.73% with 4,000 labels; 84.9% with 40;
- **FixMatch**: K Sohn, D Berthelot, C Li, Z Zhang, N Carlini, E Cubuk, A Kurakin, H Zhang, C Raffel. FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence. 2020
	- https://github.com/google-research/fixmatch
	- CIFAR: 94.9% with 250 labels, 88.6% with 40 (4 per class);

## Images Tasks
- AutoEncoder:
	- Google-Brain. Building high-level features using large scale unsupervised learning. ICML'12
		- ICML'22 Test of time award
- **Context** prediction:
	- C. Doersch et al. Unsupervised Visual Representation Learning by Context Prediction, ICCV'15
		- **Spatial relationship** for two image patches;
	- D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. Efros. Context encoders: Feature learning by inpainting. CVPR'16
	- Mehdi Noroozi, Paolo Favaro. Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles. ECCV'16
	- **Colorization**: R Zhang, P Isola, A Efros. Colorful Image Colorization, ECCV'16
	- T Jakab, A Gupta, H Bilen, A Vedaldi. Unsupervised Learning of Object Landmarks through Conditional Image Generation. NIPS'18
	- **BEiT**: H Bao, L Dong, S Piao, F Wei. BEiT: BERT Pre-Training of Image Transformers. ICLR'22
		- https://github.com/microsoft/unilm/tree/master/beit
		- 224 x 224 image into 14 x 14 grid, each of size 16x16;
		- Tokenized with dVAE rather than raw pixels; (stage I)
			- q(z|x; φ) maps image x to visual codebook;
			- p(x|z; ψ) reconstruct;
			- Loss Ez∼qφ(z|x)[log pψ(x|z)] with gumbel-softmax;
		- Replace 40% patches with a masked embedding z (learnable);
		- Pretraining: predict masked tokens; (stage II)
			- max Σx E_M[Σlog pMIM(zi|xM)]; or logp(zi|x-mask)
		- ImageNet:
			- BEIT-B: 83.2% (224x224), 84.6% (384x384)
			- BEIT-L: 85.2% (224x224), 86.3% (384x384)
	- **MAE**: FAIR. Masked Autoencoders Are Scalable Vision Learners. CVPR'22 Best Paper Nominee
		- https://github.com/facebookresearch/mae
		- An important design of our MAE is to skip the mask token [M] in the encoder and apply it later in the lightweight decoder.
- Pseudo-labels:
	- Predict image rotation:
		- S. Gidaris et al. Unsupervised Representation Learning by Predicting Image Rotations. ICLR'18
	- Clustering:
		- **Pseudo-label**: D Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. ICMLW'13
			- Layerwise denoised autoencoder with cross-entropy(x, x^)
			- Equivalent to Entropy Regularization;
		- M Caron, P Bojanowski, A Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. ECCV'18
			- Extract CNN and run K-means
			- Train on cluster id;
		- **SWAV**: M Caron, I Misra, J Mairal, P Goyal, P Bojanowski, A Joulin. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. NIPS'20
			- https://github.com/facebookresearch/swav
			- Insight: prototypes C as a pseudo K-means;
			- Given two augmented xt, xs, predict code qs from pt;
				- l(zt, qs) = −∑qs(k) logpt(k)
				- L(zt, zs) = l(zt, qs) + l(zs, qt),
		- X Ji, J Henriques, A Vedaldi. Invariant Information Clustering for Unsupervised Image Classification and Segmentation. ICCV'19
			- https://github.com/xu-ji/IIC
		- Yang'16;
		- Zhuang 19;
		- PCL: Li 20
		- Asano: 2020 SeLa;
		- Dwibedi 21, NNCLR;
- Dictionary Learning:
	- **LocalAggr**: C Zhuang, A Zhai, and D Yamins. Local aggregation for unsupervised learning of visual embeddings. ICCV'19	
- Consistency:
	- **Contrastive** Exemplar: NPDC, MoCo, SimCLR, CPC
		- Z Wu, Y Xiong, S Yu, and D Lin. Unsupervised feature learning via non-parametric instance discrimination. CVPR'18
			- https://github.com/zhirongw/lemniscate.pytorch
			- Constrastive estimation;
		- CPC: DeepMind. Representation Learning with Contrastive Predictive Coding. NIPS'18
			- A query frame x_t at time t;
			- Recent 8 frames: positive;
			- Other uniformly sampled frames: negative;
		- AMDIM: P Bachman, R D Hjelm, and W Buchwalter. Learning representations by maximizing mutual information. NIPS'19
			- https://github.com/Philip-Bachman/amdim-public
			- Maximize mutual-information I(aug1(x), aug2(x));
			- Also between scales;
		- Y Bengio. Learning deep representations by mutual information estimation and maximization. ICLR'19
		- UDA: Q Xie, Z Dai, E Hovy, M Luong, and Q Le. Unsupervised data augmentation for consistency training. 2019
			- https://github.com/google-research/uda
			- Semi-supervised learning:
				- Labeled: l(p(y|x), y)
				- Unlabeled: consistency between augmented data: p(y|x), p(y|x')
		- **MoCo**: FAIR. Momentum Contrast for Unsupervised Visual Representation Learning. CVPR'20
			- Contrastive learning; (1-positive + K-negative), InfoNCE applied with softmax-cross-entropy;
			- Dictionary as a queue; (>> batch-size, no gradients, only learn query-encoder)
			- Momentum update: slowly updating key encoder; θ-k = 0.999 θ-k + 0.001 θ-q;
			- Positive: random augmentation (with grayscale);
			- Shuffling-BN: train with multiple GPUs independently, the sample order of minibatch randomly shuffled;
			- Exp:
				- Linear protocol: 68% on ImageNet, similar to CMC, CPC and LocalAggr;
				- PASCAL VOC Faster-RCNN end-to-end [AP: 74.4 -> 75.6];
				- COCO end-to-end MaskRCNN [AP: 54.7 -> 55.4]
		- **SimCLR**: Google-Brain. A Simple Framework for Contrastive Learning of Visual Representations. ICML'20
			- https://github.com/ildoonet/pytorch-SimCLR
			- Data augmentation: color distortion is critical;
			- InfoNCE loss;
			- Shuffle BN to solve information leakage (similar to MoCo);
			- Large batch-size (4k - 8k, requires TPU support), longer training (1000 epochs);
		- **MoCo-V2**: FAIR. Improved Baselines with Momentum Contrastive Learning. 2020
			- Verify the effectiveness of two of SimCLR's design:
				- **MLP projection head**
				- More **data augmentation**
		- Z Xie, Y Lin, Z Yao, Z Zhang, Q Dai, Y Cao, and H Hu. Self-supervised learning with swin transformers.
		- **MoCo-V3**: An Empirical Study of Training Self-Supervised Vision Transformers. ICCV'21
			- Large batch-size instead of memory queue, vit backbone;
			- Symmetrized loss: l(q1, k2) + l(q2, k1)
		- Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis. CVPR'22
	- Non-Contrastive:
		- **BYOL**: Bootstrap Your Own Latent A New Approach to Self-Supervised Learning. NIPS'20
			- No negative pairs, target network;
		- **simsiam**: Xinlei Chen, Kaiming He. Exploring Simple Siamese Representation Learning. CVPR'21 best paper honorable mention
			- Insight: minimize negative cosine similarity between positive pairs only;
				- p: extra-MLP after z;
				- d(p1, z2) = - (p1, z2) / |p1| |z2|
				- loss: L = 1/2[d(p1, z2) + d(p2, z1)]
			- Techniques not used in the paper:
				- (i) negative sample pairs,
				- (ii) large batches,
				- (iii) momentum encoders
		- **Barlow Twins**: FAIR. Barlow Twins: Self-Supervised Learning via Redundancy Reduction. ICML'21
			- Insight: feature correlation within a batch should be closed to identity;
		- **DINO**: M Caron, H Touvron, I Misra, H Jegou, J Mairal, P Bojanowski, A Joulin. Emerging Properties in Self-Supervised Vision Transformers. ICCV'21
			- https://github.com/facebookresearch/dino
			- Insight: match output feature distribution by distillation: -p2logp1, p1 student, p2 teacher;
				- p1: softmax(s_out)
				- p2: softmax(t_out - C); C: center ema-updated;
				- Teacher/student: global/local view;
				- Self-attention of [CLS] token groups objects;
			- Teacher: parameter (stop grad, moving avr of student), centering layer, take >50% image;
			- Student: to learn, with small image;
			- SWAV mean C trick;
			- DINO linear: 75.3% res-50, 77% vit, 80% vit-b;
		- **MSN**: M Assran, M Caron, I Misra, P Bojanowski, F Bordes, P Vincent, A Joulin, M Rabbat, N Ballas. Masked Siamese Networks for Label-Efficient Learning.
			- DINO + mask;
- Other prior:
	- Symmetry:
		- Andrea Vedaldi. Modelling and unsupervised learning of symmetric deformable object categories. NIPS'18

## Analysis
- Visualization:
	- F Bordes, R Balestriero, P Vincent. High Fidelity Visualization of What Your Self-Supervised Representation Knows About. 2021
- Yuandong Tian, Lantao Yu, Xinlei Chen, Surya Ganguli. Understanding Self-supervised Learning with Dual Deep Networks. arxiv
- Yuandong Tian, Xinlei Chen, Surya Ganguli. Understanding Self-supervised Learning Dynamics without Contrastive Pairs. ICML'21 Outstanding Paper Award Honorable Mention
- Li Jing, Pascal Vincent, Yann LeCun, Yuandong Tian. Understanding Dimensional Collapse in Contrastive Self-supervised Learning. ICLR'22
- Deep Contrastive Learning is Provably (almost) Principal Component Analysis. arxiv'22

## Weakly Supervised
- Classification:
	- **URU**: D Mahajan, R Girshick, V Ramanathan, K He, M Paluri, Y Li, A Bharambe, L van der Maaten. Exploring the Limits of Weakly Supervised Pretraining. ECCV 2018
		- **IG-1B** dataset: Instagram
		- Pretrain on a larger dataset (1.5 billion Instagram) + (1.5k, 8.5k, 17k hashtag classes);
		- Preprocessing: image-deduplication, hash-tags clean up (SynNet);
		- Model: ResNext;
		- Infra: Caffe2-1hr, synchronous-SGD; 42 Machine x 8 GPU/m = 336 GPU; 22 days;
		- Conclusion 1: hash-tags v.s. accuracy; (always a gain, fine-grained target needs a fine-grained source);
		- Conclusion 2: larger amount of pretrain -> better target domain accuracy;
		- Conclusion 3: the smaller the label noise (hashtags are noisy) -> better target accuracy;
	- A Joulin, L van der Maaten, A Jabri, N Vasilache. Learning Visual Features from Large Weakly Supervised Data. ECCV'16
- Weak detetection (Zhenheng):
	- Image has only image level labeling (no bbox or segments);
	- Hundreds of candidate proposals (can't penalize bbox reg-loss or classification loss);
	- Max-pooling/top-k then direct classification;
	- Half mAP

## Videos
- **Frame** prediction forward or backward
	- D. Wei et al. Self-supervision using the arrow of time, CVPR 2018
	- I. Misra, C. L. Zitnick, and M. Hebert. Shuffle and Learn: Unsupervised Learning using Temporal Order Verification. ECCV'16
		- Predict shuffle;
	- Vondrick et al NIPS'16, CVPR'17
	- E. Denton et al. Unsupervised learning of disentangled representations from video, NIPS'17
	- **CMC**: Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. ICLR'20 rejected
- Predict motion/flow/ego-motion/motion-mask:
	- P. Agrawal, J. Carreira, and J. Malik. Learning to see by moving. ICCV'15
	- D. Jayaraman and K. Grauman. Learning image representations tied to egomotion. ICCV'15
	- Walker et al. ECCV'16
	- D. Pathak, R. Girshick, P. Dollar, T. Darrell, and B. Hariharan. Learning features by watching objects move. CVPR'17
- **Tracking**:
	- X. Wang and A. Gupta. Unsupervised learning of visual representations using videos. ICCV'15
- **Coloring** videos
	- C. Vondrik et al. Tracking emerges from colorizing videos, ECCV 2018
- **Multi-modality** prediction from each other
	- V. de Sa, Learning classification from unlabeled data, NIPS 1994
	- R. Arandjelovic et al. Object that sound, ECCV 2018
- R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun. Unsupervised learning of spatiotemporally coherent metrics. ICCV'15
- Self-Supervised Video Representation Learning With Odd-One-Out Networks
- Matthias Minderer, Chen Sun, Ruben Villegas, Forrester Cole, Kevin Murphy, Honglak Lee. Unsupervised learning of object structure and dynamics from videos. NIPS'19

## 3D
- Alexey Dosovitskiy. Unsupervised Learning of Shape and Pose with Differentiable Point Clouds. NIPS'18

## Google Brain
- F Locatello, S Bauer, M Lucic, G Rätsch, S Gelly, B Schölkopf, O Bachem. Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. ICML'19
	- https://github.com/google-research/disentanglement_lib

## InfoNCE
- **CPC**: Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Predictive Coding. NIPS'18
	- Insight: an encoder g_enc(x_t)=z_t, as well as c_t (info accumulated until t), given some {z_t+k, z_j1, z_j2, ...}, z_t+k as positive and others uniformly sampled as negative, do classification **InfoNCE**;
	- Prediction\
		<img src = '/Weak-Unsupervised/images/cpc2.png' width = '400'>
	- Loss: InfoNCE\
		<img src = '/Weak-Unsupervised/images/cpc3.png' width = '400'>
	- Application in vision:
		- ResNet-101: g-enc\
			<img src = '/Weak-Unsupervised/images/cpc4.png' width = '500px'>
- **GIM (Greedy-InfoMax)**: Sindy Löwe, Peter O’Connor, Bastiaan S. Veeling. Putting An End to End-to-End: Gradient-Isolated Learning of Representations. NIPS'19 Honorable Mention Outstanding New Directions Paper Award
	- https://github.com/loeweX/Greedy_InfoMax
	- Key insight: layer-wise information preservation;
	- Algorithm: Divide CNN by depth into M modules;
	- Supervision: similar to CPC;
	- Experiments:
		- Vision: STL-10;
		- Audio: LibriSpeech speaker recognition (100-hour);
