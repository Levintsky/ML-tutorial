# Bandit

## Resources
- http://iosband.github.io/2015/07/28/Beat-the-bandit.html
- https://github.com/bgalbraith/bandits
- Book: Csaba Szepesvari and Tor Lattimore. Bandit Algorithms. 2020

## Stanford-cs234 (lec-10)
- A tuple of (A, R)
	- Q(a) = E[r|a]
	- V\* = max_a Q(a)
	- Regret: lt = E[V\*-Q(at)]
	- total regret: Lt = E[ΣV\*-Q(at)] = ΣE[Nt(a)]∆a (visit times gap)
	- Goal: maximize cumulative Σrt or minimize total regret;
- Greedy: could stuck on suboptimal;
	- Estimate ri with MC;
	- Greedy to select the highest value;
- ε-greedy
- Theory (Lai and Robbins): Asymptotic total regret is at least logarithmic in number of steps:
	- limt->∞ Lt = logt Σ ∆a/KL(Ra|Ra\*)
- UCB:
	- Estimate upper bound Ut(a), s.t. Q(a) < Ut(a) with high probability;
	- Theory (Hoeffding's Inequality) X1,..., Xn i.i.d in [0,1], Xn' as sample mean, then:
		- P(E[x] > Xn'+u) <= exp(-2nu^2)
	- Estimate Q(a)' and Q(a) close enough:
		- P(|Q(a)-Q(a)'|>=sqrt(Clogt/N(a))) <= δ/T
	- Number of time a non-optimal arm is pulled at most O(logT) times:
		- Q(a)+sqrt(Clogt/N(a))) >= Q(a\*)+sqrt(Clogt/N(a\*))) >= Q(a\*)
		- Q(a) + 2sqrt(Clogt/N(a))) >= Q(a\*)
		- Nt(a) <= 4ClogT/∆a^2
	- UCB: a = argmax μa + sqrt(2lnT/N(a))
- Optimistic initialization:
	- Initialize Q(a) high;
	- Update: Q(at) += 1/N(at)(rt-Qt)
- Probably Approximately Correct: mostly based on optimism or Thompson Sampling;
	- Choose ε-optimal, s.t. p(Q(a)>=Q(a\*)-ε) > 1-δ
	- Polynomial in the problem parameters (#actions,)
- Bayesian Bandit:
	- Knowledge about prior p(R), p[R|ht], ht=(a1,r1,...)
	- Posterior guided exploration;
	- p(φi|ri) = p(ri|φi)p(φi)/p(ri) = p(ri|φi)p(φi)/∫p(ri|φi)p(φi)dφi
	- Conjugate (exponential family);
	- e.g. known binary reward 0,1, sampled from Bernoulli θ, Beta(α,β);
- Probability matching:
	- π(a|ht) = p(Q(a)>Q(a'), ∀a'≠a|ht)
	- Thompson sampling:
		- Sample a reward Ra from posterior;
		- Q(a)=E[Ra]
		- at = argmax Q(a)
		- Observe r
		- Update posterior p(Ra|r) with Bayesian;
	- Thompson sampling implements probability matching:
		- π(a|ht) = E[1(a=argmaxQ(a))]
	- Bayesian regret;
- Contexutal bandit:
	- Ra,s(r) = P(r|a,s), (multi-arm doesn't have s);
	- r = θφ(s, a) + ε

## NIPS'19
- Soumya Basu, Rajat Sen, Sujay Sanghavi, Sanjay Shakkottai. Blocking Bandits
- Aadirupa Saha, Aditya Gopalan. Combinatorial Bandits with Relative Feedback
- David Martínez-Rubio, Varun Kanade, Patrick Rebeschini. Decentralized Cooperative Stochastic Bandits
- Gi-Soo Kim, Myunghee Cho Paik. Doubly-Robust Lasso Bandit
- Tianyuan Jin, Jieming SHI, Xiaokui Xiao, Enhong Chen. Efficient Pure Exploration in Adaptive Round Model
- Tobias Sommer Thune, Nicolò Cesa-Bianchi, Yevgeny Seldin. Nonstochastic Multiarmed Bandits with Unrestricted Delays
- Baekjin Kim, Ambuj Tewari. On the Optimality of Perturbations in Stochastic and Adversarial Multi-armed Bandit Problems
- David Simchi-Levi, Yunzong Xu. Phase Transitions and Cyclic Phenomena in Bandits with Switching Constraints
- Virag Shah, Ramesh Johari, Jose Blanchet. Semi-Parametric Dynamic Contextual Pricing
- Sayak Ray Chowdhury, Aditya Gopalan. Bayesian Optimization under Heavy-tailed Payoffs
- Julian Zimmert, Tor Lattimore. Connections Between Mirror Descent, Thompson Sampling and the Information Ratio
- Yogev Bar-On, Yishay Mansour. Individual Regret in Cooperative Nonstochastic Multi-Armed Bandits
- Mohammad Sadegh Talebi, Odalric-Ambrym Maillard. Learning Multiple Markov Chains via Adaptive Allocation
- Sanae Amani, Mahnoosh Alizadeh, Christos Thrampoulidis. Linear Stochastic Bandits Under Safety Constraints
- Nima Hamidi, Mohsen Bayati, Kapil Gupta. Personalizing Many Decisions with High-Dimensional Covariates
- Rémy Degenne, Wouter Koolen, Pierre Ménard. Non-Asymptotic Pure Exploration by Solving Games
- Ilai Bistritz, Zhengyuan Zhou, Xi Chen, Nicholas Bambos, Jose Blanchet. Online EXP3 Learning in Adversarial Bandits with Delayed Feedback
- Shinji Ito, Daisuke Hatano, Hanna Sumita, Kei Takemura, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi. Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback
- Young Hun Jung, Ambuj Tewari. Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems
- Chao Tao, Saúl Blanco, Jian Peng, Yuan Zhou. Thresholding Bandit with Optimal Aggregate Regret
- Yoan Russac, Claire Vernade, Olivier Cappé. Weighted Linear Bandits for Non-Stationary Environments
