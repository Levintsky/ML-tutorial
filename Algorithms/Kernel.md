# Kernel, Embedding, Metric Learning

## Basics
- Kernel functions:
	- RBF, Mercer, linear, ...
	- Kernel Trick: NN, ridge regression, PCA, SVM;
- Tutorials, Books
	- R Herbrich. Learning Kernel Classifiers. 2001
	- B Scholkopf, A Smola. Learning with kernels. 2002
	- J Shawe-Taylor and N Cristianini. Kernel Methods for Pattern Analysis. 2004

## Kernel Methods (PRML-6, K.Murphy-14)
- 14.1 Introduction
- 14.2 Kernel functions;
	- κ(x, x') ∈ R
	- κ(x, xˆ) = exp(−1/2(x−xˆ)'Σ^(−1)(x−xˆ)
	- 14.2.2 Kernels for comparing documents
		- Cosine similarity
		- tf-idf:
			- tf(xij) = log(1+xij)
			- idf(j) = log[N/1+ΣI(xij>0)]
	- 14.2.3 Mercer kernels
		- Mercer: Gram matrix K positive definite kernel;
			- [κ(x1, x1), ..., κ(x1, xn)]
			- ...
			- [κ(xn, x1), ..., κ(xn, xn)]
		- Mercer theo: K = U†ΛU, let φ(xi)=Λ^1/2U:i
			- kij = φ(xi)†φ(xj)
		- Polynomial kernels;
		- Sigmoid kernel: not Mercer;
	- 14.2.4 Linear kernels
	- 14.2.5 Matern kernels (commonly used in GP)
		- k(r) = 2^(1−ν)/Γ(ν) (√2νr/l)^ν Kν(√2νr/l)
		- κ(r) = exp(−r/l) if ν=1/2
		- D=1, Ornstein-Uhlenbeck process;
	- 14.2.6 String;
	- 14.2.7 Pyramid match kernels;
	- 14.2.8 Kernels derived from probabilistic generative models
		- κ(xi,xj) = ∫p(x|xi)^ρ p(x|xj)^ρdx
	- Fisher kernel;
		- κ(x, x^) = g(x)'F^(−1)g(x^)
		- F = ∇∇logp(x|θ)|θˆ
- 14.3 Using kernels inside GLMs
	- 14.3.1 Kernel machines
		- GLM with input feature: φ(x) = [κ(x, μ1),..., κ(x, μK)]
	- 14.3.2 L1VMs, RVMs, and other sparse vector machines
- 14.4 Kernel trick;
	- 14.4.1 Kernel NN;
		- ∥xi −xi′∥2 = ⟨xi,xi⟩+⟨xi′,xi′⟩−2⟨xi,xi′⟩
	- 14.4.2 Kernel K-medoids; (kernelized K-means)
	- 14.4.3 Kernel ridge regression;
		- Primal: J(w) = (y−Xw)†(y−Xw) + λ∥w∥^2
		- Optimal: w = (X†X+λI)^(−1)X†y=(Σxixi†+λI)^(−1)X†y
		- Dual:
			- Dual variable: α := (K + λI)^(−1)y
			- f(x) = w(x) = Σαiκ(x,xi)
	- 14.4 Kernel PCA;
		- PCA: S=(1/N)X†X (dxd)
		- XX† (NxN): let U, Λ be eigen of XX†: (XX†)U = UΛ
			- (X†X)(X†U) = (X†U)Λ
			- eigenvector of X†X is V=X†U and Λ
			- Normalized V=X†UΛ^(-1/2)
		- KPCA: S=(1/N)Σ_i φiφi†, let Φ=[φ1,φ2,...,φN]
			- eigenvector: V=(Φ†U)Λ^(-1/2)
			- a new test vector x∗ with φ∗:
				- φ∗†Vkpca = φ∗†ΦUΛ^(−1/2) = k∗†UΛ^(−1/2)
				- k∗ = [κ(x∗,x1),...,κ(x∗,xN)]
			- Make the data φ zero-mean:
				- φiˆ = φ(xi) - 1/N Σφ
				- Kˆ = HKH, H = I - 1/N 11†
		- In practice alg:
			- Centerize K as Kˆ=HKH;
			- [U, Λ]=eig(Kˆ);
			- Normalize vi=ui/λi
			- K∗ˆ = HK∗H;
			- Z∗ = K∗ˆV[:,:L]
- 14.6 Comparison of discriminative kernel methods
- 14.7 Kernels for building generative models
	- 14.7.1 Smoothing kernels
		- ∫κ(x)dx = 1, ∫xκ(x)dx = 0, ∫x^2κ(x)dx > 0
	- 14.7.2 Kernel density estimation (KDE)
		- p(x) = 1/N Σκ(x-xi)
	- 14.7.3 From KDE to KNN
	- 14.7.4 Kernel regression
		- f(x) = Σwi(x)yi (kernel regression, kernel smoothing, or the Nadaraya-Watson mode)
		- wi(x) = κ(x-xi)/Σκ(x-xi)
	- 14.7.5 Locally weighted regression
		- f(x) = Σyiκ(x,xi)
		- min_β(xˆ) Σ.i κ(xˆ,xi)[yi-β(xˆ)φ(xi)]
		- β(x∗) = (Φ†D(x∗)Φ)^−1 Φ†D(x∗)y

## Sparse Kernel Machines (PRML, Chap 7)
- Computational learning theory:
	- The goal of the PAC framework is to understand how large a data set needs to be in order to give good generalization;
- Relevance Vector Machines:
	- Bayesian version of SVM; SVM only gives decision boundary, no posterior;
	- Problem formulation:
		- p(t|x,w,β) ~ N(y(x), β^-1)
		- y(x) = Σ wn k(x, xn) + b
		- p(w|α) = ΠN(wi|0,αi^-1)
	- Learning: p(w|t,X,α,β) = N(w|m,Σ)
		- m = β Σ Φ† t
		- Σ = (A + βΦ†Φ)^-1
		- lnp(t|X,α,β) ~ N(t|0, C)
			- C = β^-1 I + Φ A^-1 Φ†
	- Approach 1: (direct-iterative): 1. α and β; 2. mean and variance in (7.82), (7.83);
	- Approach 2: EM;
	- RVM for classification:
		- For fixed α, Laplacian approximation:
		- α:

## Classical
- A. Rahimi and B. Recht. Random features for large-scale kernel machines. NIPS'08

## Deep Learning
- Y. Cho and L. K. Saul. Kernel methods for deep learning. NIPS'09
- N Jean, S Xie, S Ermon. Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance. NIPS'18
