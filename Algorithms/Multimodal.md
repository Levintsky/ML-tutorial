# Multimodal Learning

## Basics
- Tasks:
	- Multimodal feature learning;
		- CLIP, ...
	- VQA, image captioning;
	- Text-guided image manipulation/generation;
	- Text-guided RL/navigation;
- Model:
	- Contrastive learning: (CLIP, ...)
	- Cross-modality guided manipulation;
		- Latent optimization (GAN):
			- Optimize z s.t. cross-modality agree;
	- Low-shot learning:
		- PointCLIP CVPR'22
- Multitask:
	- DeepMind: A Generalist Agent.
- Tutorial:
	- Image + text: https://lilianweng.github.io/posts/2022-06-09-vlm/

## Platform & Dataset
- **Habitat**: M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh and D Batra. Habitat: A Platform for Embodied AI Research. ICCV'19
	- Tasks: Embodied QA, Language grounding, navigation;
	- Simulator: MatterPort3D, Gibson, Replic; https://github.com/facebookresearch/habitat-sim
	- Habitat-API: https://github.com/facebookresearch/habitat-api
- Large Vision + Language 
	- K Desai and J Johnson. VirTex: Learning Visual Representations from Textual Annotations. 2020
	- M B Sariyildiz, J Perez, and D Larlus. Learning Visual Representations with Caption Annotations. 2020
	- Y Zhang, H Jiang, Y Miura, C Manning, and C Langlotz. Contrastive Learning of Medical Visual Representations from Paired Images and Text. 2020
	- OpenAI. Learning Transferable Visual Models From Natural Language Supervision. 2021
	- **ALIGN**: Google-Brain. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. ICML'21
	- N Mu, A Kirillov, D Wagner, and S Xie. SLIP: Self-supervision meets Language-Image Pre-training. 2021
	- A Fürst, E Rumetshofer, V T Tran, H Ramsauer, F Tang, J Lehner, D P Kreil, M Kopp, G Klambauer, A Bitto-Nemling, and S Hochreiter. CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP, 2022.

## Multitask Learning
- DeepMind: A Generalist Agent.

## Embedding, Feature Learning
- Model:
	- BERT-based:
		- VilBert, VisualBERT, VL-BERT, UNITER, OSCAR, VideoBERT, ActBERT, Unicoder-VL, LXMERT, MERLOT, HERO, ALBEF, ...
	- Dual-encoder contrastive:
		- CLIP, ALIGN, CoCa, Florence, MIL-NCE, BASIC, LiT, FILIP, MMV;
	- VL models;
		- SimVLM, Virtex, MAGMA, Frozen, VisualGPT, ClipClap, VC-GPT, CM3, BLIP, Uni-Perceiver, VL-BART, VL-T5, VLM, Flamingo;
	- Learning protocol:
		- Jointly training image and text: VisualBERT, SimVLM;
		- Learn image embedding as (Frozen) LM Prefix: Frozen, ClipClap;
		- Cross-attention: VisualGPT, VisualCPT, MERLOT, Flamingo;
- BERT-style:
	- **ViLBERT**: J Lu, D Batra, D Parikh, S Lee. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. NeurIPS'19
	- **VideoBERT**: C Sun, A Myers, C Vondrick, K Murphy and C Schmid. VideoBERT: A Joint Model for Video and Language Representation Learning. ICCV'19
	- **VisualBERT**: L Li, M Yatskar, D Yin, C Hsieh, K Chang. VisualBERT: A Simple and Performant Baseline for Vision and Language. arxiv'19
		- https://github.com/uclanlp/visualbert
		- Insight:
			- Both image and text embedding input to a transformer;
			- Masked language model as task; 
	- G Sigurdsson, J Alayrac, A Nematzadeh, L Smaira, M Malinowski, J Carreira, P Blunsom, A Zisserman. Visual Grounding in Video for Unsupervised Word Translation. 2020
		- Extend VideoBert to multilingual
	- W Su, X Zhu, Y Cao, B Li, L Lu, F Wei, Jg Dai. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. ICLR'20
	- Y Chen, L Li, L Yu, A Kholy, F Ahmed, Z Gan, Y Cheng, J Liu. UNITER: UNiversal Image-TExt Representation Learning. ECCV'20
	- X Li, X Yin, C Li, P Zhang, X Hu, L Zhang, L Wang, H Hu, L Dong, F Wei, Y Choi, J Gao. Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks
- Dual-encoder contrastive:
	- **CLIP**: OpenAI. Learning Transferable Visual Models From Natural Language Supervision. 2021
	- **ALIGN**: Google-Brain. ALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. ICML'21
	- CoCa: Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu. CoCa: Contrastive Captioners are Image-Text Foundation Models.
	- **Florence**: MSR Florence: A New Foundation Model for Computer Vision.
	- **MIL-NCE**: DeepMind. End-to-End Learning of Visual Representations from Uncurated Instructional Videos.
		- https://www.di.ens.fr/willow/research/mil-nce/
- Visual LM: vision+prefix -> predict postfix text;
	- **VirTex**: K Desai, J Johnson. VirTex: Learning Visual Representations from Textual Annotations. CVPR'21
	- **Frozen**: M Tsimpoukelli, J Menick, S Cabi, A Eslami, O Vinyals, F Hill. Multimodal Few-Shot Learning with Frozen Language Models. ICLR'21
	- **SimVLM**: Z Wang, J Yu, A Yu, Z Dai, Y Tsvetkov, Y Cao. SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. ICLR'22
		- Both + language -> transformer;
		- GPT-like causal prediction task;
	- **ClipCap**: R Mokady, A Hertz, A Bermano. ClipCap: CLIP Prefix for Image Captioning. arxiv'21
		- Learn a light mapping to translate CLIP features into LM space;
	- MAGMA – Multimodal Augmentation of Generative Models through Adapter-based Finetuning.
	- J Chen, H Guo, K Yi, B Li, M Elhoseiny. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. CVPR'22
	- **Flamingo**: DeepMind. **Flamingo**: a Visual Language Model for Few-Shot Learning. 2022
- Retrieval:
	- IBM, Dialog-based Interactive Image Retrieval, NIPS'18
- Downstream tasks:
	- S Shen, H Li, H Tan, M Bansal, A Rohrbach, K Chang, Z Yao, and K Keutzer. How Much Can CLIP Benefit Vision-and-Language Tasks?

## Low-Shot Learning
- **Flamingo**: a Visual Language Model for Few-Shot Learning. 2022
- R Zhang, Z Guo, W Zhang, K Li, X Miao, B Cui, Y Qiao, P Gao, H Li. PointCLIP: Point Cloud Understanding by CLIP. CVPR'22
	- 3D PC project to 2D image;
	- CLIP to optimize text to get class;
- Zero-shot detection on images. ECCV'22

## Text-based Image Generation
- Latent space mixture to manipulate one domain:
	- Basics:
		- e.g. Text-based image inpainting/generation/...
		- Encode time: concatenate enc(Text)
		- Supervision time:
	- S Zhu, S Fidler, R Urtasun, D Lin, C C Loy. Be Your Own Prada: Fashion Synthesis with Structural Coherence. ICCV'17
		- Problem setup: input image + text; output new image (focus on fashion);
		- Model:
			- Image -> Enc() -> z1;
			- Text -> Enc() -> z2;
			- [z1, z2, zNoise] -> decoder -> I;
		- Supervision: Style-Gan like;
	- **Attngan**: T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. CVPR'18
	- Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language. NIPS'18
	- Yonsei University, Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language, NIPS'18
	- **StyleCLIP**: O Patashnik, Z Wu, E Shechtman, D Cohen-Or, D Lischinski. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery. ICCV'21 oral
		- https://github.com/orpatashnik/StyleCLIP
		- Approach 1: optimize latent w, s.t.
			- CLIP(G(w), text) close + regularization (close to original w and a face recognition)
		- Approach 2: latent mapper
			- w -> Mt(.) -> Δw
			- G(w+Δw) a real image, s.t. CLIP(.,text), L2(w,w0), Lid(.) all satisfy;
		- Approach 3: global direction;
			- Align: 
- **DALL-E**: OpenAI. Zero-Shot Text-to-Image Generation. 2021
	- https://github.com/openai/DALL-E
	- Stage 1: dVAE, 256x256-dVAE-32x32x8192 tokens; train φ and θ;
	- Stage 2: concatenate 256 BPE-encoded text tokens with the 32 × 32 = 1024 image tokens, and train an autoregressive transformer pψ(y, z).
	- x: image; y: caption; z: latent;
	- ln pθ,ψ(x, y) >= Ez∼qφ(z|x) ln pθ(x|y,z)−βKL(qφ(y,z|x), pψ(y,z))
		- Latent distribution qφ(z|x): 32x32 dVAE, K=8192 tokens, with Gumbel trick;
		- Image distribution pθ(x|y,z): Log-laplace NLL loss;
		- Text token joint distribution: pψ(y, z); 12-billion parameter sparse transformer
- **GLIDE**: A Nichol, P Dhariwal, A Ramesh, P Shyam, P Mishkin, B McGrew, I Sutskever, and M Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. 2021
	- https://github.com/openai/glide-text2im.
	- CLIP-guidance v.s. classifier-free guidance;

## Vision + Action
- Speaker-Follower Models for Vision-and-Language Navigation (NIPS 2018)
	- Speaker Model (generate diverse instruction for data augmentation)
	- Follower Model
- TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environment, NIPS 2018
	- Navigate based on instructions
	- Find the TouchDown, describe, with LINGUNet
	- Good way to collect data

## Image/Video Caption
- Good Tutorials
	- https://www.leiphone.com/news/201805/aQXoPMX51DD1C7VC.html
- Metrics:
	- BLEU, METEOR, ROUGE-L, CIDEr
- Benchmarks:
	- MSCOCO
	- http://www.sohu.com/a/313942390_651893?sec=wd
	- **Visual genome**: Connecting language and vision using crowd-sourced dense image annotations.
	- Visual Dialog. CVPR 2017
		- https://visualqa.org/challenge.html
	- GQA:
		- Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei. Visual7W: Grounded Question Answering in Images. 2017
		- https://cbmm.mit.edu/research/projects-thrust/vision-and-language/grounded-question-answering
		- Object-level grounding;
- Unclassified
	- Partially-Supervised Image Captioning. NIPS'18
	- D Lin. A Neural Compositional Paradigm for Image Captioning. NIPS'18
	- Weakly Supervised Dense Event Captioning in Videos. NIPS'18
	- Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog. NIPS'18
	- F Liu, Y Liu, X Ren, X He, X Sun. Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations. NIPS'19
- Misc
	- **NeuralTalk**: A. Karpathy. https://github.com/karpathy/neuraltalk
	- A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. CVPR 2015
	- R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles. Dense-Captioning events in videos. ICCV 2017
	- G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby talk: Understanding and generating image descriptions. CVPR 2011
	- J. Lu, J. Yang, D. Batra, and D. Parikh. Neural baby talk. CVPR 2018
	- O Vinyals, A Toshev, S Bengio, D Erhan. **Show and Tell**: Lessons learned from the 2015 MSCOCO Image Captioning Challenge. 2015
	- K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.
	- **NMN**: Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering with neural module networks. CVPR'16.
	- C. Wang, H. Yang, C. Bartz, and C. Meinel. Image captioning with deep bidirectional lstms. ACMMM'16
		- Training: bidirectional;
		- Inference: uni-directional; Beam search in each direction; BiDirectional RNNs to rescore candidates;
	- **BiBS**: Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-in-the-Blank Image Captioning. CVPR'17
	- Novel Visual Concept (NVC) dataset https://github.com/mjhucla/NVC-Dataset
		- TF-mRNN: https://github.com/mjhucla/TF-mRNN
		- mRNN-CR: https://github.com/mjhucla/mRNN-CR
- VQA:
	- Misc:
		- Learning Conditioned Graph Structures for Interpretable Visual Question Answering. NIPS'18
		- Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering. NIPS'18
		- J Kim, J Jun, B Zhang. Bilinear Attention Networks. NIPS'18
	- Vqa: Visual question answering. ICCV'15.
	- Devi, Dhruv: Visual Dialog. CVPR 2017
		- https://visualqa.org/challenge.html
	- A-star best performer:
		- LSTM for the question to encode a vector;
		- Vector as query, do attention, softmax, sum on CNN spatial feature;
		- Vector produces all answers (VQA has an answer set);
	- **MAC**: D Hudson, C Manning. Compositional Attention Networks for Machine Reasoning. ICLR'18
		- MAC recurrent unit
	- **MCB**: A Fukui, D H Park, D Yang, A Rohrbach, T Darrell, M Rohrbach. Multimodal Compact Bilinear Pooling for VQA. EMNLP'16
	- **SOA**: P Anderson, X He, C Buehler, D Teney, M Johnson, S Gould, L Zhang. Bottom-up and top-down attention for image captioning and visual question answering. CVPR'18
		- Faster RCNN + ResNet-101
	- **MUREL**. Remi Cadene, Hedi Ben-younes, Matthieu Cord, Nicolas Thome. MUREL: Multimodal Relational Reasoning for Visual Question Answering. 2019
		- https://github.com/Cadene/murel.bootstrap.pytorch
	- Baselines:
		- https://github.com/Cyanogenoid/pytorch-vqa
	- Counting:
		- Learning to Count Objects in Natural Images for Visual Question Answering, ICLR 2018
			- https://github.com/Cyanogenoid/vqa-counting
	- Attention:
		- Z. Yang, X. He, J. Gao, L. Deng, and A. J. Smola. Stacked attention networks for image question answering. CVPR'16

## Visual Reasoning
- **NMN**: J Andreas, M Rohrbach, T Darrell, D Klein. Neural Module Networks. CVPR'16
	- Compositional reasoning;
- Justin Johnson, Judy Hoffman, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick. Inferring and Executing Programs for Visual Reasoning. ICCV'17
	- Insight: program generator + execution engine; built on NMN; train by REINFORCE;
	- Algorithm:\
		<img src="/Grounding/images/vqa-exe.png" alt="drawing" width="400"/>
	- Program generator: LSTM seq2seq, output predicted program z;
	- Execution engine: input program z and image x, a=phi(x, z).
- Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli and Joshua B. Tenenbaum. Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding, NIPS 2018
	- An interpretable VQA model that disentangles language reasoning from visual understanding
	- For visual understanding, first perform objects segmentation and then learn to obtain structural scene representation (with supervision) such as color, size, shape, position.
	- For language reasoning, they learn to translate natural language question into a deterministic program such as filter_shape(scene, large) or count(scene). 
	- Finally, they execute the program on the structural scene representation to obtain the final answer
	- 99.8% on CLEVR
- **VCR**: Visual Commonsense Reasoning. CVPR'19

## RL + Language
- Program Guided
	- S Sun, T Wu, J Lim. Program Guided Agent. ICLR'20
		- Input: perception (2D) + DSL program;
		- Output: action;
- Navigation
	- H Yu, H Zhang, W Xu. A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment. 2017
	- K M Hermann, F Hill, S Green, F Wang, R Faulkner, H Soyer, D Szepesvari, W M Czarnecki, M Jaderberg, D Teplyashin, M Wainwright, C Apps, D Hassabis, P Blunsom. Grounded Language Learning in a Simulated 3D World
		- Instruction based
		<img src="/RL/images/navigation/language-based.png" alt="drawing" width="500"/>
- Deep Reinforcement Learning with a Natural Language Action Space
- Gated-Attention Architectures for Task-Oriented Language Grounding
- Language Understanding for Text-based Games using Deep Reinforcement Learning, EMNLP 2015
- https://github.com/devendrachaplot/DeepRL-Grounding
