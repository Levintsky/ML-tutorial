# Learning Theory

## Basics
- https://cs229.stanford.edu/extra-notes/hoeffding.pdf
- Tutorial:
	- Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning- From Theory to Algorithms. 2014
- Resources:
	- https://zhuanlan.zhihu.com/p/337298338
	- https://docs.google.com/spreadsheets/d/1sLxEPagWrCiYBbkpLUcr5VOjgGjaZ-5mMsrKhTDhm40/edit#gid=0

## Prepration
- **Markov Inequality**: X>0 random var,
	- P(X≥a) ≤ E[X]/a
	- Proof: divide by X≥a and X≤a, bound each part;
- **Chebyshev**:
	- P(|Z-E[Z]|≥t) ≤ Var(Z)/t^2
	- Proof: Markov with |Z-E[Z]| as positive var;
- **Chernoff bounds**: for ∀ t ≥ 0,
	- P(Z ≥ E[Z] + t) ≤ minλ≥0 E[exp(λ(Z−E[Z]))]exp(−λt) = minλ≥0 M(z-E[z],λ)exp(−λt)
	- P(Z ≤ E[Z] - t) ≤ minλ≥0 E[exp(λ(E[Z]-Z))]exp(−λt) = minλ≥0 M(E[z]-z,λ)exp(−λt)
	- Proof: Markov with exp[λ(Z-E[Z])] as positive var;
		- P(Z−E[Z] ≥ t) = P(exp(λ(Z−E[Z])) ≥ exp(λt)) ≤ E[exp(λ(Z−E[Z]))]exp(-λt)
- **MGF**: moment generation function;
	- M(z, λ) := E[exp(λz)]
		- M(z, λ) ≤ exp(C^2λ^2/2), ∀ λ ∈ R, for some C ∈ R, depends on distribution of z. Always used with Chernoff bound;
	- Gaussian Z ∼ N (0, σ^2),
		- M(z, λ)=exp(σ^2λ^2/2)
	- Rademacher rv (±1 with p=1/2)
		- E[exp(λS)] ≤ exp(λ^2/2)
		- Combine with Chernoff to bound the sum of random sign Z=∑Si
		- P(Z≥t) ≤ E[exp(λZ)]exp(-λt) ≤ exp(nλ^2/2)exp(-λt)
			- Choose λ to minimize nλ^2/2-λt
			- P(Z≥t) ≤ exp(-t^2/2n)
- **Hoeffding's lemma**: required for Hoeffding's Inequality, zi∈[a, b] bounded
	- E[exp(λ(z−E[z]))] ≤ exp(λ^2(b-a)^2/8), ∀ λ ∈ R
	- Insight: symmetrization + Jensen + Rad-inequality, zi∈[a, b] required to bound (z-z')^2.
	- Proof:
		- Ez(exp(λ(z−E[z]))) = Ez(exp(λ(z−Ez'[z']))), (symmetrization)
		- ≤ Ez[Ez' exp(λ(z-z'))], (Jensen)
		- = Ez,z'[Es[exp(λs(z-z'))|z,z']], (Introduce S ∈ {−1, 1} is a random sign variable)
			- Es[exp(λs(z-z'))|z,z'] ≤ exp(λ^2(z-z')^2/2), (Rademacher rv, i.e. E[exp(λS)] ≤ exp(λ^2/2))
		- ≤ exp(λ^2(a-b)^2/2), (weaker version, with 2 not 8)
- **Hoeffding**: z1, z2, ... zn independent, zi ∈ [a, b] with a,b bounded. X̄ = 1/n ΣXi, let μ=E[x]
	- P(|X̄−μ|≤ε) ≥ 1 - 2exp(-2nε^2 / (b-a)^2)
	- Insight: Chernoff + Hoeffding Lemma;
	- Proof:
		- p(X̄−μ ≥ t) ≤ E[exp(λn(X̄−μ))]exp(-λnt); (Chernoff)
		- = ∏E[exp(λ(zi-μ))]exp(-λnt)
		- ≤ ∏exp(λ^2(b-a)^2/8) exp(-λnt); (Hoeffding's Lemma)
			- Minimize w.r.t. λ≥0, we have
		- p(X̄−μ ≥ t) ≤ exp(-2nt^2/(b-a)^2)
- **Jensen**: f: R → R convex, then f(E[z]) ≤ E[f(z)]
- Symmetrization: z' an indepdent copy of z.

## Supervised Learning Formulations (Stanford CS-229/Stat-214)
- 1.1 Supervised learning
	- training set;
	- predictor, hypothesis, model;
	- expected loss, population loss, expected risk, population risk;
	- hypothesis class, hypothesis family, excess risk;
- 1.2 Empirical risk minimization
	- empirical risk: on training set;
		- equal to population risk in expectation;

## Asymptotic Analysis (Stanford CS-229/Stat-214)
- 2.1 Asymptotics of empirical risk minimization
	- L(θˆ) − infL(θ) ≤ c/n + o(1/n)
	- Theo 2.1 θˆ →p θ∗ as n → ∞
		- √n(θˆ−θ∗) →d Gaussian
		- n(L(θˆ)−L(θ∗)) →d Gaussian
	- Central Limit Theorem
	- Theo 2.4 (MLE paradigm) suppose P(y|x; θ), θ∈Θ; loss MLE, nll l(xi,yi,θ)=-logP(y|x; θ), then
		- θ converge;
		- E[∇l(θ∗)] = 0;
		- Cov(∇l) = ∇^2 L; (integral by part)
		- √n(θˆ−θ∗) →d N(0, ∇2L(θ∗)^−1).
- 2.2 Limitations of asymptotic analysis
	- Obscure dependencies on higher order terms

## Concentration Inequalities (Stanford CS-229/Stat-214)
- Insight:
	- 1. X1 +...+Xn concentrates around E[X1 +...+Xn].
	- 2. More generally, f(X1,...,Xn) concentrates around E[f(X1,...,Xn)].
- 3.1 The big-O notation
- 3.2 Chebyshev's inequality
	- Theo 3.1 (**Chebyshev**)
		- P(|Z-E[Z]|≥t) ≤ Var(Z)/t^2
	- Intuition: tail behavior, density decay at least at 1/t^2;
	- Proof: requires Markov Inequality:
		- Let X=(Z-μ)^2 and a=t^2, apply Markov;
- 3.3 Hoeffding's inequality
	- Theo 3.2 (**Hoeffding**) X1, X2, ... Xn independent real, ai ≤ Xi ≤ bi almost surely. X̄ = 1/n ΣXi, let μ=E[x]
		- P(|X̄−μ|≤ε) ≥ 1 - 2exp(-2n^2ε^2 / Σ(bi-ai)^2)
	- 1/n^2 Σ(bi-ai)^2: upper bound or proxy of Var(Xi)
	- Take ε=O(σ√logn)=σ√(clogn), P(|X̄−μ|≤ε) decays at n^(-2c)
	- Compact form when Xi are bounded;
- 3.4 Sub-Gaussian random variables
	- Def. E[exp λ(X-μ)] ≤ exp(σ^2λ^2/2). σ-sub-Gaussian and say it has variance proxy σ^2.
	- Theo 3.7 sub-Gaussian.
		- Pr[|X−μ|≥t] ≤ 2exp(−2t^2/σ2), ∀t∈R.
	- Theo 3.10 sum of sub-Gaussian.
	- 3.4.1 Examples of sub-Gaussian random variables
		- Rademacher random variables
		- Random variables with bounded distance to mean
		- Bounded random variables
- 3.5 Concentrations of functions of random variables
	- Theo 3.15 (**McDiarmid**'s inequality) f(X1,...,Xn) not overly sensitive to change of a single coord. Then
		- P(|f(X1..,Xn)-E[f]|≥t) ≤ 2exp(−2t^2/Σci^2)
	- 3.5.1 Bounds for Gaussian random variables
		- Theo 3.18 (**Gaussian Poincare inequality**) X1, X2, ..., Xn i.i.d. N(0,1)
			- Var(f(X1,...,Xn)) ≤ E[|∇f(X1,...,Xn)|^2]
		- Theo 3.19 (**Wainwright**) f: L-Lipschitz w.r.t. Euclidean distance, Xi i.i.d. N(0,1)
			- P(|f(X)-E[f(X)]|≥t) ≤ 2exp(−t^2/2L^2)

## Generalization Bounds via Uniform Convergence
- Insight:
	- non-asymptotic analysis;
- 4.1 Basic concepts
	- Uniform convergence is a property of a parameter set Θ, which gives us bounds of the form:
		- Pr[|Lˆ(θ)−L(θ)|)≥ε] ≤ δ; ∀θ ∈ Θ.
	- 4.1.1 Motivation: Uniform convergence implies generalization
		- L(θˆ)−L(θ∗) = [L(θˆ)−Lˆ(θˆ)] + [Lˆ(θˆ)−Lˆ(θ∗)] + [Lˆ(θ∗)−L(θ∗)]
		- 1st: ?
		- 2nd: <0
		- 3rd: O(1/√n) via Hoeffding's inequality (l(.,.) bounded)
	- 4.1.2 Deriving uniform convergence bounds
		- 1. Suppose we have a bound of the form Pr[|Lˆ(θ) − L(θ)| ≥ ε′] ≤ δ′ for some single, fixed choice of θ.
		- 2. all possible values of θ.
	- 4.1.3 Intuitive interpretation of uniform convergence
		- Uniform convergence implies generalization, excess risk is small;
		- Asymptotic: L(θ) “close” to Lˆ(θ)
		- Uniform: shape L clsoe to Lˆ
- 4.2 Finite hypothesis class
	- Theo 4.1 H is finite, loss l() bounded in [0, 1], then with p≥1−δ
		- |L(h)-Lˆ(h)| ≤ √[(ln|H|+ln(2/δ))/2n]
	- Proof: union of sum
	- 4.2.1 Comparing Theorem 4.1 with standard concentration inequalities
		- ∀h ∈ H, w.h.p. |L(h)−L(h)| ≤ O(1/√n), concentration inequality
		- w.h.p., ∀h ∈ H |L(h)−L(h)| ≤ O(ln|H|/√n), uniform convergence
	- 4.2.2 Comparing Theorem 4.1 with asymptotic bounds
		- L(hˆ) − L(h∗) ≤ c/n + o(n^−1)
- 4.3 Bounds for infinite hypothesis class via discretization
	- Assume parameter within a L2-ball:
		- H={hθ: θ ∈ R, ∥θ∥ ≤ B}.
	- 4.3.1 Discretization of the parameter space by ε-covers
		- Def: ε-cover
		- Lemma 4.5 (ε-cover of l2 ball) B,ε > 0, and let S = {x ∈ Rp : ∥x∥ ≤ B}, then there exists an ε-cover with at most max((3B√p/ε)^p ,1) elements.
			- Proof: set C = {x ∈ S : xi =ki√p, ki ∈Z, |ki| ≤ B√p/ε} grid has at most max((3B√p/ε)^p ,1) balls.
	- 4.3.2 Uniform convergence bound for infinite H
		- Def. κ-Lipschitz functions: |L(θ) − L(θ′)| ≤ κ∥θ − θ′∥.
		- Theo 4.8 l((x, y), θ) ∈ [0, 1] and κ-Lipschitz in θ w.r.t. l2-norm. Then with p≥1−O(exp(−Ω(p))), we have:
			- ∀θ, |Lˆ(θ)−L(θ)| ≤ O(√(p max(ln(κBn), 1)/n))
		- Proof sketch: get finite ε-cover and bound with theo 4.1, and extend to all θ in each ball by Lipschitz.
		- Proof: Fix δ, ε > 0, let C be a ε-cover.
			- Event (within the ball) E={∀θ ∈ C, |Lˆ(θ)−L(θ)| ≤ δ). P(E) ≥ 1-2|C|exp(-2nδ^2); (Theo 4.1)
			- ∀θ ∈ S, we pick a ball containing it (θ0 ∈ C s.t. ∥θ−θ0∥ ≤ ε)
				- Within the ball, |Lˆ(θ)−L(θ)| ≤ |Lˆ(θ)−Lˆ(θ0)|+|Lˆ(θ0)−L(θ0)|+|L(θ0)−L(θ)| ≤ 2κε + δ;
				- Let ε = δ/(2κ), |Lˆ(θ) − L(θ)| ≤ 2δ
			- We set δ = √[c0p max(1,ln(κBn)))/n], with c0=36
				- Since ln |C| ≤ p ln(3B/(δ/2)), (Lemma 4.5)
				- ln|C|−2nδ^2 ≤ pln(6Bκ/δ) - 2nδ ≤ ... ≤ -p
			- Therefore, with
				- p ≥ 1 − 2|C|exp(−2nδ^2) = 1 − 2exp(ln|C|−2nδ^2) ≥ 1−O(exp(−p)),
				- |L(θ) − L(θ)| ≤ 2δ = O(√p/n max(1, ln(κBn))
- 4.4 Rademacher complexity
	- 4.4.1 Motivation for a new complexity measure
		- 1. If the hypothesis class H is finite,
			- L(hˆ) − Lˆ(hˆ) ≤ O(√(log|H|/n))
		- 2. p-dimensional
			- L(hˆ) − Lˆ(hˆ) ≤ O(√(p/n))
		- not precise enough: it depends solely on p and is not always optimal.
			- L(hˆ) − Lˆ(hˆ) ≤ O(√(Complexity(Θ)/n))
	- 4.4.2 Definitions
		- A weaker goal:
			- Uniform (supereme): sup_h∈H(L(h)−Lˆ(h))
			- Here (mean): E[sup(L(h)−Lˆ(h))] ≤ upper bound.
		- Def 4.10 (**Rademacher complexity**) f: Z → R,
			- Rn(f) := Ez[Eσ[sup 1/n∑σif(zi)]]
		- Theo 4.13 Ez[sup_f[1/n∑f(zi) - E_z[f(z)]]] ≤ 2Rn(f)
			- Proof: symmetrization.
		- e.g. 15. Binary classification, y ∈ {±1}, l0-1((x,y),h)=(1-yh(x))/2, zero-one loss;
			- Rn(F) = Rn(H)/2
	- 4.4.3 Dependence of Rademacher complexity on P
		- Extreme example: z=z0 almost surely, Rn(f) = 1/√n
- 4.5 Empirical Rademacher complexity
	- Def 4.17 (**Empirical Rademacher complexity**) Rs(F), on dataset S={z1,z2,...,zn}
	- Theo 4.18 f∈F, 0≤f(z)≤1, Then with p≥1−δ
		- sup_f[1/n∑f(zi) - E_z[f(z)] ≤ 2Rs(F) + 3√[log(2/δ)/2n]
		- Proof:
			- Let g(z1,...,zn) = sup_f[1/n f(zi)−E[f(z)]]
			- 1. McDiarmid: change one z, diff ≤ 1/n
				- Then, p(g≥E[g]+ε) ≤ exp(-2nε^2); (McDiarmid)
			- 2. E[g] ≤ 2Rn(F); (Theo 4.13)
			- 3. empirical Rad-comp g'(z1,...,zn)=Rs(F)
				- g'(z1,...,zn)-g'(z1,..,zi',.,zn) ≤ 1/n, (similar to step-1, for emp)
				- p(g'-E[g']≥ε) ≤ exp(-2nε^2); (McDiarmid)
			- 4. set δ s.t. exp(−2nε^2) = δ/2, our goal (excess diff) satisfies with p≥1−δ:
				- g ≤ E[g] + ε
				-   ≤ 2Rn(F) + ε; (4.13)
				-   ≤ 2(Rs(F)+ε) + ε; (step-3)
				-   = 3Rs(F) + ε
	- 4.5.1 Rademacher complexity is translation invariant
		- Prop 4.5.1 Proposition 4.5.1. Let F be a family of functions mapping Z → R and define F′ = {f′(z) = f(z) + c0 | f ∈ F} for some c0 ∈ R. Then RS(F) = RS(F′) and Rn(F) = Rn(F′).
- 4.6 Covering number upper bounds Rademacher complexity
	- All potential output space:
		- Q := {(f(z1),...,f(zn)) : f ∈ F} ⊆ Rn,
	- Prop 4.6.1 (**Massart’s finite lemma**) if 1/√n |v| ≤ M < ∞
		- Rs(F) ≤ √[2M^2log|Q|/n]
	- Corollary 4.21 f: z->R, if for all f, √[1/n Σf(zi)^2] ≤ M
		- Rs(F) ≤ √[2M^2log|F|/n]

## PAC Basics
- Generalization error: E(h, D)
- Empirical error: E^(h,D)
- Inequalities:
	- Jensen: f() convex, then f(E(x)) <= E(f(x))
	- Hoeffding: m sample, P(|E^x-Ex|>=ε) <= exp(-2m ε^2)
	- McDiarmid: suppose sup|f(x1,..xi,..)-f(x1,..xi',..)|<=c, then p(|f(x)-f(xi')|>=ε)<=exp(-2ε^2/Σ(ci^2))
- Algorithm: L
- Concept: C: X to Y;
- Mapping: h: X to Y;
- Hypothesis space: H, containing all h;
- Separable/consistent: all correct
- Non-separable/consistent:
- PAC-identify: 0 < ε, δ < 1, P(E(h)<=ε)>=1-δ, then L can identify concept C from H;
- PAC-learnable: exist algorithm L, poly s.t. m>=poly(1/ε, 1/δ, |x|, |c|), L can PAC-identify C;
	- Insight: L can identify C, with error at most ε, with prob 1-δ;
- PAC-learning algirthm;
- Sample Compexity: minimum m, s.t. m>=poly();
- Case 1: separable:
	- m >= (ln|H| + ln(1/δ))/ε, O(1/m) convergence;
	- Intuition, a bad h() with generalization error > ε, but do well on training set is at most exp(-m ε);
- Case 2: non-separable:
	- p(E^(h)-E(h)>=ε)<= exp(-2m ε^2) (Hoeffding)
	- Agnostic PAC learnable: suppose h' best in hypothesis space, then L can learn P(E(h)-E(h')<=ε)>=1-δ;
- VC-dimension:
	- Growth function: all number of possibilities π(H,m)=max{|(h(x1),...,h(xm)|h in H}
	- Dichotomy:
	- Shattering: m samples, at most 2^m possibilites. Every assignment: dichotomy, pi(H,m)=2^m, then D is shattered;
	- VC-dim: max m s.t. π(H,m)=2^m;
	- Theorem: VC(H)=d, then P(E(h)-E^(h)) has a bound;
	- VC dimentionality is **distribution independent**;
- Rademacher complexity:
	- A tighter bound (distribution dependent);
	- Given z1, ..., zm, E_σ (sup_h (σ1 h(x1)+σ2 h(x2)+...)/m), where {σ} is a random variable half +1, half -1;
- Stability:
	- Three costs: l(L,D) generalization; l^(L,D) empirical; l(L,D-i) leave-one-out;
	- beta-uniform stability: |l(L,D)-l(L,D-i)| <= β;
	- ERM (empirical-risk-minimization)
	- Theorem: algorithm L ERM and beta-stable, then hypothesis H learnable;

## PAC-Bayes
- Currently the best theory to explain NN generalization;
- Legacy:
	- Shawe-Taylor J, Williamson R C. A PAC analysis of a Bayesian estimator[C]//Proceedings of the tenth annual conference on Computational learning theory. ACM, 1997
	- David McAllester. Some PAC-Bayesian theorems. COLT'98
		- Concept space observes any distribution D;
		- Prior in hypothesis space H;
		- Measure of C and H space;
		- U: a subspace of H;
		- P(E(U)<=...)>=1-δ; E(U) is generalization error;
	- David McAllester. Some PAC-Bayesian theorems. Machine Learning'99
		- First paper on PAC-Bayes;
	- McAllester D A. PAC-Bayesian model averaging. COLT'99
	- John Langford and John Shawe-Taylor. Pac-bayes & margins. NIPS'02
	- McAllester D A. PAC-Bayesian stochastic model selection. Machine Learning'03
	- David McAllester. Simplified pac-bayesian margin bounds. 2003
	- Seeger M. Pac-bayesian generalisation error bounds for gaussian process classification. JMLR'03
	- Shawe-Taylor J, Langford J. PAC-Bayes & margins. NIPS'03
	- Langford J. Tutorial on practical prediction theory for classification. JMLR'05
	- Germain P, Lacasse A, Laviolette F, et al. PAC-Bayesian learning of linear classifiers. ICML'09
	- Catoni O. PAC-Bayesian supervised classification: the thermodynamics of statistical learning. arxiv'07	
- Matthew Holland. PAC-Bayes under potentially heavy tails. NIPS'18

## ERM
- Chi Jin, Lydia T. Liu, Rong Ge, Michael I. Jordan. On the Local Minima of the Empirical Risk. NIPS'18
- Marco Loog, Tom Viering, Alexander Mey. Minimizers of the Empirical Risk and Risk Monotonicity. NIPS'18

## Mixture
- Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes

## NIPS'18 Learning Theory
- Fengxiang He, Tongliang Liu, Dacheng Tao. Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence
- Kevin Bello, Jean Honorio. Exact inference in structured prediction
- Sushrut Karmalkar, Adam Klivans, Pravesh Kothari. List-decodable Linear Regression
- Chenri Ni, Nontawat Charoenphakdee, Junya Honda, Masashi Sugiyama. On the Calibration of Multiclass Classification with Rejection
- Chen Dan, Hong Wang, Hongyang Zhang, Yuchen Zhou, Pradeep Ravikumar. Optimal Analysis of Subset-Selection Based L_p Low-Rank Approximation
- Yihe Dong, Samuel Hopkins, Jerry Li. Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved Outlier Detection
- Dylan Foster, Spencer Greenberg, Satyen Kale, Haipeng Luo, Mehryar Mohri, Karthik Sridharan. Hypothesis Set Stability and Generalization
- Sauptik Dhar, Vladimir Cherkassky, Mohak Shah. Multiclass Learning from Contradictions
- Gilad Yehudai, Ohad Shamir. On the Power and Limitations of Random Features for Understanding Neural Networks
- Amir Najafi, Shin-ichi Maeda, Masanori Koyama, Takeru Miyato. Robustness to Adversarial Perturbations in Learning from Incomplete Data
- Yaqi Duan, Tracy Ke, Mengdi Wang. State Aggregation Learning from Markov Transition Data
- Nika Haghtalab, Cameron Musco, Bo Waggoner. Toward a Characterization of Loss Functions for Distribution Learning
