# Learning Theory on Probility Models

## Mixture Models
- H Ashtiani, S Ben-David, N Harvey, C Liaw, A Mehrabian, Y Plan. Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes. NeurIPS'18
	- Insight: Θ(kd2/ε2) samples are necessary and sufficient for learning a mixture of k Gaussians in Rd, up to error ε in total variation distance.