# Information Theory

## Basics
- Entropy
	- H(X) = -Σp(x)logp(x)
- KL-divergence
	- KL(p|q) = Σp(x)log[p(x)/q(x)]
	- Cross entropy: -Σp(x)logq(x)
	- Theo-2.8.1: KL(p|q) ≥ 0 with equality iff p = q
- Mutual Info
	- I(X;Y) = ΣΣp(x,y)log[p(x,y)/p(x)p(y)]
	- I(X;Y) = H(X) - H(X|Y)
	- Data Processing Inequality (DPI): X → Y → Z, then
		- I(X;Y) ≥ I(X;Z)
	- Reparametrization invariance: Two invertible functions f1, f2, then
		- I(X;Y) = I(f1(X);f2(Y))
- IB:
	- x → z → y, z as info-bottleneck
	- Learn z with high MI(z,y) and low MI(z,x); (max-ent principle)
	- Forces Z to act like a minimal sufficient statistic of X for predicting Y;
	- Maximize: I(Z,Y;θ) − βI(Z,X;θ)
		- Lower-bound: I(Z,Y), by approx q(y|z)
			- I(Z,Y) ≥ E.p(y,z)[log(q(y|z))] + H(y), gap: KL(p(y|z),q(y|z))
		- Upper-bound: I(Z,X), by approx q(z)
			- I(Z,X) ≤ E.p(x,z)[log[p(z|x)/q(z)]], gap: KL(p(z)|q(z))
	- Connection with VAE: no Y, just index i=1/N, each item a class, then:
		- max I(Z, X) − βI(Z, xi),
- F-divergence: more general:
	- Df(P|Q) = ∫f(dP/dQ)dQ = ∫f(dp(x)/dq(x))q(x)dx
	- Let f(x)=xlogx -> KL-divergence
	- DV (Donsker-Varadhan) representation (dual of KL)
		- KL(p,q) = sup.g E.p[g(X)] - log(E.q[exp(g(X))])
		- Proof by construction: when G=P, gap is zero;
- Info-plane:
	- X-axis: The sample-complexity of hi depends on I(X;hi).
		- how many samples to achieve certain perf.
	- Y-axis: The perf (generalization accuracy) depends on I(hi;Y).
- Resources:
	- K-Murphy-2.8
	- D MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003.

## Information Bottleneck
- Application: IB Methods
	- Markov-Cahin: Y → X → S(X) → Xˆ
		- Xˆ = argmin I(S(X);X) s.t. I(S(X);Y) = I(X;Y)
	- Relaxation: Shamir, Sabato [TCS'10]
		- Xˆ = argmin.p(xˆ|x) I(Xˆ;X)-βI(Xˆ;Y)
	- Rate-Distortion with KL; Bachrach Navot [COLT'06]
		- d.IB(x, xˆ) = KL(p(y|x), p(y|xˆ))
	- DPI + Statistical consistency; Harremoes [ISIT'08]
- IB: N Tishby, F Pereira, and W Biale. The information bottleneck method. Allerton'99
- W Bialek, I Nemenman, and N Tishby. Predictability, complexity, and learning. NC'01
- S Still and W Bialek. How many clusters? an information-theoretic perspective. NC'04
- D Barber and F Agakov. The IM algorithm: a variational approach to information maximization. NIPS'04
- N Slonim, G Atwal, G Tkacik, and W Bialek. Information-based clustering. PNAS'05
- O Shamir, S Sabato, and N Tishby. Learning and generalization with the information bottleneck. Theoretical Computer Science'10
- S Palmer, O Marre, M Berry, and W Bialek. Predictive information in a sensory population. PNAS'15
- A Achille and S Soatto. Information dropout: Learning optimal representations through noisy computation. 2016.
- A Alemi, B Poole, I Fischer, J Dillon, R Saurous, and K Murphy. An information-theoretic analysis of deep latent-variable models. 2017
- DVIB: A Alemi, I Fischer, J Dillon, K Murphy. Deep Variational Information Bottleneck. ICLR'17
	- Formulation:
		- max_θ I(Z,Y;θ) s.t. I(X,Z;θ) <= Ic
		- Goal: maximize R = I(Z,Y;θ) − βI(Z, X; θ)
	- Assumption: Y ↔ X ↔ Z
		- p(Z|X,Y) = p(Z|X), makes unsupervised learning possible;
		- I(Z, Y) ≥ E_yz[log(q(y|z))] + H(y)
		- I(Z, X) ≤ E_xz[log[p(z|x)/q(z)]]
		- Loss: I(Z,Y;θ) − βI(Z, X; θ) with ELBO;
		- L ≈ 1/N Σn E_z|xn[logq(yn|z) - βlog[p(z|xn)/q(z)]]
		- Encoder: p(z|x) = N(z|f.µe(x), f.Σe(x)),
			- Reparametrization: p(z|x)dz = p(ε)dε
		- JIB = 1/N Σn Eε[−logq(yn|f(xn, ε))] + βKL[p(Z|xn), q(Z)].
- A Saxe, Y Bansal, J Dapello, M Advani, A Kolchinsky, B Tracey, D Cox. On the Information Bottleneck Theory of Deep Learning. ICLR'18
- X B Peng, A Kanazawa, S Toyer, P Abbeel, S Levine. Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow. ICLR'19

## Mutual information
- Application of MI:
	- InfoGAN: MI(c, G(c, z)) as a regularizer;
		- When learn G() maximize MI between c and generated G(c, z);
		- min.G max.D L.GAN(D, G) - λLI(c, c')
	- SSL:
		- NLP: PPMI, PMI(w, c) = log[p(w,c)/p(w)p(c)]
		- CV: MI(aug1(x), aug2(x));
	- InfoGAIL:
		- GAIL: min.π max.D E.π[logD(s,a)] + E.demo[log(1-D(s,a))] - λH(π)
		- InfoGAIL: min.π,Q max.D L-GAIL(π, D) - λLi(π, Q)
			- Q(c|τ): context
- M Donsker and S Varadhan. Asymptotic evaluation of certain markov process expectations for large time. '83
- S Nowozin, Cseke, and R Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. NeurIPS'16
- MINE: I Belghazi, A Baratin, S Rajeswar, S Ozair, Y Bengio, A Courville, and R Hjelm. Mine: mutual information neural estimation. ICML'18
	- Problem setup: estimate MI;
	- Upper bound with neural approx: I(X;Z) ≥ I(X,Z;Θ)
	- Learn f: (x, z; Θ) → R to maximize
		- V = 1/b ∑f(xi,zi;Θ) - log[1/b ∑exp[f(xi,zi;Θ)]]
		- Minibach + SGD;
	- Application: maximize MI to improve GAN on mode-collapse;
- DIM: R Hjelm, A Fedorov, S Lavoie-Marchildon, K Grewal, P Bachman, A Trischler, Y Bengio. Learning deep representations by mutual information estimation and maximization. ICLR'19
	- https://github.com/rdevon/DIM
	- Problem setup: unsupervised learning;
	- Model:
		- Im -> [conv] -> f-low -> [conv] -> f-high
		- Im1 -> f1-high+f1-low -> [D] -> real
		- Im2 -> f1-high+f2-low -> [D] -> fake
	- Formulation:
		- ψ = argmax I(X;E(ψ(X)));
		- Loss: global-MI + local-MI + prior-matching;
	- Different ways to estimate MI:
		- MINE: based on DV (Donsker-Varadhan representation)
		- JS-MI:
		- InfoNCE:
- AMDIM: P Bachman, R Hjelm, W Buchwalter. Learning Representations by Maximizing Mutual Information Across Views. NIPS'19
	- https://github.com/Philip-Bachman/amdim-public
	- Problem setup: unsupervised learning;
	- Insight: maximizing mutual information between features extracted from multiple views of a shared context;
- M Gabrié, A Manoel, C Luneau, J Barbier, N Macris, F Krzakala, L Zdeborová. Entropy and mutual information in models of deep neural networks. NeurIPS'18

## DL Training behavior
- Resources:
	- https://lilianweng.github.io/lil-log/2017/09/28/anatomize-deep-learning-with-information-theory.html
	- Information Theory in Deep Learning (Youtube): https://www.youtube.com/watch?v=bLqJHjXihK8&feature=youtu.be
- DPI in neural net:
	- H(X) ≥ I(X;hi) ≥ I(X;hi+1) ≥ ...
	- I(X;Y) ≥ I(hi;Y) ≥ I(hi+1;Y) ≥ ...
- Two Optimization Phases:
	- Early: ∥μ∥ >> ∥σ∥
	- Later: std σ becomes much noiser;
	- σ noiser when a layer gets further away from output;
- N. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. In IEEE Information Theory Workshop, 2015
- R Shwartz-Ziv and N Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017
	- Deep networks undergo 2 phases: initial fitting + compression;
	- Fitting: fit the data.
	- the compression: learns to ignore irrelevant details;
	- I(x;h) v.s. I(h;y) first moves up, then moves left;
- G Pereyra, G Tuckery, J Chorowski, and L Kaiser. Regularizing neural networks by penalizing confident output predictions. ICLRW'17

## DL Theory
- Learning Theory:
	- Old Generalization Bounds:
		- https://mostafa-samir.github.io/ml-theory-pt1/
		- https://mostafa-samir.github.io/ml-theory-pt2/
		- ε^2 < (log|Hε|+1/δ) / 2m
	- New Input compression bound;
