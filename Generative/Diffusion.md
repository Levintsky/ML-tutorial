# Diffusion Process

## Basics
- Framework:
	- x0: signal; xT ~ N(0, I)
	- Prob model: Markov-Chain: xT → ... → xt → xt-1 → x0
		- Denoising: p(x0:T)=p(xT)∏p(xt-1|xt)
		- latent model: p(x0) = ∫p(x0:T;θ)dx1:T
		- p(xt-1|xt) ~ N(μ(xt, t), Σ(xt, t))
	- Approx: x0 → x1 → x2 → ... → xT 
		- Forward/diffusion process: q(x1:T|x0)=∏q(xt|xt-1);
		- q(xt|xt-1) ~ N(√(1-βt)xt-1, βt)
		- β can be constant or learned by reparametrization trick;
	- Goal: maximise p(x0;θ) = ∫p(x0:T) dx1:T
		- VAE trick, z as x1:T
		- ELBO: E[-logp(x0)] = Eq[-log p(x0:T)/q(x1:T|x0)]
		- q(xt|x0) ~ N(xt; √(α't)x0, (1-α't)I), because of Gaussian additive!
		- **Traceable p(xt-1|xt,x0)** given x0, xt
		- Loss: KL(q(xt-1|xt,x0)|p(xt-1|xt))
	- Training: optimize any term of Lt-1=KL(q(xt-1|x0, xt) || p(xt-1|xt))
	- Inference: start from XT, xt-1=xt+Σt z, z ~ N(0, 1) Langevin dynamics;
- Continuous time variance-preserving Markov process:
	- z = {zλ | λ ∈ [λmin, λmax]};
	- q(zλ|x) ~ N(αλx, σλ^2I), where αλ^2 = 1/(1+e−λ), σλ^2 =1−αλ^2;
	- q(zλ|zλ′) ~ N((αλ/αλ′)zλ′, σλ′I), where λ<λ′, σ2′ =(1−eλ−λ′)σλ2
	- Forward: decreaing λ;
- Score-based: check math/DiffEqn.md
	- Connetion denoising to estimating score ∇xlog(p(x)) with s(x; θ)
	- Reverse-time SDE: dX = [μt - σt^2 ∂logp/∂x]dt + σt dBt
	- Known score ∂logp/∂x, we can recover signal:
		- xt - xt+Δt = [μt - σ^2 ∂logp/∂x]Δt + σ√Δt
	- NCSN: perturb with different noise level, estimate s(x; θ) with one neural net;
- Guidance:
	- Classifier: ε̃θ(zλ,c) = εθ(zλ,c) − wσλ ∇zλ logpθ(c|zλ) ≈ −σλ ∇zλ[logp(zλ|c) + wlogpθ(c|zλ)],
	- Classifier-free: ε̃θ(xt|c) = εθ(xt|∅) + w(εθ(xt|c) − εθ(xt|∅))
- Continuous time (Connection with score-based):
	- ∇x logp(x; θ)
- Techniques:
	- Key design for any extension methods:
		- Efficient q(xt|x0);
		- Efficient q(xt-1|xt, x0);
	- Network design: U-Net;
	- Loss/objective weighting: E[wt ∥ε-ε(xt;θ)∥^2]
		- DDPM: ignore, wt ≡ 1;
		- MLE: wt = β(t)/σt^2
	- Cascade:
	- Noise scheduling: βt (forward variance):
		- DDPM: linearly increase: β1=1e-4, βT=0.02
		- Improved DDPM: cosine-based schedule;
		- Kingma NeurIPS'22: SNR;
		- DPM: inference-time;
	- Parametrization of Σ(θ);
		- DDPM: Σ(xt, t;θ) = σt^2 I;
		- Improved DDPM: learned;
	- Speedup:
		- Improved DDPM: reduced to S steps; (by [T/S] times)
			- Sample every [T/S] steps;
		- DDIM: sampling only S steps;
- Tutorial:
	- https://lilianweng.github.io/posts/2021-07-11-diffusion-models/
- Software:
	- MidJourney: https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F
	- RunwayML;
	- https://stability.ai/

## Diffusion Model
- J Sohl-Dickstein, E Weiss, N Maheswaranathan, and S Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. ICML'15
	- Insight: 1st to propose Markov Chain and piecewise KL loss;
- DDPM: J Ho, A Jain, P Abbeel. Denoising Diffusion Probabilistic Models. NeurIPS'20
	- https://github.com/hojonathanho/diffusion
	- Denote αt:=1−βt and α't:=∏s=1..t αs
	- q(xt|x0) ~ N(√(α't)x0, (1−α't)I)
	- q(xt−1|xt,x0) ~ N(μ't(xt,x0),β'tI),
		- with μt(xt,x0):=√(α't−1)βt/(1-α't)x0 + √(αt)(1−α't−1)/(1-α't)xt, 
		- β't=(1−α't−1)βt/(1-α't)
	- Loss for one-step: Lt−1 = Eq(|μ't(xt,x0)−μθ(xt,t)|^2/2σt2) + C
		- Reparametrize: xt(x0, ε) = √(α't)x0 + √(1−α't)ε for ε∼N(0, I)
	- High-quality inference result with Langevin dynamics;
- DDIM: J Song, C Meng, and S Ermon. Denoising diffusion implicit models. ICLR'21
	- Key insight: extend DDPM to non-Markov, given xt, sample x0 then xt-1;
	- 待定系数法/Method of undetermined coefficients:
		- 保持 p(xt|x0)与ddpm一致;
		- 求解p(xt−1|xt,x0), s.t. 边缘分布保持:
			- ∫p(xt−1|xt,x0)p(xt|x0)dxt = p(xt-1|x0)
		- 2方程, 3未知数;

## Continuous Time, Score SDE/ODE
- Denote: p.data as data 分布, p(x;θ) = exp[-f(x;θ)]/Z(θ)
	- Score function定义 data := ∇x logp(x)
	- Model: s(x; θ) = ∂logp(x; θ)/∂x = -∇x f(x;θ); Z(θ)是θ函数, ∇x Z(θ)=0;
		- 神经网络 fit score: J(θ) = 1/2 E.p(x)[∥s(x;θ) − ∇x logp(x)∥^2]
		- Equivalent to: E[p'(x)[tr(∇x sθ(x)) + 2∥sθ(x)∥^2]]
	- Inference: Langevin dynamics;
		- x.t+1 = x.t + ε/2 ∇x logp(xt) + √ε zt, zt ~ N(0, I)
- A Hyvärinen. Estimation of non-normalized statistical models by score matching. JMLR'05
	- Insight: first paper on score matching;
	- Theo-1: Assume s(x; θ) is differentiable, with weak regularity cond. Then loss can be formulated as:
		- J(θ) = E.q′(x)[E.p(x)[∂s(xi)/∂xi + si(xi)^2/2)]] + C
		- Proof: integral by part;
- Denoising score matching: P Vincent. A Connection Between Score Matching and Denoising Autoencoders. NC'11
	- Insight: first paper on conditional score matching;
		- assume data ~ q(x'|x), circumvents tr(∇x sθ(x))
	- L(θ) = E.p(x0)p(xt|x0)[∥s(x;θ) - ∇x p(x|x0)∥^2]
- Y Song, S Garg, J Shi, and S Ermon. Sliced score matching: A scalable approach to density and score estimation. UAI'19
	- Insight: efficient with a random vector v, p(v)
		- Ep(v)[Ep(data)[v† ∇xsθ(x) v + 1/2|sθ(x)|^2]]
- NCSN/SMLD: Y Song, S Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. NeurIPS'19
	- Insight: prove the equivalence of diffusion model and SDE;
		- Training: s(x, σ;θ)
		- Inference: annealed Langevin dynamics;
	- Noise Conditional Score Network (NCSN): sθ(x, σ)
		- θ=argmin_θ σi^2 Ep'(x)Epσi(x'|x) ∥sθ(x',σi)-∇x logpσi(x'|x)∥^2.
- Y Song and S Ermon. Improved Techniques for Training Score-Based Generative Models. NeurIPS'20
	- Tech 1: initial noise σ1 = max.i,j(|xi-xj|);
	- Tech 2: geometric progression γ选择;
	- Tech 3: sθ(x,σ) = sθ(x)/σ;
	- Tech 4: inference: selecting T and ε;
	- Tech 5: EMA θ′ ← mθ′ + (1 − m)θi;
		- Inference time 用θ′;
- Y Song, J Sohl-Dickstein, D Kingma, A Kumar, S Ermon, B Poole. Score-Based Generative Modeling through Stochastic Differential Equations. ICLR'21
	- Insight: unify SMLD and DDPM in a SDE framework, propose a stochastic solver (PC) and a deterministic (ODE);
	- Define p(xt|x0) ~ N(αt'x0, βt'^2I), boundary condition: α0=1, α1=0, β0=0, β1=1;
	- 给定p(xt|x0)边缘分布, 求SDE: 待定系数法:
		- dx = fdt + gt dw
		- Condition: ∫p(xt+Δt|xt,x0)p(xt|x0)dxt = p(xt+Δt|x0)
		- ft = 1/αt (dαt/dt)
		- gt^2 = 2αtβt d/dt(βt/αt)
	- DDPM and SMLD as SDE:
		- Forward: dx = f(x,t)dt + g(t) dw
		- Reverse: dx = [f(x, t) - g(t)^2 ∇xlogpt(x)]dt + g(t)dw
		- SMLD as VE-SDE: dx = √(d(σ(t)^2)/dt)dw;
		- DDPM as VP-SDE: dx = -1/2 β(t)xdt + √(β(t))dw
		- Proposed new VP-SDE: dx = -1/2 β(t)xdt + √(β(t)(1-exp(-2∫0..t β(s)ds))dw
	- 数值解 + Predictor/Corrector;
		- Predictor: xi = (2-√(1-β))xi+1 + βi+1 sθ(xi+1, i+1)) + √(βi+1)z
		- Corrector: xi = xi + εi sθ(xi, i) + √(εi)z
- ODE:
	- Poisson flow generative models. NeurIPS'22
	- Interpretable ODE-style Generative Diffusion Model via Force Field Construction. 2023
	- Meta-AI. Flow Matching for Generative Modeling. 2023
- Fast ODE/SDE solver:
	- Euler: Song '20, '21
	- Heun solver: NVIDIA. Elucidating the design space of diffusion-based generative models. NeurIPS'22
		- https://github.com/NVlabs/edm
	- NVIDIA Caltech. Fast sampling of diffusion models via operator learning. NeurIPSW'22
		- Insight: Li'20 FNO-PDE-solver;
			- semi-linear ODE: 积分因子凑全微分ODE;
		- DFNO: diffusion Fourier Neural Operator模拟solver;
		- Temporal-conv block FFT, then iFFT;
	- GaTech: Fast sampling of diffusion models with exponential integrator. ICLR'23
		- https://qsh-zh.github.io/deis

## Techniques
- Latent space:
	- Esser et al., ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis, NeurIPS'21
		- https://compvis.github.io/imagebart/
		- Autoregressive + Diffusion;
	- NVIDIA: Score-based generative modeling in latent space. NeurIPS'21
		- https://nvlabs.github.io/LSGM
		- Decompose KL into negative entropy and cross entropy;
			- q(z0) non-Normal, can't analytical;
	- Stable diffusion;
- Variance/noise scheduling:
	- Time fed in as spatial addition or adaptive group norm;
	- Training time: (noise scheduling)
		- D Kingma, T Salimans, B Poole, and J Ho. Variational diffusion models. arxiv'21
			- Learned SNR(t) for noise scheduling;
			- 理论上 KL()项 写成 SNR 形式;
		- Improved DDPM: A Nichol and P Dhariwal. Improved Denoising Diffusion Probabilistic Models. ICML'21
			- https://github.com/openai/improved-diffusion
			- learn σt -> fewer steps;
			- mixture with v:
			- Σ(xt, t;θ) = exp[vlogβt + (1-v)logβt']
		- On the Importance of Noise Scheduling for Diffusion Models, 2023.
	- Inference time:
		- Analytic-DPM: F Bao, C Li, J Zhu, B Zhang. Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models. ICLR'22
		- Extended-Analytic-DPM: F Bao, C Li, J Sun, J Zhu, B Zhang. Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models. ICML'22
			- Allow different items on diagonal;
		- Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
- Advanced reverse process:	More complicated than Gaussian;
	- Gao et al., Learning energy-based models by diffusion recovery likelihood, ICLR'21
		- Insight: EBM f(x;θ) 变为 f(x, t;θ)
	- Xiao et al., Tackling the Generative Learning Trilemma with Denoising Diffusion GANs, ICLR'22
		- GAN-style for D(q(xt-1|xt), p(xt-1|xt))
		- Discriminator: time-dependent D(xt; t);
		- Generator: x0 = G(xt, z, t), then sample xt-1 based on estimated x0;
- Condition/Guidance:
	- Choi et al., ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models, ICCV'21
		- https://github.com/jychoi118/ilvr_adm
		- Unconditional proposal: xt -> xt-1
		- Then condition refinement: 
	- Classifier: log pφ(y|xt);
		- ADMNet: OpenAI. Diffusion Models Beat GANs on Image Synthesis. 2021
			- Insight: increase IS score but decreased diversity;
			- µθ(xt|y) = µθ(xt|y) + s Σθ(xt|y)∇xt log pφ(y|xt)
			- Use ε̃θ(zλ,c) as score in place of εθ(zλ,c) ≈ −σλ∇zλ logp(zλ|c);
				- ε̃θ(zλ,c) = εθ(zλ,c) − wσλ ∇zλ logpθ(c|zλ) ≈ −σλ ∇zλ[logp(zλ|c) + wlogpθ(c|zλ)],
				- w controls guidance strength;
		- SDG: Berkeley. More Control for Free! Image Synthesis with Semantic Diffusion Guidance. WACV'23
			- https://xh-liu.github.io/sdg/
			- Shifted mean: pθ(xt−1|xt)pφ(y|xt−1) ~ N(μ+Σg,Σ)
			- g = ∇xt−1 log pφ(y|xt−1)
	- Classifier-free (implicit):
		- J Ho and T Salimans. Classifier-Free Diffusion Guidance. NeurIPS'21 Workshop
			- One-model for score εθ(xt) and εθ(xt|y), set y=∅;
			- class-conditional diffusion model θ(xt|y) is replaced with a null label ∅ with a fixed probability during training:
				- εθ(xt|y) = εθ(xt|∅) + s(εθ(xt|y) − εθ(xt|∅))
			- Implicit classifier: pi(y|xt)∝p(xt|y)/p(xt)
				- ∇xtlog p(xt|y) ∝ ∇xt log p(xt|y) − ∇xt log p(xt) ∝ ε∗(xt|y)−ε∗(xt)
			- εθ(xt|c) = εθ(xt|∅) + w(εθ(xt|c) − εθ(xt|∅))
- Cascaded/multi-resolution:
	- J Ho, C Saharia, W Chan, D Fleet, M Norouzi, and T Salimans. Cascaded Diffusion Models for High Fidelity Image Generation. 2021
	- GLIDE, 
- Adding more noise: Mimicing inference time;
	- GLIDE,
- Fast/Acceleration
	- E Luhman and T Luhman. Knowledge distillation in iterative generative models for improved sampling speed. 2021
		- https://github.com/tcl9876/Denoising_Student
	- Brain. On distillation of guided diffusion models. NIPSW'22
		- Insight: 2 stages for classifier-free diffusion;
		- Stage 1: student for both conditional and unconditional;
		- Stage 2: distillation for fewer steps;
	- Salimans & Ho, Progressive distillation for fast sampling of diffusion models, ICLR'22
		- Smaller model and half-steps each distillation;
		- Progressive: student becomes teacher next;
- Discrete state diffusion:
	- Max-Welling. Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions, NeurIPS'21
		- Argmax Flows:
		- Multimodal diffusion: cat + transition-matrix;
	- D3PMs: Google-Brain. Structured Denoising Diffusion Models in Discrete State-Spaces, NeurIPS'21
		- q(xt|x0) = Cat(xt;p = x0 Qt'), with Qt' = Q1 Q2...Qt
		- Marginal: q(xt−1|xt,x0) = q(xt|xt-1,x0)q(xt-1|x0)/q(xt|x0)
		- Designing Qt to be:
			- Uniform, absorbing, ...
	- Chang Huiwen, MaskGIT: Masked Generative Image Transformer, CVPR'22
		- https://masked-generative-image-transformer.github.io/
		- Stage 1: Tokenizer;
		- Stage 2: Mask scheduling;
	- Google-Brain. Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning. ICLR'23
		- https://masked-generative-image-transformer.github.io/
		- Bit -> continuous (analog bits)
		- Two techniques:
			- Self-conditioning;
			- Asymmetric time intervals;

## Downstream Tasks
- Edit:
	- Sdedit: Image synthesis and editing with stochastic differential equations. ICLR'22
		- https://github.com/ermongroup/SDEdit
		- Add noise, start from t ∈ [0.3, 0.6]
	- ControlNet: L Zhang, M Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. 2023
	- Imagen editor: https://imagen.research.google/editor/
- Text-to-image:
	- CLIP: given text c, image x, (f(x), g(c)) as guidance;
	- GLIDE: OpenAI. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. 2021
		- https://github.com/openai/glide-text2im.
		- Insight: CLIP-guidance in classifier style: generate im xt s.t. f(xt) is close to g(c) 
		- µθ(xt|c) = µθ(xt|c) + s Σθ(xt|c) ∇xt[f(xt)g(c)]
	- DALL-E-2: OpenAI. Hierarchical Text-Conditional Image Generation with CLIP Latents. 2022
		- Image caption pair (x, y), embedding (z.im, z.text);
			- CLIP: z.im (ViT), z.text (GPT-3), trained with SAM;
		- Prior: P(z.im|text)
			- Predict image embedding z.im based on caption y;
			- CLIP + DALL-E dataset;
			- Prior 1: AR with quantization; PCA + SAM?
			- Prior 2: DDPM without ε;
		- Decoder: pretrained and fixed; to invert image encoder;
			- P(x|y) = P(x,z.im|y) = P(x|z.im, y)P(z.im|y), 
			- Decoder: zi(64x64) - 256x256 - 1024x1024
			- Stage 1 (GLIDE): Gaussian blur;
			- Stage 2 (upsampler): BSR degradation, unconditional ADMNet;
	- Stable diffusion: R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer. High-Resolution Image Synthesis with Latent Diffusion Model. CVPR'22
	- Imagen: Google-Brain. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. NeurIPS'22
		- Cascaded + classifier-free guidance + dynamic thresholding + forzen LLM (T5-XXL)
		- https://imagen.research.google
- Super-resolution
	- SR3. Google-Brain. Image Super-Resolution via Iterative Refinement. ICCV'21
		- upsampling + Gaussian blur;
	- K Zhang, J Liang, L V Gool, and R Timofte. Designing a Practical Degradation Model for Deep Blind Image Super-Resolution. ICCV'21
		- BSR noise;
- Colorization:
	- Saharia, Palette: Image-to-Image Diffusion Models, arXiv'21
- Outpainting/panorama;
- Video Generation
	- https://github.com/showlab/Awesome-Video-Diffusion
	- Yang et al., Diffusion Probabilistic Modeling for Video Generation, arXiv, 2022
	- Brain. J. Video diffusion models. ICLRW'22
		- https://video-diffusion.github.io/
	- Brain. Imagen video: High definition video generation with diffusion models. 2022
		- https://imagen.research.google/video/
	- Harvey et al., Flexible Diffusion Modeling of Long Videos, NeurIPS'22
	- FAIR. Make-A-Video: Text-to-Video Generation without Text-Video Data.
	- Höppe et al., Diffusion Models for Video Prediction and Infilling, arXiv, 2022
	- Voleti et al., MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation, arXiv, 2022
	- NVIDIA: Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. CVPR'23
		- https://research.nvidia.com/labs/toronto-ai/VideoLDM/
- AE, with latent code:
	- Preechakul et al., Diffusion Autoencoders: Toward a Meaningful and Decodable Representation, CVPR'22
- Audio/1D Generation
	- NVIDIA: DiffWave: A Versatile Diffusion Model for Audio Synthesis. 2020
	- Brain: Wavegrad: Estimating gradients for waveform generation. ICLR'21
	- Grad-TTS: A diffusion probabilistic model for text-to-speech. 2021
- Semantic Segmentation:
	- Baranchuk et al., Label-Efficient Semantic Segmentation with Diffusion Models, ICLR'22
		- https://github.com/yandex-research/ddpm-segmentation
		- DDPM feature + MLP for sem-seg;
	- A Generalist Framework for Panoptic Segmentation of Images and Videos: https://arxiv.org/abs/2210.06366
- Inverse-problems: 
	- Medical imaging: reconstruct from sparse signal;
	- Song, Y., Shen, L., Xing, L., and Ermon, S. Solving inverse problems in medical imaging with score-based generative models. ICLR'22
	- Diffusion posterior sampling for general noisy inverse problems. ICLR'23
	- Pseudoinverse-guided diffusion models for inverse problems. ICLR'23
- 3D:
	- Zhou et al., 3D Shape Generation and Completion through Point-Voxel Diffusion, ICCV'21
	- Luo and Hu, Diffusion Probabilistic Models for 3D Point Cloud Generation, CVPR'21
	- DreamFusion: https://dreamfusion3d.github.io
- Data for training:
	- Synthetic Data from Diffusion Models Improves ImageNet Classification: https://arxiv.org/abs/2304.08466

## Trendy
- Control/conditional:
	- Erasing Concepts from Diffusion Models
- Scalable Diffusion Models with Transformers, Dec 2022
	- https://www.wpeebles.com/DiT.html
- simple diffusion: End-to-end diffusion for high resolution images, 2023.
- NVIDIA. ediff-i: Text-to-image diffusion models with ensemble of expert denoisers. 2022
- Genie: Higherorder denoising diffusion solvers. 2022
- Kawar, B., Elad, M., Ermon, S., and Song, J. Denoising diffusion restoration models. 2022
- Photorealistic text-to-image diffusion models with deep language understanding. 2022
- A Bansal, E Borgnia, H Chu, J Li, H Kazemi, F Huang, M Goldblum, J Geiping, T Goldstein. Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise. 