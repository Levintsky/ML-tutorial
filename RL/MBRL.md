# Model-based RL

## Basics
- Motivation and application:
	- Robotic control, safety, human-AI interaction, games, science (chemical synthesis), operation research;
- Problem setup:
	- Transition: st+1 = fs(st, at)
	- Reward: rt+1 = fr(st, at)
- Learned models:
	- States: s = [x, dx/dt, φ];
		- Galileo [Wu, 2015], GNN [ICML'18], local-linear [Yip 2014]
	- Observations: image; (reconstuct image)
		- Ebert, Finn, [2018]; Finn & Levine [2017]; Finn, Goodfellow, & Levine [2016]
	- Latent space: z; (reconstruct image from z)
		- World-Models [Ha, NeurIPS'18]
		- State-Space-transition [Buesing ICML'18]
		- Structured Latent: MONet [Burgess 2019], COBRA [Watters 2019]
	- Object-centric;
	- Recurrent value models:
		- DeepMind: muzero;
- Models:
	- Parametric
	- Non-Parametric:
		- Kovar. Motion Graphics. [TOG'02]
		- Gaussian Process: PILCO [ICML'11]
		- Transition graphs: Composable planning [ICML'18], Koltun Semi-parametric [ICLR'18]
		- Empirical replay buffer: DeepMind. When to use [NeurIPS'19]
- How to leverage models in RL?
	- Simulation: mix real/model-gen experience, and apply model-free RL;
		- Tzeng [17]; Dexterous [OpenAI'19]
	- Assist learning: Policy backprop, SVG, dreamer;
		- ∂s′/∂s, ∂s′/∂a, ∂r/∂a, ∂r/∂s;
		- Backprop through s′ = fs(s,a) and r=fr(s,a)
		- SVG [15], dream-to-control [Hafner'20], Imagined-value-grad [Byravan'20]
	- Strengthen policy; inf/learning time;
- Plan with models:
	- BG planning (learning time):
		- Learn θ for any situation; (Habit)
		- Simulate: dyna, MVE, MBPO;
		- Assist learning: Policy backprop, SVG, dreamer;
	- Decision-Time (inference planning):
		- Plan a0, a1, ... for current goal; (Improvisation)
		- Discrete actions: Heuristic search, MCTS;
		- Continuous:
			- Shooting: iLQR, DDP, PIˆ2;
			- Collocation: direct, STOMP;
	- Challenge:
		- Sensitivity and poor conditioning: error will accumulate;
			- Collocation;
		- Only local optimum: CEM/PI^2 to avoid;
		- Slow convergence: 2nd-order opt
			- LQR '72; linear dyn + quadratic reward;
			- iLQR: Todorov '05
			- DDP: fr = s†Qs + a†Ra
- Model-based control in the loop:
	- Offline-RL: MOReL, MOPO;
	- Data Augmentation;
	- Plan under imperfect model: replan (closed loop);
		- Replan with MPC (model-predictive control);
		- Plan conservatively:
			- with avg/worst model: Rajeswaran EPOpt;
			- Ensemble-CIO;
	- Stay close to states where model is certain;
		- Implicitly: Peters [12]; GPS [14]
		- Explicitly: MOReL;
	- Estimate model uncertainty: GP; BNN; Pseudo-count; Ensembles;
- Combine planning with learning;
	- Distillation: Dagger;
	- Terminal value functions
	- Planning as policy improvement 
	- Implicit planning: VIN [NeurIPS'16], GVIN [AAAI'18], VPN [NeurIPS'17] Diff-CEM [Amos 20]
- Others: curiosity/... (check Explore)
- From Spinningup
	- Model learned: I2A, MBMF, MVE, STEVE, ME-TRPO, MB-MPO, world-models...
	- Model given: AlphaGo, Exit;
- Resources:
	- Mordatch/Hamrick Tutorial (ICML'20)
	- https://zhuanlan.zhihu.com/p/72642285
	- Sergey Levine (294 lec-11,12)

## Basics
- Learn model and plan (without policy)
	- Iteratively collect more data to overcome distribution mismatch
	- Replan every time step (MPC) to mitigate small model errors
	- Learn policy
- Backpropagate into policy (e.g., PILCO) – simple but potentially unstable
	- Imitate optimal control in a constrained optimization framework (e.g., GPS)
	- Imitate optimal control via DAgger-like process (e.g., PLATO)
	- Use model-free algorithm with a model (Dyna, etc.)
- Linear case:
	- LQR: linear transition, quadratic cost
	- Solved with backward recursion (dynamic programming)
- Nonlinear case: DDP/iterative LQR
	- iLQR: approx with LQR locally;
- Learn dynamics (294, lec-12)
	- Uncertainty-aware models
	- Aleatoric or statistical uncertainty
	- Epistemic or model uncertainty
	- Bayesian neural network
	- Bootstrap ensembles p(θ|D)
	- Moment matching (project to Gaussian)

## Unclassified
- A Strehl, L Li, M Littman. Incremental model-based learners with formal learning-time guarantees. UAI'06
- **MBIE-EB**: A Strehl, M Littman. An analysis of model-based interval estimation for Markov decision processes. JoCS'08
	- Init: ε, δ, m;
	- n(s,a,s')=0, rc(s,a)=0, n(s,a)=0, Q(s,a)=1/(1−γ);
	- Loop:
		- at = argmax Q(st,a);
		- Observe rt and state st+1;
		- Update stat n(s,a), n(s,a,s'), rc(s,a);
		- Update model: R(st,at) and T(s'|s,a)
		- Set Q(s, a) as exploration based reward by looping until convergence:
			- Q(s,a) = R(s,a) + γΣT(s'|s,a)maxQ(s',a) + β/√(n(s,a))

## Over-Optimistic
- Model-based Offline batch RL:
	- Learn a model penalize model uncertainty during planning;
	- Empirically very promising on D4RL tasks;
	- MOPO: T Yu, G Thomas, L Yu, S Ermon, J Zou, S Levine, C Finn, T Ma. MOPO: Model-based Offline Policy Optimization. NIPS'20
	- R Kidambi, A Rajeswaran, P Netrapalli, T Joachims. MOReL: Model-Based Offline Reinforcement Learning. NIPS'20

## MDP/POMPD
- Stanford cs-234 (Lec-2)
- Assumption:
	- Dynamics and reward model available;
	- p(st+1|st, at) = p(st+1|ht, at)
- MDP + π(a|s) = MRP (Markov Reward Process)
	- V = R + γPV -> V = (I-γP)^-1 R
	- Bellman backup (iterative):
		- V(s;π) = Σ_a π(a|s)[r(s,a) + γΣ_s' p(s'|s,a)V(s';π)]
- Optimal: π∗(s) = argmax_π V(s;π)
	- Existance and unique;
- State-action: Q(s,a;π) = R(s,a) + γΣ_s' p(s'|s,a)V(s';π)]
	- Policy iteration (PI): iteratively do value-eval and policy improvement;
		- Policy improvement: take argmax, then follow π, always gets better;
		- π∗(s) = argmax_a Q(s,a;π)
	- Value iteration (VI):
		- V=BV, or V(s) = max_a[R(s,a)+γΣ_s' p(s'|s,a)V(s')]
- Policy evaluation:
	- MC: Gt = rt + γrt+1 + γ^2rt+2 + ... (high var, unbiased)
		- Incremental: G(s) += Gi,t, V(s;π)=G(s)/N(s)
	- TD(0): V(s) += α(r+γV(st+1)-V(st)), (low var, biased)
- Thompson Sampling for MDP:
	- Init prior p(Ras), p(T(s'|s,a))
	- Loop:
		- Sample: (s,a), T(s'|s,a), R(s,a)
		- Compute optimal Q;
		- at = argmaxQ(s,a)
		- Observe rt, st+1;
		- Update posterior p(Ras|rt), p(T|st+1) using Bayesian rule;

## Learn a Model for Data Generation
- S Gu, T Lillicrap, I Sutskever, S Levine. Continuous Deep Q-Learning with Model-based Acceleration. ICML'16
- G Kalweit, J Boedecker. Uncertainty-driven Imagination for Continuous Deep Reinforcement Learning CoRL2017.
	- Insight: model could be inaccurate!
	- Experience Replay Buffer divided into 2 categories:
		- traditional replay buffer
		- imaginary replay buffer
- **ME-TRPO**: Kurutach. Model-Ensemble Trust-Region Policy Optimization. ICLR'18

## Learn a Model for Planning
- Model for Better Value Estimation
	- **MVE**: V Feinberg, A Wan, I Stoica, M Jordan, J Gonzalez, S Levine. Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning. ICML'18
		- Insight: approximate, few-step simulation of a reward- dense environment
		- Problem setup: continuous state and action;
		- RL: AC, on-policy; (IS not required)
	- **STEVE**: Google-Brain. Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion. NIPS'18
- Learn a model as context:
	- **I2A**: DeepMind. Imagination-Augmented Agents for Deep Reinforcement Learning. NIPS'17
		- Framework: two-streams
			- Model-based: Imag xn;
			- Model-free:
		- Game: Sokoban;
	- VPN: V Feinberg, A Wan, I Stoica, M Jordan, J Gonzalez, Sergey Levine. Value Prediction Network. NIPS'17
		- https://github.com/junhyukoh/value-prediction-network
- MuZero: DeepMind. Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. 2019
	- MuZero = VPN (learn abstract states + dynamics) + AlphaZero-planning (MCTS)

## Learn a Model
-  DeepMind. Neural Predictive Belief Representations, ICLR'19
	- 1-step frame prediction
	- Two variants of Contrastive-Predictive Coding (CPC), CPC|Action
	- DeepMind Lab: navigation;
- J Oh, X Guo, H Lee, R Lewis, and S Singh. Action-conditional video prediction using deep networks in atari games. arxiv'15.
- D Ha, J Schmidhuber. Recurrent World Models Facilitate Policy Evolution， NeurIPS'18
	- VAE to encode frames for compression and regularization
	- RNN to Predict next step
	- Game: CarRacing, 
	- https://worldmodels.github.io
- SimPLe: Google-Brain.Model Based Reinforcement Learning for Atari. ICLR'20
	- https://ai.googleblog.com/2019/03/simulated-policy-learning-in-video.html
	- Open sourced at https://github.com/tensorflow/tensor2tensor
	- Insight: learn world model to generate data, learn policy in world model;
		- Play/interact -> Update-world model -> policy-world-model;
	- World model: ff CNN predict next frame and reward from past 4 frames;
	- RL: PPO
	- Experiment: 100k interactions;

## Continuous (MuJoCo, Robotics)
- Gaussian-Process Model (non-parametric):
	- C Rasmussen and M Kuss. Gaussian processes in reinforcement learning. NIPS'03
	- J Kocijan, R Murray-Smith, C Rasmussen, and A Girard. Gaussian process model based predictive control. ACC'04
	- J Ko, D Klein, D Fox, and D Haehnel. Gaussian processes and reinforcement learning for identification and control of an autonomous blimp. ICRA'07
	- D. Nguyen-Tuong, J. Peters, and M. Seeger. Local Gaussian process regression for real time online model learning. NIPS'08
	- A. Grancharova, J. Kocijan, and T. A. Johansen. Explicit stochastic predictive control of combustion plants based on Gaussian process models. Automatica'08
	- M Deisenroth, C Rasmussen, D Fox. Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning. RSS'11
		- Gaussian Process to learn the dynamics
	- M Deisenroth, D Fox, and C Rasmussen. Gaussian processes for data-efficient learning in robotics and control. PAMI'14
	- S Kamthe and M Deisenroth. Data-efficient reinforcement learning with probabilistic model predictive control. AISTATS'18
- Bayesian Model:
	- E Hernandaz and Y Arkun. Neural network modeling and an extended DMC algorithm to control nonlinear systems. ACC'90
	- W Miller, R Hewes, F Glanz, and L Kraft. Real-time dynamic control of an industrial manipulator using a neural network-based learning controller. 1990
	- L Lin. Reinforcement Learning for Robots Using Neural Networks. 1992
	- A Draeger, S Engell, and H Ranke. Model predictive control using neural networks. 1995
- Local linear/NN (time-varying) Model:
	- Guided policy search (model-based RL) for image-based robotic manipulation
		- https://github.com/cbfinn/gps
		- Model: local linear time-varying p(ut|xt) ~ N(Ktxt + kt, Ct)
		- Plan: min.p(τ)∈N(τ) E.p[l(τ)] s.t. KL(p(τ)|pˆ(τ)) ≤ ε;
	- S Levine, C Finn, T Darrell, and P Abbeel. End-to-end training of deep visuomotor policies. JMLR'16
	- C Finn, X Tan, Y Duan, T Darrell, S Levine, and P Abbeel. Deep spatial autoencoders for visuomotor learning. ICRA'16
	- Y Chebotar, K Hausman, M Zhang, G Sukhatme, S Schaal, and S Levine. Combining model-based and model-free updates for trajectory-centric reinforcement learning. ICML'17
- NN Model:
	- I. Lenz, R. Knepper, and A. Saxena. DeepMPC: Learning deep latent features for model predictive control. RSS'15
	- A. Punjani and P. Abbeel. Deep learning helicopter dynamics models. ICRA'15
	- J. Fu, S. Levine, and P. Abbeel. One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. IROS'16
	- A. Baranes and P.-Y. Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. 2016
	- I. Mordatch, N. Mishra, C. Eppner, and P. Abbeel. Combining model-based policy search with online model learning for control of physical humanoids. ICRA'16
	- S Depeweg, J Hernández-Lobato, F. Doshi-Velez, and S. Udluft. Learning and policy search in stochastic dynamical systems with Bayesian neural networks. 2016
	- Y Gal, R McAllister, and C Rasmussen. Improving PILCO with Bayesian neural network dynamics models. ICMLW'16
	- P Agrawal, A Nair, P Abbeel, J Malik, and S Levine. Learning to poke by poking: Experiential learning of intuitive physics. NeurIPS'16
	- G Williams, N Wagener, B Goldfain, P Drews, J Rehg, B Boots, and E Theodorou. Information theoretic MPC for model-based reinforcement learning. ICRA'17
		- Replan
	- A Nagabandi, G Kahn, R Fearing, S Levine. Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. ICLR'18
		- At beginning: avoid overfitting (MBRL is better)
		- Later: avoid underfitting (MF-RL is better)
		- Mb-Mf best
- Bayesian-NN, Uncertainty-Aware:
	- I. Osband. Risk versus uncertainty in deep learning: Bayes, bootstrap and the dangers of dropout. NIPSW'16
- **MBMF**: S Bansal, R Calandra, K Chua, S Levine, C Tomlin. MBMF: Model-Based Priors for Model-Free Reinforcement Learning. NIPS'17
	- Learn a probabilistic dynamics model and leveraging it as a prior for the intertwined model-free optimization;
- **PETS**: K Chua, R Calandra, R McAllister, S Levine. Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. NIPS'18
	- https://github.com/kchua/handful-of-trials
	- CEM actions;
- **MB-MPO**: Clavera. Model-Based Reinforcement Learning via Meta-Policy Optimization. 2018
- **SOLAR**: M Zhang, S Vikram, L Smith, P Abbeel, M Johnson, S Levine. SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning. ICML'19
- **MBPO**: M Janner, J Fu, M Zhang, S Levine. When to Trust Your Model: Model-Based Policy Optimization. NeurIPS'19
	- https://zhuanlan.zhihu.com/p/105645139

## Legacy
- **PILCO**: M P Deisenroth, C E Rasmussen. PILCO: A Model-Based and Data-Efficient Approach to Policy Search. ICML'11
- **DDP**: Mayne, Jacobson. Differential dynamic programming. 1970
