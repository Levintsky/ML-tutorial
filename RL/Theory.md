# Theory

## Theory Analysis
- Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. NIPS'00.
	- Contributions: Established policy gradient theorem and showed convergence of policy gradient algorithm for arbitrary policy classes.
- Tsitsiklis and Van Roy. An Analysis of Temporal-Difference Learning with Function Approximation. TAC'97.
	- Contributions: Variety of convergence results and counter-examples for value-learning methods in RL.
- Reinforcement Learning of Motor Skills with Policy Gradients, Peters and Schaal, 2008. 
	- Contributions: Thorough review of policy gradient methods at the time, many of which are still serviceable descriptions of deep RL methods.
- Approximately Optimal Approximate Reinforcement Learning, Kakade and Langford, 2002.
	- Contributions: Early roots for monotonic improvement theory, later leading to theoretical justification for TRPO and other algorithms.
