# (Model-Free) RL Algorithms

## Basics
- PG:
	- Log-trick, IS
	- TRPO:
		- Bound distribution change;
		- Bound objective value;
		- Natural gradient to bound π(a|s) change;
- AC:
	- Loss: L = |Adv(s,a)|^2 - ΣAdv(s,a)logπ(a|s)
	- Advantge: A(π) = Q(s,a;π) - V(s;π) = r(s,a) + γV(s') - V(s)
	- GAE: 
- Q-learning
	- Value function: V(s;π) = (1-γ)E[Σγ^t R(st,at)|π,s];
	- BV: Q(s,a) = r(s,a) + max_a' Q(s',a'), contraction by γ;
	- Non-tabular: V = πBV, with π(.) as NN fitting;
	- Exploration: ε, greedy, Boltzmann;
	- Target-network, replay buffer;
	- Over-estimate: Double-Q, multi-step;
	- PER, dueling-network;
- Continuous Q-learning:
	- Sampling: CEM, CMA-ES;
	- Easily to optimize: NAF (normalized advantage function);
	- Learn an optimizer: DDPG;
- Soft-Q-learning: HMM-style forward+backward
	- p(Ot=1|st,aT) := exp(r(st,at))
	- Forward: αt = p(st|O1:t-1)
	- β(st) = p(O.t:T|st)
		- V(s) := log[β(s)] = log∫exp(Q(s,a))da
	- β(st,at) = p(O.t:T|st,at)
		- Q(s,a) := log[β(s,a)] = r(s,a)+log(E.s'|s,a[exp(V(s'))])
	- π(at|st) ∝ β(st,at)/β(st)
	- Minimize loss: KL(pˆ(τ)|p(τ)) equivalent to maximize
		- E.sˆ[H(π(at|st))]+E.sˆaˆ[r(s,a)]

## Pessimistic
- Assume pessimistic for insufficient overlap/support (s,a) space;
- Y Liu, A Swaminathan, A Agarwal, E Brunskill. Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration. NIPS'20
	- Filtration function: ξ(s,a;μ,b) = 1(μ(s,a)>b)
	- Bellman op: Tf(s,a) = r(s,a) + γEs'[max_a' ξ(s',a')f(s',a')]; (0 for insufficient data)
	- Error bound;
- PS Thomas, B Castro da Silva, AG Barto, S Giguere. Preventing undesirable behavior of intelligent machines. Science'19
	- Optimizing while ensuring solution won't exhibit undesirable behavior;

## Policy Gradient
- Basic PG (from Sergey Levine CS-294):
- Goal: J(θ) = E..τ~p(θ)[Σr(st, at)]; θ = argmax J(θ)
	- Insight: increase the probability of high-reward trajectory;
- PG:
	- ∇J(θ) = ∫ πlogπ(τ;θ)r(τ)dτ
	- ∇J(θ) = E..τ~ p(θ)[∇logπ(τ;θ)r(τ)]
- Policy:
	- Softmax: φ(s,a)'θ, π(s,a;θ) = softmax(φ(s,a)θ)
	- Gaussian: mean μ(s)=φ(s)θ, fixed σ^2
		- ∇logπ(s,a) = φ(s)(a-μ(s))/σ^2;
- Supervision:
	- Loss: L = -E[logπ(τ;θ)r(τ)]
- Advantages:
	- On-Policy
	- Better Convergence
	- Effective in high-dimensional or continuous action spaces
	- Can learn **stochastic** policies
- Trick:
	- π(τ;θ)∇logπ(τ;θ) = ∇π(τ;θ)
	- Score function: ∇logπ(s,a)=φ(s,a)-E[φ(s,.)]
	- Baseline for variance reduction:
		- E[∇logπ(τ;θ)b] = 0, so we can use any b in ∇logπ(r-b);
		- min_b Var(∇J(θ)) -> dVar/db=0;
			- b = E[g(τ)^2 r(τ)]/E[g(τ)^2], with g(τ)=∇logπ(τ;θ);
		- State-wise: b(st) (actor-critic)
	- IS/off-policy PG:
		- Insight: we have trajectory of τ ~ p(θ) and try to optimize J(θ');
		- Weight each trajectory τ ~ p(θ) with p(θ')/p(θ), which is also π(τ;θ')/π(τ;θ);
		- PG of θ' becomes: weighted(Σ∇logπ)(Σr(st,at)) under τ ~ p(θ);
		- Keep up-to t and ignore future, we get TRPO;
		- Σ∇logπ(τ;θ') (Πt'=1..t π(θ')/π(θ)) (Σt'=t..T r(st,at))
	- Natural-PG [NIPS'21]
	- Damped update: π-new=(1-α)π+απ' [ICML'02]
	- Trust-Region NPG: TRPO [ICML'15]
	- PPO: Clip IS weight in (1-ε,1+ε)
- Legacy:
	- **REINFORCE**: Williams. 1992
	- Baxter & Bartlett 2001
	- Peters & Schaal 2008
- Recent:
	- **NPG**: S Kakade. A Natural Policy Gradient. NIPS'01
		- Utility: J(θ) = Σs,a ρ(s;π)π(a;s)R(s,a), we assume ρ(s;π) is a well-defined stationary distribution;
		- ∇J = Σs,a ρ(s;π)∇π(a;s)R(s,a)
		- Proposed: ∇J = F(θ)^(-1) ∇J
	- S Kakade, J Langord. Approximately Optimal Approximate Reinforcement Learning. ICML'02
		- Insight: conservative mixture π-new=(1-α)π+απ' has theoretical guarante;
		- Def. μ restart distribution: draws next state from μ.
		- Discounted future state distribution: ρ(s;π) = (1-γ)Σ[γ^t p(st=s;π)]
		- Desired: more uniformly from explotary μ as J(π;μ) = E..s~μ[V(s;π)]
		- Proposed approach: π-new(a,s) = (1-α)π(a;s) + απ'(a,s)
		- Policy improvement:
			- A(π';π,μ) = E..s~ρ(π,μ)[E..a~π'(a,s) [A(s,a;π)]], same state distribution and advantage, change policy as a ~ π'(a,s).
			- Theory: J(π-new,μ)-J(π,μ) >= α/(1-γ)(Adv - 2αγε/(1-γ(1-α)))
	- **GPS**: S, Levine & Koltun. Guided policy search: deep RL with importance sampled policy gradient.  ICML'13. (unrelated to later discussion of guided policy search)
	- **TRPO**: J Schulman, S Levine, P Moritz, M Jordan, P Abbeel. Trust region policy optimization. ICML'15
		- Insight: proposed utility J=L(θ;θ-old) - C KL(θ-old;θ), with C chosen theoretically;
			- J(θ) >= L(θ;θ-old) - CKL(.,.);
		- **Trick-1**: make new expectation as old plus Advantage; J(π')=J(π)+E(Adv(s,a)), notice the expectation is under new policy π';
			- J(θ')-J(θ) = E..τ~p(;θ') (Σt=0..∞ γ^t Adv(st, at;πθ))
			- Let L(π-new) = J(π) + Σs,a[ρ(s;π)Σa π-new(a|s)A(s,a;π)]
			- Then L(π) and J(π) agrees on value and gradient near θ if policy π(a|s;θ) if parametrized by θ;
				- L(θ) = J(θ); ∇L(θ) = ∇J(θ)
		- **Trick-2**: not able to get expectation under new policy? IS, Expectation under p[f(x)] versus Expecation under q(x) q[q/p f(x)]
		- **Trick-3**: bound the difference between π'(s) and π(s) to make p(st;θ) and p(st;θ') closed, |π'(s)-π(s)| < ε easier to bound with KL-divergence;
			- p(s;θ') = (1-ε)^t p(s;θ) + (1-ε)^t p(s;other); still takes the same action with θ';
			- |p(s;θ')-p(s;θ)| <= 2εt; (Taylor Expansion)
			- J(θ')-J(θ) Sample from old trajectory instead will result in at most Σ..t 2εtC loss, with constant C bounded by O(Trmax) or O(rmax/1-γ);
		- **Trick-4**: First order approx for utility J(.), natural gradient for update;
			- θ' = argmaxθ' Σt E..st~p(st;θ)[E..at~π(at|st;θ)[π(;θ')/π(;θ) γ^t A(st,at;πθ')]]; traj, policy all sampled from old, IS advantage for new;
			- s.t. KL(π(at|st;θ')||π(at|st;θ')) < ε; constrained optimization;
			- Utility approx: argmax ∇..θ A(θ)(θ'-θ), s.t. KL(π(θ'),π(θ)) < ε;
				- We know KL(π(θ'),π(θ)) ~ 1/2 (θ'-θ)F(θ'-θ), with F as Fisher;
			- θ' = θ + αF^(-1)∇..θ J(θ), with α=√(2ε/∇J F ∇J), largest step within trust-region;
		- https://zhuanlan.zhihu.com/p/26308073
		- In summary: Theoretical Guarantee of monotonic improvement if KL constraint satisfied; Surrogate loss; Line search to make the best stepsize update;
		- W Zaremba: https://github.com/wojzaremba/trpo
	- **PPO**: J Schulman, P Wolski, P Dhariwal, A Radford and O Klimov. Proximal policy optimization algorithms: deep RL with importance sampled policy gradient. 2017
		- Let rt(θ) = π(at|st;θ)/π(at|st;θ-old), so rt(θ-old)=1;
		- Lcpi(θ) = E[π(at|st;θ)/π(at|st;θ-old)At]=E[rt(θ)At], IS-advantage;
		- Lclip(θ) = E[min(rt(θ)At, clip(rt(θ),1-ε,1+ε)At)], clip IS in (1-ε,1+ε)
	- **PPO-Penalty**: DeepMind. Emergence of Locomotion Behaviours in Rich Environments. NIPS'17
		- Insight: adaptive KL-penalty;
		- J(θ) = Σ At π(at|st,θ)/π-old - λ KL(π-old|π)
		- Update value network V(;φ)
		- If KL(π-old|π) > β-high KL-target: 
			- λ=αλ
		- If KL(π-old|π) < β-low KL-target: 
			- λ=λ/α

## Value + Policy, Actor-Critic
- Goal: PG with variance reduction (adv)
	- Actor: π(st|at;θ)
	- Critic: Advantage A(s,a)=Q(s,a)-V(s)
		- Value V can be supervised by V=E[Q]
	- Reduce variance of policy gradient: update with A(s,a)∇logπ
- A general framework (for Implementation):
	- Give nstate x, neural net for V(s), π(a|s), entropy H(π);
	- Action: deterministic argmax π(a|s) or sample;
- Supervision: Learning of both actor and critic (A2C/PPO/...): 
	- Critic target: GAE/n-step for V(st+1)
	- Loss: L = A(s,a)^2 - A(s,a) log π(a|s)
- Value/Advantage estimation:
	- Adv(s,a) = r(s,a)+γV(s')-V(s), low variance but high-bias, b/c value net could be wrong;
	- Adv(s,a) = Σ.t' γ^(t'-t)r(st',at')-V(st), no bias but high variance, b/c single traj;
	- Eligibility traces & n-steps, combine the two;
		- Adv(st,at) = Σ_t..t+n γ^(t'-t)r(st',at') + γ^nV(st+n) - V(st)
	- GAE: weighted comb of different steps: Adv_GAE(st,at) = Σ.n wn Adv_n
- Model design:
	- DPG [ICML'14]: value Q(s,a;θ) with action a=μ(θ)=argmax.a Q(s,a)
		- Q(s,a;θ) learning: θ += E.s[∇.θ Q(a, μ(s;θ))]
		- Chain rule: θ += E.s[∇.a Q(a, μ(s)) ∇.θμ(s)]
- Tricks:
	- Hogwild: A3C [ICML'16]
	- Offline: ACER (with IS) [DeepMind, ICLR'17], SAC, DDPG
	- Optimizer: ACKTR (KFAC) [G-Brain, 17]
- Legacy
	- Sutton, McAllester, Singh, Mansour. Policy gradient methods for reinforcement learning with function approximation: actor-critic algorithms with value function approximation. NIPS'00
- DPG: DeepMind. Deterministic policy gradient algorithms. ICML'14
	- ∇..φ J(φ)=E[∇..aQ(s,a;π) ∇..φπ(s;φ)]
	- where Q(s,a;π)=E[Rt|s,a]
- GAE: ICLR'16

## Value Function, Q-learning
- Framework:
	- Collect some samples;
	- Fit a model for estimated return: Q(s,a;π) = r(s,a) + γE_s'~ p(s'|s,a)[V(s;π)]
	- V(s;π) = max_a Q(s,a;π)
- Online Q-iteration (SARSA, TD): eval current policy;
	- Take action ai ~ π; (off-policy if not π)
	- Observe (si,ai,si',ri);
	- Update Q(si,ai) += α(rt+γQ(st+1,at+1)-Q(st,at))
	- Update π as ε-greedy: π = argmax_aQ(s,a) with prob 1-ε else random;
- Q-learning (off-policy): only diff with SARSA is Q-update;
	- Q(si,ai) += α(rt+γmax_a Q(st+1,a)-Q(st,at));
	- **Biased** due to max-op proof: Mannor, Simester, Sun and Tsitsiklis. Bias and Variance Approximation in Value Function Estimates. Management Science 2007
	- Double Q-learnign to remove bias;
- Value Fitting:
	- Q-fit: Q(si, ai)
		- Target (Q-learning): y = r(si,ai) + γmax_a'Q(si',ai';φ)
			- φ -= α dQ(si,ai;φ)/dφ (Q(si,ai;φ)-yi)
		- Target (MC): y=Gt
		- Target (SARSA): y=rt+1 + γQ(st+1,at+1)
			- Q(st,at) += α(y - Q(st,at))
	- V-fit:
		- L = E_s|V(s;π)-V(s;π,θ)|
		- V(s) += α(r+γV(s')-V(s)); TD(0)
		- Proof: for linear function approximator V=ws, converge;
	- IS in policy evaluation:
		- V(s;π) ~ Σi=1..Nτ~ π' p(τ|π,s)/p(τ|π',s) Rτ
		- Don't need the dynamic model, just weight by π;
- Model design:
	- Neural Net backbone: DQN [2013, Nature'15]
	- Dueling-Net head [ICML'16 best-paper]: separate head for state and advantage;
		- Q(s,a;θ,α,β) = V(s;θ,β) + A(s,a;θ,α),
	- DDPG [ICLR'16]: deep version of DPG
		- Value Q(s,a;φ) with target yj = rj + γmax_a Q(sj',μ(sj';θ);φ)
		- Actor with loss: -Q(s, μ(θ);φ)
	- SVG [DeepMind NIPS'15] Stochastic policy + value:
		- Assumption:
			- Stochastic dynamics: s' = f(s, a, ξ), noise ξ ∼ ρ(ξ);
			- Stochastic policy: a = π(s, η; θ), noise η ∼ ρ(η);
		- V(s) = E.ρ(η)[r(s,π(s,η;θ))+γE.ρ(ξ)[V'(f(s, π(s, η; θ), ξ))
		- Insight: gradient w.r.t. reparametrization (sample from noise);
			- e.g. y = f(x, ξ), where ξ ~ ρ(.);
			- ∇xE_p(y|x)[g(y)] = E_ρ(ξ)[gyfx] ≈ 1/MΣgyfx|ξ=ξi (MC estimator)
		- SVG(0): model free, sample noise;
		- SVG(1): one-step dynamics p(s'|s,a)
		- SVG(∞)
	- SAC: [ICML'18] as random as possible; 
		- 3 separate net: V(.;ψ), V(.;ψ'), Q(s,a;θ), π(.|s,φ);
		- Actor: J(π) = Σr(s,a) + αH(π(.|s));
		- Critic: standard L2 V-loss;
		- SGD: ψ -= λ∇Jv(ψ); θ -= λ∇Jq(θ); φ -= λ∇Jπ(φ);
		- Target-net: ψˆ = τψ + (1-τ)ψ

- Loss: over-optimistic target;
	- Double-Q [NIPS'15]: 2 networks, 1 for Q, 1 for a;
	- TD3 [ICML'18]: Critic x2 Q(;θ1), Q(;θ2);
		- Deterministic actor π(;φ);
		- For value iteration: target always pick the minimum
			- y = r + γmin_i=1,2(Qi(s',a'))
- Tricks:
	- Q-learning: Off-Policy
	- Experience Replay
		- PER [ICLR'16]: prioritize high TD-error;
		- HER [NIPS'17]: reward based on goal: R(s,a,g), highsight relabel replay buffer to match final state;
	- Exploration:
		- π(at|st) = 1-ε if at = argmax_a Q(st,at;φ); and random uniform other actions;
		- π(at|st) ∝ exp(Q(st,at;φ))
	- Fixed Q-targets: fixed target y, just regression, no gradient;
	- Multi-step return; yj,t = Σ_t' rj,t' + γ^n max_a Q(sj,t+n,aj,t+n)
	- Combination of all tricks: Rainbow [AAAI'18]: 
		- Double Q-learning + PER + Dueling + Multi-step learning + Distributional + Noisy Nets
	- Different update frequency for a/c: TD3 [ICML'18]
- Theory:
	- Define op B: BV=max_a ra + γTaV, with Ta dynamics;
	- Optimal V∗ is a fixed point: V∗ = BV∗
	- B is a contraction |BV1-BV2| <= γ|V1-V2|, w.r.t. ∞-norm;
	- Non-tabular case:
		- Define operator ∏ (projection): ∏V=argmin_V∈Ω 1/2|V'(s)-V(s)|^2
		- V <- ∏BV (project to a neural net in sense of L2-norm)
		- V' <- argmin_V∈Ω 1/2|V'(s)-BV(s)|^2
		- ∏ is a projection in sense of L2: |∏v1-∏v2|<=|v1-v2|
		- but ∏B is not a contraction of any kind;
	- SAC:
		- Lemma 1: soft-value iteration converges;
		- Lemma 2: KL-minimizer guarantees improved Q(s,a);
		- Theo 1: soft policy eval + iter with π ∈ Π converge to optimal π∗;
- Classic
	- Multi-Step Returns: DeepMind. Bellemare. Safe and Efficient Off-Policy Reinforcement Learning. NIPS'16
- Legacy:
	- **TD**: Sutton, R. S. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9–44, 1988.
	- C. J. Watkins and P. Dayan. Q-learning. Machine learning'92.
	- **Experience Replay**: Lin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and teaching. ML'92
	- **SARSA**: Singh, S., Jaakkola, T., Littman, M. L., and Szepesvari, C. Convergence results for single-step on-policy reinforcement-learning algorithms. ML'00
	- Precup, D., Sutton, R. S., and Dasgupta, S. Off-policy temporal-difference learning with function approximation. ICML'01
- More modern techniques:
	- **DRQN**: M Hausknecht, P Stone. Deep Recurrent Q-Learning for Partially Observable MDPs. AAAI'15
		- Could bootstrap from start of the episode or random point;
	- **Noisy Nets**: DeepMind. Noisy networks for exploration. ICLR'18
	- **Non-delusional Q-learning and value iteration**, NIPS'18 best paper award:
		- Delusion: parameter update inconsistent with following policy;
		- PCVI: Tabular (model-based MDP)
		- PCQL: Q-learning (model-free)
- Continuous:
	- S Gu, T Lillicrap, I Sutskever, S Levine. Continuous Deep Q-Learning with Model-based Acceleration. ICML'16
	- Google-Brain. QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. CoRL'18
- **Overestimation**:
	- Thrun, S. and Schwartz, A. Issues in using function approximation for reinforcement learning. 1993.
- Bias:
	- **SBEED**: B Dai, A Shaw, L Li, L Xiao, N He, Z Liu, J Chen, L Song. SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation. ICML'18
		- Value iteration does not have a cost function to optimize, it is just a fixed point iteration;
		- Key idea: use conjugate of the square function to avoid the bias introduced in square function;
		- Tradeoff: introduce a mini-max formulation;
	- Y Feng, L Li, Q Liu. A Kernel Loss for Solving the Bellman Equation. NIPS'19

## Unclassified
- P Rauber, A Ummadisingu, F Mutz, J Schmidhuber. Hindsight policy gradients. ICLR'19
- O'Donoghue, B., Osband, I., Munos, R., and Mnih, V. The uncertainty bellman equation and exploration. 2017
- **Smoothed**: Nachum, O., Norouzi, M., Tucker, G., and Schuurmans, D. Smoothed action value functions for learning gaussian policies. 2018
- Distributional:
	- **C51**: DeepMind. A distributional perspective on reinforcement learning. ICML'17
		- Q(s, a) from scalar to a categorical 51 classes (linear between v-min to v-max)
		- Do KL divergence training between Q(st, at) and r(st, at) + max_a Q(st+1, a), when take argmax action, just calculate expectation (marginalize 51 categories);\
			<img src="/RL/images/algos/c51.png" alt="drawing" width="400"/>
	- **QR-DQN**: DeepMind. Distributional Reinforcement Learning with Quantile Regression. AAAI'18\
		<img src="/RL/images/algos/qr-dqn.png" alt="drawing" width="400"/>
	- **IQN**: DeepMind. Implicit Quantile Networks for Distributional Reinforcement Learning. ICML'18
	- DeepMind. Distributional policy gradients. ICLR'18
- Variance-reduction:
	 - Anschel, O., Baram, N., and Shimkin, N. Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning. ICML'17
- Action Dependent: (in AC, value function should be state-dependent)
	- **Q-Prop**: S Gu, T Lillicrap, Z Ghahramani, R E. Turner, S Levine. Sample-efficient policy-gradient with an off-policy critic: policy gradient with Q-function control variate. ICLR'17
		- Taylor approx of DPG/DDPG for value function, mixture of DPG/DDPG
		- https://github.com/shaneshixiang/rllabplusplus
		<img src="/RL/images/q-prop.png" alt="drawing" width="600"/>
	- **Stein-Control-Variate**: H Liu, Y Feng, Y Mao, D Zhou, J Peng, Q Liu. Action-depedent Control Variates for Policy Optimization via Stein's Identity. ICLR'18
	- The Mirage of Action-Dependent Baselines in Reinforcement Learning.
		- Contribution: interestingly, critiques and reevaluates claims from earlier papers (including Q-Prop and stein control variates) and finds important methodological errors in them.
		- Claims V(s,a) style does not reduce variance
- PG + Q-Learning:
	- **Reactor**: DeepMind. The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning. ICLR'18
	- **IPG**: 	Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning

## Evoluation Strategy
- https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html
- Basics:
	- Check Optimization/Black-Box.
- **ES-RL**: OpenAI. Evolution Strategies as a Scalable Alternative to Reinforcement Learning. 2017
	- NES as a gradient-free optimizer to find optimal policy θ;
	- Let θ ~ N(θ', σ^2 I)
	- Sample to get ∇f(θ):
		- ∇E[f(θ)] = ∫p(ε) ∇logp(ε) ∇F(θ+σε)dε, reparametrizatio trick, ε ~ N(0, I)
		- ∇E[f(θ)] = 1/σ E[ε F(θ+σε)]
	- ES and parallel version:
	- Adaptive stepsize does not help;
	- Comparison with PG:
		- PG: Var(∇f(θ)) ~ Var(R(a))Var(∇logp(a;θ))
		- ES: Var(∇f(θ)) ~ Var(R(a))Var(∇logp(θ';θ))
	- Experiments: 3D humanoid walking in 10 minute, competitive on Atari;
- Uber-AI. Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents. NIPS'18
	- NES-based on **novelty**, not only evaluation score;
	- b(π): domain-specific behavior characterization; (could be location for locomotion, ...)
	- N(θ, A) = 1/λ Σ|b(π) - bi-knn|, find KNN in Action set A;
	- ∇E[f(θ)] = 1/σ E[ε N(θ+σε,A)]
- NSRAdapt-ES (NSRA-ES): perf + novelty in es search;
	- θ = θ + α 1/σ Σ_i εi((1-w)N(θ+ε,A) + wF(θ+ε))
- ERL: S Khadka, K Tumer. Evolution-Guided Policy Gradient in Reinforcement Learning. NIPS'18
	- Combine Cross Entropy Method (CEM) with DDPG or TD3.
- CEM-RL: A Pourchot, O Sigaud. CEM-RL: Combining evolutionary and gradient-based methods for policy search. ICLR'19
	- The mean actor of the CEM population π is initialized with a random actor network;
	- The critic network Q is initialized too, updated by DDPG/TD3;
	- Repeat:
		- sample population of actors ~ N(π, Σ);
		- Half of population evaluated, fitness score used as cumulative reward R;
		- The other half updated together with critic;
		- New π, Σ computed using elite samples;
- PBT:
	- POET: R Wang, J Lehman, J Clune, K Stanley. Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions. 19