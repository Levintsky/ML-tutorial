# RL-Basics

## Resources
- Courses
	- DeepMind/UCL: David Silver
	- Berkeley cs294: Sergey Levine
	- From OR point of view
		- http://www.columbia.edu/~sa3305/
	- Standord cs234;
- Good Summaries
	- Algorithms: https://github.com/tigerneil/awesome-deep-rl
	- L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. JAIR, 1996.
- Books
	- Reinforcement Learning: An Introduction
		- Assignment: https://github.com/ShangtongZhang/reinforcement-learning-an-introduction
	- Richard Bellman. Dynamic Programming. Princeton University Press, 1957.
- https://spinningup.openai.com/en/latest/spinningup/keypapers.html
- https://github.com/navneet-nmk/pytorch-rl
- https://github.com/sweetice/Deep-reinforcement-learning-with-pytorch
- Chinese:
	- https://zhuanlan.zhihu.com/sharerl
	- https://zhuanlan.zhihu.com/p/68205048
- Open-AI Baselines:
	- https://github.com/openai/baselines
	- A2C, ACER, ACKTR / DDPG / DQN / GAIL / HER
	- PPO1 (Multi-CPU using MPI), PPO2 (Optimized for GPU), TRPO
- **rlkit**
	- https://github.com/vitchyr/rlkit
	- RIG, TDMs, HER, DQN, SAC, TD3
- Pytorch libraries:
	- DQN Adventure: https://github.com/higgsfield/RL-Adventure
	- Ye Yuan (CMU): https://github.com/Khrylx/PyTorch-RL
	- Shangtong Zhang: https://github.com/ShangtongZhang/DeepRL
	- Ilya Kostrikov (NYU): https://github.com/ikostrikov
	- Rainbow: https://github.com/Kaixhin/Rainbow
	- A3C LSTM: https://github.com/dgriff777/rl_a3c_pytorch
- Tensorflow libraries:
	- https://github.com/brendanator/atari-rl
	- https://github.com/steveKapturowski/tensorflow-rl
	- https://github.com/tensorflow/agents
- Shane Gu: https://github.com/shaneshixiang/rllabplusplus
- Berkeley RL: https://github.com/rll/rllab
- GA3C: https://github.com/NVlabs/GA3C
- Model-free RL from Spinningup:
	- DQN: DQN, DRQN; Duel DQN; PER; Rainbow;
	- PG: A3C, TRPO, GAE, PPO, ACKTR, ACER, SAC;
	- DPG: DPG, DDPG, TD3;
	- Distributional: C51, QR-DQN, IQN, Dopamine;
	- Action-dependent: Q-prop, Stein control, mirage;
	- PCL: PCL, Trust-PCL;
	- Other continuous combine pg and q: PGQL, Reactor, IPG, equivalence between...;
	- Evolutionary: ES;

## Misc
- UD-RL
	- J Schmidhuber. Reinforcement Learning Upside Down: Don’t Predict Rewards - Just Map Them to Actions. 2019
	- R K Srivastava, P Shyam, F Mutz, W Jaśkowski, J Schmidhuber. Training Agents using Upside-Down Reinforcement Learning. 2019

## Applications of RL
- System Optimization
	- E. Ipek, O. Mutlu, José, and R. Caruana. Self-optimizing memory controllers: A reinforcement learning approach. ISCA, 2008.
- Web Browse
	- E Z Liu, K Guu, P Pasupat, T Shi, P Liang. Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration. ICLR'18
- Computer Vision (Attention)
	- DeepMind. Recurrent Models of Visual Attention. NIPS'14
	- DeepMind. Multiple object recognition with visual attention. ICLR'15.
	- P. Sermanet, A. Frome, and E. Real. Attention for fine-grained categorization. ICLR'15
	- D Jayaraman, K Grauman. Learning to look around: Intelligently Exploring Unseen Environments for Unknown Tasks. CVPR'18
- NTM
	- W. Zaremba and I. Sutskever. Reinforcement learning neural turing machines. arxiv, 2015.
- Recommendation
	- L Li, W Chu, J Langford, R Schapire. A Contextual-Bandit Approach to Personalized News Article Recommendation, WWW'10
	- Fighting Boredom in Recommender Systems with Linear Reinforcement Learning. NIPS'18
	- Horizon: Facebook. Horizon: Facebook's Open Source Applied Reinforcement Learning Platform. 2019
- Misc
	- Data center cooling using model-predictive control. NIPS'18 Tutorial
- Unclassified
	- Learning Temporal Point Processes via Reinforcement Learning. NIPS'18
	- Reinforcement Learning of Theorem Proving. NIPS'18

## Poker/Chess
- Chess
	- Gammon: G Tesauro. Temporal difference learning and td-gammon. 1995.
	- Chess: M. Campbell, A. J. Hoane, and F. hsiung Hsu. Deep blue. Artificial intelligence, 2002.
	- Go-Legacy:
		- M. Enzenberger. The integration of a priori knowledge into a go playing neural network. URL: http://www.markus-enzenberger.de/neurogo.html, 1996.
		- M. Enzenberger. an open-source framework for board games and go engine based on monte carlo tree search. 2010.
		- **Pachi**: P. Baudis and J. loup Gailly. Pachi: State of the art open source go program. 2012.
		- D. Silver, Temporal-Difference Search in Computer Go, 2012
		- Y. Tian and Y. Zhu. Better computer go player with neural network and long-term prediction. arxiv, 2015.
	- DeepMind. Move evaluation in go using deep convolutional neural networks. arxiv, 2014.
	- C. Clark and A. Storkey. Teaching deep convolutional neural networks to play go. ICML'15.
	- AlphaGo: DeepMind. Nature'14
	- AlphaGo-Zero: DeepMind. Nature'17
	- AlphaZero: chess+shogi+Go. Science'19
	- PhoenixGo: https://github.com/Tencent/PhoenixGo
	- MuGo: https://github.com/brilee/MuGo
	- Leela: https://github.com/gcp/leela-zero
	- MiniGo: https://github.com/tensorflow/minigo
	- ELF OpenGo. ICML'19
- Poker (Texas Hod'em)
	- Check Game Theory;
- Rubik's Cube
	- Basics:
		- How to solve the rubik's cube - beginners method. https://ruwix.com/the-rubiks-cube/how-to-solve-the-rubiks-cube-beginners-method/.
	- Mathematics:
		- D Kunkle and G Cooperman. Twenty-six moves suffice for rubik’s cube. ISSAC'07
		- S Radu. Rubik’s cube can be solved in 34 quarter turns. Jul 2007.
			- http://cubezzz.dyndns.org/drupal/?q=node/view/92, 
		- M Reid. Superflip requires 20 face turns. Jan 1995.
			- http://www.math.rwth-aachen.de/~Martin.Schoenert/Cube-Lovers/michael_reid__superflip_requires_20_face_turns.html, 
		- T Rokicki. Twenty-two moves suffice for rubik’s cubeR. 2010.
		- T Rokicki. God’s number is 26 in the quarter-turn metric. http://www.cube20.org/qtm/, Aug 2014
		- T Rokicki, H Kociemba, M Davidson, and J Dethridge. The diameter of the rubik’s cube group is twenty. SIAM Review'14.
		- M Thistlethwaite. Thistlethwaite’s 52-move algorithm. https://www.jaapsch.net/puzzles/thistle.htm, Jul 1981.
	- Legacy:
		- P Lichodzijewski and M Heywood. The rubik cube and gp temporal sequence learning: an initial study. 2011.
	- **Korf**:
		- A Brown. Rubik’s cube solver. https://github.com/brownan/Rubiks-Cube-Solver, 2017.
		- Deepening A∗ search.
		- R Korf. Finding optimal solutions to rubik’s cube using pattern databases. AAAI’97/IAAI’97
	- **Kociemba**:
		- Two-phase algorithm details. http://kociemba.org/math/imptwophase.htm.
		- M Tsoy. Kociemba. https://github.com/muodov/kociemba, 2018.
	- Deep-learning:
		- R Brunetto and O Trunda. Deep heuristic-learning in the rubik’s cube domain: an experimental evaluation. 2017.

## HRL
- From Spinningup: STRAW, FUN, HIRO;
- Legacy
	- Hierarchical Learning in Stochastic Domains: Preliminary Results
	- R Sutton, D Precup, and S Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. AI'99
	- A McGovern and A Barto. Automatic discovery of subgoals in reinforcement learning using diverse density. 2001.
	- M Stolle and D Precup. Learning options in reinforcement learning. 2002.
	- A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning 2010
	- D Silver and K Ciosek. Compositional planning using optimal option models.
- HRL
	- A summary: https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/
	- A Deep Hierarchical Approach to Lifelong Learning in Minecraft, AAAI'17
	- T Shu, C Xiong, R Socher. Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning. ICLR'18
	- T Kulkarni, K Narasimhan, A Saeedi, and J Tenenbaum. Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. NIPS'16
	- S Zheng, Y Yue and P Lucey. Generating long-term trajectories using deep hierarchical networks. NIPS'16.
		- Long-term planning;
	- E Zhan, Stephan Zheng, Generative Multi-Agent Behavioral Cloning
	- **HIRO**: O Nachum, S Gu, H Lee, Sergey Levine. Data-Efficient Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies. NIPS'18
	- A Levy, G Konidaris, R Platt, K Saenko. Learning Multi-Level Hierarchies with Hindsight. ICLR'19
	- Z Pang, R Liu, Z Meng, Y Zhang, Y Yu, T Lu. On Reinforcement Learning for Full-length Game of StarCraft. 2019
	- **MPH**: A Pashevich, D Hafner, J Davidson, R Sukthankar, C Schmid. Modulated Policy Hierarchies. 2019
- Joint Skills and Meta-Controller
	- P Dayan, Peter and G Hinton. Feudal reinforcement learning. NIPS'93.
	- P Bacon and D Precup. The option-critic architecture. NIPSW'15
	- P Bacon, J Harb, and D Precup. The Option-Critic Architecture, AAAI'17
	- DeepMind. Learning and transfer of modulated locomotor controllers. 2016
	- STRAW: DeepMind. Strategic Attentive Writer for Learning Macro-Actions. NIPS'16
		- Stick to the plan (commit)\
			<img src="/RL/images/hrl/straw1.png" alt="drawing" width="500"/>
		- Attention model:\
			<img src="/RL/images/hrl/straw2.png" alt="drawing" width="500"/>
		- Algorithm:\
			<img src="/RL/images/hrl/straw3.png" alt="drawing" width="400"/>
		- Experiments: 2D-maze; Atari;
	- FUN: DeepMind. Feudal Network for Hierarchical Reinforcement Learning. ICML'17
		- https://github.com/dmakian/feudal_networks
	- S Krishnan, R Fox, I Stoica, and K Goldberg. Ddco: Discovery of deep continuous options for robot learning from demonstrations. ICLR'17
	- K Frans, J Ho, X Chen, P Abbeel, J Schulman. Meta Learning Shared Hierarchies. ICLR'18
	- Main problem:
		- the meta-policy does not select "bad" options, so these options do not receive any reward signal to improve.
		- Google-Brain. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 2017
- Options
	- D Mankowitz, T Mann, and S Mannor. Time-regularized interrupting options. ICML'14
	- R Fox, S Krishnan, I Stoica, and K Goldberg. Multi-Level Discovery of Deep Options. 2017
	- M Machado, C Rosenbaum, X Guo, M Liu, G Tesauro, M Campbell. Eigenoption Discovery through the Deep Successor Representation, ICLR'18

## Memory
- From OpenAI spinning-up:
	- **MFEC**: Blundell et al. Model-Free Episodic Control. 2016
	- **NEC**: Pritzel et al. Neural Episodic Control. 2017
	- **Neural Map**: Parisotto and Salakhutdinov. Neural Map: Structured Memory for Deep Reinforcement Learning. 2017
	- **MERLIN**: Wayne et al. Unsupervised Predictive Memory in a Goal-Directed Agent. 2018
	- **RMC**: Santoro et al. Relational Recurrent Neural Networks. 2018

## Safety RL
- Legacy:
	- **CMDP**: E Altman. Constrained Markov Decision Processes. 1999
	- E Uchibe and K Doya. Constrained reinforcement learning from intrinsic and extrinsic rewards. ICDL'07
		- Heuristic CMDP: gradient projection
	- T Moldovan and P Abbeel. Safe Exploration in Markov Decision Processes. ICML'12
- Y Chow, M Ghavamzadeh, L Janson, and M Pavone. Risk-Constrained Reinforcement Learning with Percentile Risk Criteria. JMLR'15
	- **Primal-dual** subgradient method for risk-constrained reinforcement learning 
	- Takes policy gradient steps on an objective that trades off return with risk
	- Simultaneously learning the trade-off coefficients (dual variables).
- B Ammarm, R Tutunov, and E Eaton. Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret. ICML'15
	- Theoretic
- Amodei et al. Concrete Problems in AI Safety, 2016. 
	- Contribution: establishes a taxonomy of safety problems, serving as an important jumping-off point for future research. We need to solve these!
	- 1. Avoiding Negative Side Effects;
	- 2. Avoiding Reward Hacking;
	- 3. Scalable Oversight;
	- 4. Safe Exploration;
	- 5. Robustness to Distribution Shift;
- S Shalev-Shwartz, S Shammah, and A Shashua. Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving. 2016
	- avoid the problem of enforcing constraints on parametrized policies by decomposing 'desires' from trajectory planning;
	- the neural network policy learns desires for behavior
	- while the trajectory planning algorithm (which is not learned) selects final behavior and enforces safety constraints;
- **LFP**: Christiano et al. Deep Reinforcement Learning From Human Preferences, 2017.
- **CPO**: Achiam et al. Constrained Policy Optimization. 2017.
	- near-constraint satisfaction at each iteration\
		<img src="/RL/images/safety/cpo.png" alt="drawing" width="400"/>
- D Held, Z Mccarthy, M Zhang, F Shentu, and P Abbeel. Probabilistically Safe Policy Transfer. ICRA'17
- **HIRL**: Saunders et al. Trial without Error: Towards Safe Reinforcement Learning via Human Intervention, 2017.
- **Intrinsic-fear**: Z Lipton, J Gao, L Li, J Chen, and D Deng. Combating Deep Reinforcement Learning’s Sisyphean Curse with Intrinsic Fear.
- **Leave-No-Trace**: Eysenbach et al. Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning, 2017.
- Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M. Risk-constrained reinforcement learning with percentile risk criteria. JMLR'17
- **Safety-Layer**: Dalal et al. Safe Exploration in Continuous Action Spaces. ICML'18. 
	- Algorithm: DDPG+Safety Layer.
	- Problem setup: bounded CMDP, safe control in physical systems\
		<img src="/RL/images/safety/safety-layer1.png" alt="drawing" width="400"/>
	- Plug-in for DDPG:\
		<img src="/RL/images/safety/safety-layer2.png" alt="drawing" width="400"/>
	- The safety layer;\
		<img src="/RL/images/safety/safety-layer3.png" alt="drawing" width="400"/>
- Y Chow, O Nachum, E Duenez-Guzman, M Ghavamzadeh. A Lyapunov-based Approach to Safe Reinforcement Learning. NIPS'18
	- CMDP (Constrained MDP), with upper-bound cost we should satisfy;
	- safe policy iteration (SPI) and safe value iteration (SVI);
	- safe DQN, safe DPI;
- Constrained Cross-Entropy Method for Safe Reinforcement Learning. NIPS'18
- Y Chow, O Nachum, A Faust, E Duenez-Guzman, M Ghavamzadeh. Lyapunov-based Safe Policy Optimization for Continuous Control. ICML'19
	- Lagrange method: max_θ min_λ c(xt,at) + λ (D(xt|pi,x0)-d)
	- Lyapunov constraint: feasibility set;
- Robust Control
	- **LQG-Robust**: Sarah Dean, Nikolai Matni, Benjamin Recht, and Vickie Ye. Robust Guarantees for Perception-Based Control. 2019
		- https://github.com/modestyachts/robust-control-from-vision
		- Affine error-profile model;
		- Model: MBRL (LQR)
		- Evaluated on synthetic example CARLA

## Reproducibility, Analysis, and Critique
- OpenAI. Benchmarking Deep Reinforcement Learning for Continuous Control. ICML'16
	- https://github.com/rll/rllab
- Islam et al. Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control. 2017.
- Henderson et al. Deep Reinforcement Learning that Matters. 2017.
- Henderson et al. Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods. 2018.
- Ilyas et al. Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms? 2018.
- Mania et al. Simple Random Search Provides a Competitive Approach to Reinforcement Learning. 2018.
- Wang et al. Benchmarking Model-Based Reinforcement Learning. 2019.

## RTS- Self-replay
- Challenge:
	- no single best strategy: rock-paper-scissors (Starcraft II)
	- Imperfect information
	- Long term planning
	- Real time
	- Large action space
- Model:
	- Starcraft I [FAIR]: LSTM for each unit;
	- Starcraft II [DeepMind]: Transformer + deep-LSTM
		- Transformer Torso (Deep reinforcement learning with relational inductive biases, ICLR'19)
	- OpenAI-Five: LSTM for each player;
- Learning techniques:
	- Self-replay;
	- Imitation learning to warm up: Starcraft II
	- PBT: Starcraft-II;
	- Distributed Learning: Rapid (OpenAI);
	- Binary reward: OpenAI-Five;
- Starcraft-II
	- https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/
	- https://github.com/deepmind/pysc2
	- Network
		- Auto-regressive policy head with a pointer network
		- a centralised value baseline (Counterfactual Multi-Agent Policy Gradients AAAI'18)
- FAIR: Starcraft
	- https://github.com/TorchCraft/TorchCraft	
	- **POMDP**
	- LSTM for each unit;
	- Train stage I: off-policy;
	- Stage II: on-policy;
	- Stardata: A starcraft ai research dataset
	- High-level strategy selection under partial observability in starcraft: Brood war
	- Forward modeling for partial observation strategy games-a starcraft defogger. NIPS'18
- OpenAI Five: Dota
	- https://openai.com/blog/openai-five/
	- Pure from selfplay.
	- Distributed training: Rapid, applicable to gym environment
		- Workers push data of game play
		- Optimizer: P100 GPU, PPO with Adam, batch size 4096, BPTT 16 time steps, NCCL to average gradients (previously with MPI allreduce)
		- Eval workers: 2500 CPUs, v.s. hardcoded scripted bots and self
	- Difference versus humans: 150-170 actions per minute;
	- Creep blocking can be learned from scratch.

## Multi-Agent
- MA-DDPG: OpenAI. 2018
- Legacy:
	- M. Lauer and M. Riedmiller. An algorithm for distributed reinforcement learning in cooperative multi-agent systems. ICML'00
	- L. Panait and S. Luke. Cooperative multi-agent learning: The state of the art. AAMAS'05
	- L. Matignon, G. J. Laurent, and N. Le Fort-Piat. Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams. IROS'07
	- L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforcement learning. 2008
- Particle-World: Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments, 2018
	- https://github.com/openai/multiagent-particle-envs
- J Suarez, Y Du, P Isola, I Mordatch. Neural MMO: A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents. ICLR 2019 reject
- S Liu, G Lever, N Heess, J Merel, S Tunyasuvunakool, and T Graepel. Emergent coordination through competition. ICLR'19
- DeepMind. Human-level performance in 3D multiplayer games with population-based reinforcement learning. Science'19
	- PBT;
	- CNN + Slow/fast-RNN;
- Hide-and-Seek. OpenAI. Emergent Tool Use From Multi-Agent Autocurricula. 2019
	- http://openai.com/blog/emergent-tool-use
	- http://github.com/openai/multi-agent-emergence-environments
	- Key insight: six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt
	- Policy: PPO + GAE;
	- Platform: rapid;
	- Entity centric and attention based to capture object-level info;
	- Net structure: CNN-RNN;

## Navigation
- GA3C: https://github.com/tgangwani/GA3C-DeepNavigation
- pycolab: https://github.com/deepmind/pycolab
- Platform, Challenge and Benchmark:
	- VizDoom 2016
	- DeepMind Lab 2016
	- HoME 2017
	- House 3D: FAIR. 2018
		- https://github.com/facebookresearch/House3D
		- https://github.com/jxwuyi/HouseNavAgent
	- Chalet 2018
	- AI2-THOR 2017
	- **DeepMind-Lab**: Learning to navigate in complex environments. ICLR'17
	- Realistic: Matterport, AdobeIndoorNav, Stanford 2D-3D-S, Scannet, Gibson, MINOS
	- **StreetLearn**: DeepMind. The StreetLearn Environment and Dataset. 2019
		- Google Street View;
		- http://streetlearn.cc
	- Baidu XWorld (Zihang Dai): https://github.com/zihangdai/pytorch_xworld
- VIN
	- Good summaries:
		- https://zhuanlan.zhihu.com/p/25515755
		- https://zhuanlan.zhihu.com/p/24478944
	- **VIN**: A Tamar, Y Wu, G Thomas, S Levine, P Abbeel. Value Iteration Network. NIPS'16
		<img src="/RL/images/navigation/vin1.png" alt="drawing" width="600"/>
		<img src="/RL/images/navigation/vin2.png" alt="drawing" width="600"/>
	- **GVIN**: S Niu, S Chen, H Guo, C Targonski, M Smith, J Kovačević. Generalized Value Iteration Networks: Life Beyond Lattices. AAAI'18
		<img src="/RL/images/navigation/gvin.png" alt="drawing" width="600"/>
	- Sources:
		- https://github.com/kentsommer/pytorch-value-iteration-networks
- **Learning to Act by Predicting the Future**, ICLR 2017
	- Alexey Dosovitskiy, Vladlen Koltun
	- Win the 2nd Vizdoom competition
- Y Wu, Y Tian. Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning. ICLR'17
	- VizDoom, known map;
	- Batch A3C;
- DeepMind. Vector-based Navigation using Grid-like Representations in Artificial Agents. Nature'18
- P Mirowski. et.al. Learning to navigate in cities without a map. NIPS'18
- T Chen, S Gupta, and A Gupta. Learning exploration policies for navigation. 2019
- D Chaplot, S Gupta, A Gupta, R Salakhutdinov. Modular Visual Navigation using Active Neural Mapping. ICLR'19
- D Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov. Learning to Explore using Active Neural SLAM. ICLR'20
	- https://github.com/devendrachaplot/Neural-SLAM
	- https://www.cs.cmu.edu/~dchaplot/projects/neural-slam.html
	- Insight: combination of classics and deep learning, analytical path planners with learned SLAM module;
	- Winner of the CVPR 2019 Habitat PointGoal Navigation Challenge;
- K Hermann, M Malinowski, P Mirowski, A Banki-Horvath, K Anderson, R Hadsell. Learning to Follow Directions in Street View. AAAI'20
	- Input: front-view images (Google street-view), instructions; output: policy;
	- CNN-RNN;
