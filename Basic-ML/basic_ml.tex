\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\title{Basic ML, Bayes}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Zhuoyuan Chen\thanks{}\\%Use footnote for providing further information
    %about author (webpage, alternative address)---\emph{not} for acknowledging
    %funding agencies.} \\
  Facebook AI Research\\
  Menlo Park, CA 94025 \\
  \texttt{chenzhuoyuan07@gmail.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Basic machine learning
\end{abstract}

\section{Probability}
Quantile: cdf $F$, quantile $F^{-1}$

\subsection{Discrete}
Multinomial distribution:
\begin{eqnarray*}
Mu(x|n,\theta)=C(n;x_1,...,x_K)\prod_{j=1}^K\theta_j^{x_j}
\end{eqnarray*}

Poisson (to count rare events):
\begin{equation*}
Poi(x|\lambda)=e^{-\lambda}\frac{\lambda^x}{x!}
\end{equation*}

Empirical distribution:
\begin{equation*}
p_{emp}(A)=\frac{1}{N}\sum\delta_{x_i}(A)
\end{equation*}

\subsection{Continuous}
Gaussian:
\begin{eqnarray*}
N(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\
N(x|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]
\end{eqnarray*}

Student-t:
\begin{eqnarray*}
N(x|\mu,\sigma^2,\nu)=[1+\frac{1}{\mu}(\frac{x-\mu}{\sigma})^2]^{-(\frac{\nu+1}{2})}
\end{eqnarray*}
mean: $\mu$, model: $\mu$, variance: $\frac{\nu\sigma^2}{\nu-2}$

Laplace:
\begin{equation*}
Lap(x|\mu,b)=\frac{1}{2b}\exp(-\frac{|x-\mu|}{b})
\end{equation*}

Beta:
\begin{equation*}
Beta(x|a,b)=\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}
\end{equation*}

\subsection{Joint Distribution}
covariance:
\begin{equation*}
cov[X, Y]=\mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]
\end{equation*}

Dirichlet distribution (support on simplex):
\begin{equation*}
Dir(x|\alpha)=\frac{1}{B(\alpha)}\prod_{k=1}^K x_k^{\alpha_k-1}
\end{equation*}

Transformation:
\begin{equation*}
p_y(y)=p_x(x)|det J|
\end{equation*}

Central limit theorem, i.i.d. $p(x)$ with mean $\mu$ and variance $\sigma^2$.
\begin{equation*}
Z_n=\frac{\bar{X}-\mu}{\sigma/\sqrt{N}}
\end{equation*}

\section{Generative Modeling}
Naive Bayes: assume features are independent
\begin{equation*}
p(x|y=c,\theta)=\prod_{j=1}^D p(x_j|y=c,\theta_{jc})
\end{equation*}

\section{Gaussian}
Gaussian:
\begin{equation}
N(x|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]
\end{equation}

Theorem: N i.i.d. samples $x_i\sim N(\mu,\Sigma)$, then MLE for parameter is:
\begin{eqnarray}
\mu_{mle}=\frac{1}{N}\sum_ix_i\\
\Sigma_{mle}=\frac{1}{N}(x_i-\bar{x})(x_i-\bar{x})^T
\end{eqnarray}

Theorem: let $q(x)$ be any density $\int q(x)x_ix_j=\Sigma_{ij}$, Then $h(q)\le h(p)$
\subsection{LDA}

\subsection{Joint inference}
$\mu=\begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix} $
$\Sigma=\begin{pmatrix} \Sigma_{11} & \Sigma_{12}  \\ \Sigma_{21}  & \Sigma_{22}  \end{pmatrix} $
$\Lambda=\Sigma^{-1}=\begin{pmatrix} \Lambda_{11} & \Lambda_{12}  \\ \Lambda_{21}  & \Lambda_{22}  \end{pmatrix} $

The marginal:
\begin{eqnarray}
p(x_1) = N(x_1|\mu_1,\Sigma_{11})\\
p(x_2) = N(x_2|\mu_2,\Sigma_{22})
\end{eqnarray}

Posterior:
\begin{eqnarray}
p(x_1|x_2) &=& N(x_1|\mu_{1|2},\Sigma_{1|2})\\
\mu_{1|2} &=& \mu_1 - \Lambda_{11}^{-1}\Lambda_{12}(x_2-\mu_2)\\
        &=& \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2)\\
\Sigma_{1|2} &=& \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}=\Lambda_{11}^{-1}
\end{eqnarray}

2D-case:
$\Sigma=\begin{pmatrix} \sigma_1^2 & \rho\sigma_1\sigma_2  \\ \rho\sigma_1\sigma_2  & \sigma_2^2  \end{pmatrix} $
Then
\begin{equation}
p(x_1|x_2)=N(\mu_1+\frac{\rho\sigma_1}{\sigma_2}(x_2-\mu_2),\sigma_1^2(1-\rho^2))
\end{equation}

\end{document}
